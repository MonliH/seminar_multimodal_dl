## Text supporting computer vision models

*Chapter 2.4 / Topic 7*

*Author: Max Schneider*

*Supervisor: Jann Goschenhofer*

### Introduction

> "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.
> [\ldots] Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available.
> Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.
> [\ldots] One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great."
>
> --- @sutton2019bitterlesson

Most model choices presented in this chapter seem be directly inspired from this insight.
Each model can be seen as an attempt of its corresponding company to employ their available resources on the largest possible scale, with a particular focus on the size of the dataset.
This largely becomes feasible through the implementation of recent natural language processing (NLP) findings <!-- TODO: reference to Cem --> in the field of computer vision (CV). \
On the one hand, translations of architectural concepts firstly discovered in NLP are tried out for CV [see, e.g., the vision transformer\; @ImageT]. <!-- TODO: reference to Vladana -->
On the other hand, the power of these new NLP models, mostly of the transformer [@vaswani2017attention], is leveraged as text encoder building blocks inside bigger models, giving this chapter its name.
It has subchapters on the recent and relevant CV models CLIP [@radford2021learning], ALIGN [@jia2021scaling] and Florence [@solawetz2021florenceopen] and discusses some of their core concepts.
Their accuracy confirms the potential, hinted at by the impressive GPT-3 [@brown2020language], of improving scale using NLP concepts for CV.

### Concepts

#### Web-scale data

A core problem bugging researchers is the lack of robustness of previous state-of-the-art CV models to distribution shifts, i.e., when a pre-trained model with good accuracy on the original dataset fails to generalize to new datasets with the same categories.
E.g., a ResNet101, trained on ImageNet to 76.2% accuracy, turns out to only have 32.6% accuracy when tested on ObjectNet [@radford2021learning].
This suggests that perhaps the model did not learn correct latent representations, but overfit to the specific data-generating process. \
One way to tackle this would be to make changes to the architecture and extend it in order to achieve better robustness.
But these kind of adaptations, using expert knowledge, seem to repeat the mistake pointed out by @sutton2019bitterlesson; "micromanaging" a model is likely to hinder its scaling.
<!-- E.g., introducing long short term memory to recurrent neural networks (RNN) improved them in the short term, but made them more computationally expensive, while not solving their core problem of poor parallelization, which resulted in it being outcompeted by the heavily parallelizable transformer architecture. | Is this really true?-->

A different solution, based on scale, would be to increase the sample size, which is the approach explicitly taken by @jia2021scaling and, e.g., what @radford2021learning consider when making simplifications to their underlying architecture.
The large numbers of observations

* 400 million [CLIP\; @radford2021learning]
* 900 million [Florence\; @yuan2021florence]
* 1.8 billion [ALIGN\; @jia2021scaling]

are achieved by tapping into the vast amount of image-text pairs produced and readily available on the internet.
Thus, error prone, cost and labor intensive (difficult to scale), manual labeling is avoided. \
But these web data have some<!-- kind of heavy --> drawbacks.
Because of their extremely noisy nature, some form of pre-processing is needed, e.g., filtering for English language, excluding graphic content and images with non-informative alt-texts.
I.e., still some degree of dataset curation is involved, necessitating arbitrary choices. \
Also the social biases inherent to the internet are reproduced and while the approach improves data efficiency somewhat (see subsection \@ref(contrObj)), the poor performance of deep learning in this area is not substantially enhanced and mainly just compensated for with its super scalable source of supervision [@radford2021learning].

<!-- TODO: Other chapters discussing web-scale data? -->

#### Contrastive objective {#contrObj}

The information contained in the co-occurrence of the image with its alt-text is made available through the framework of natural language supervision.
The architectures use two jointly trained sub-networks to encode the image and the text, respectively.
The vector encodings are then aligned in the latent representation space by minimizing a variant of the contrastive objective loss function \@ref(eq:contrLoss) [@tian2020contrastive].

\begin{equation}
  \ell_1^{V_\text{img}, V_\text{txt}} = - \underset{\{v_\text{img}^1, v_\text{txt}^1, \ldots, v_\text{txt}^N\}}{\mathbb{E}} \left( \log \frac{h_\theta(\{v_\text{img}^1,v_\text{txt}^1\})}{h_\theta(\{v_\text{img}^1,v_\text{txt}^1\}) + \sum_{k=2}^N h_\theta(\{v_\text{img}^1, v_\text{txt}^k\})} \right)
  (\#eq:contrLoss)
\end{equation}

$\ell_1^{V_\text{img}, V_\text{txt}}$ is part of the loss for the first image-text pair, $v_\text{img}^1$ and $v_\text{txt}^1$ are latent representations of image 1 and text 1 and $h_\theta(\cdot)$ is a similarity measure.
In order to guarantee symmetry, the total loss is the sum of $\ell_1^{V_\text{img}, V_\text{txt}}$ and $\ell_1^{V_\text{txt}, V_\text{img}}$, where the similarity of one text to all images is averaged insted of the other way around.
<!-- Question: Is the second part really necessary? If all columns are optimized, all elements are considered. Maybe only if the similarity measure really is asymmetrical?-->

Figure \@ref(fig:contr-viz) visualizes this.
As the initialization, all images and texts of a given set are encoded by their respective sub-networks.
Using these latent vectors, a similarity matrix with elements $h_\theta(\{v_\text{img}^i,v_\text{txt}^j\})$ can be calculated.
Loosely speaking, the contrastive objective seeks to maximize diagonal elements and minimize the rest.

```{r contr-viz, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:contr-viz)'}
knitr::include_graphics('figures/02-04-text-support-img/contrastive-pre-training.png')
```
(ref:contr-viz) Visualization of a contrastive objective [@radford2021learning].

Another viewing angle on contrastive learning is shown in figure \@ref(fig:contr-vs-pred-learn).
The examplary task of predicting an images color based on its black and white version illustates an interesting difference between predictive and contrasitve learning.
In contrast to predictive learning, where first a prediction based on the input is made, and then the loss between this and the truth is calculated, contrastive learning has both the black and white image and its colors as input.
After encoding both, the loss is calculated, based on their latent versions, in the representation space.

```{r contr-vs-pred-learn, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:contr-vs-pred-learn)'}
knitr::include_graphics('figures/02-04-text-support-img/tian-predictive-vs-contrastive.png')
```
(ref:contr-vs-pred-learn) Predictive vs. contrastive learning: Contrastive loss is measured is in the latent representation space [@tian2020contrastive].

The deciding factor to use this objective function was data efficiency.

```{r data-efficiency, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:data-efficiency)'}
knitr::include_graphics('figures/02-04-text-support-img/data-efficiency.png')
```
(ref:data-efficiency) Data efficiency of contrastive objective [@radford2021learning].

* Pro: Efficient training <!-- citation e.g., ALIGN -->
* Pro: Out of box zero shot -> can serve as _foundation model_ [@bommasani2021opportunities] <!-- ref to zero shot chapter -->
* Contra: No longer a generative model, e.g., no flexible caption generation, see CLIP Limitations

#### Zero shooting and foundation models

<!-- But language models like BERT and GPT-3 have a big impact on another aspect of their field:
They become to serve as so called foundation models, future architectures use them as building blocks. (ref to foundation model paper, ref to DALLE oder so)
A trend which is observable for this new wave of CV models, too.
They show a large potential to be the CV counterparts to NLP foundation models. (ref Florence paper, ref models using CLIP as image encoder) -->

_Zero shooting_ is a paradigm coming from NLP research.
It means the previously fitted model is applied to a new, unseen dataset.
In a way each dataset can be seen as a different task and used to evaluate the models ability to perform it.
<!-- TODO: Name some datasets and their associated tasks. -->
This is done in order to avoid a bias in performance evaluation, where the model overfitted on the specific data-generating distribution.
<!-- TODO: Look up perfomance gain. -->
This is possible due to the flexible text encoding of CLIP.
<!-- TODO: Explain this better. -->
The model can readily function as a classifier by:

  1. Encoding all class labels.
  1. Predicting for an image, which encoded class label is most likely to come with it.

But in order to enhance performance by a margin of %d percent the prompts are engineered further.
They embed the class labels in sentence, e.g., "Picture of a (word)", which seemingly was necessary for the model to make full use of its learned parameters.

#### Connecting image representations to language

* Semantic concepts
* Learn a representation _and_ connect it to language (-> NLP)
* Directly communicate visual concepts to the model like "picture" or "macro" or "drawing"

### Architectures

#### CLIP

<!-- Making use of their available resources and the aggressive parallelization capabilities of transformer architectures. -->
<!-- Intro -->
* Focus on _task learning_ (datasets as proxies to tasks) instead of _representation learning_
* Contrastive, Language, Image, Pre-training
* Ref to CLIP inspiration from medical field with contrastive objective formula

Architecture

* Original transformer with modifications used for GPT family as a text encoder
* ResNet or vision transformer as a image encoder.
  * Vision transformer: much less compute
* High parallelization capabilities (transformer)
* Can CLIP be seen as a step closer to human-like AI?
  * No: performance drop from zero- to one-shot setting
  * No: contrastive objective?
  * Yes: visual representations connected to natural language

#### ALIGN

<!-- Intro -->
* Over one billion image alt-text pairs
* Name comes from alignment of visual and language representations trough the beloved and known contrastive loss or, very intuitively, "A Large-scale ImaGe and Noisy-text embedding"
* Dual encoder architecture
* Image + text image retrieval (e.g., Image of Eiffel tower + "snow" -> snowy Eiffel tower)
* Key difference to CLIP: training data. ALIGN does not filter that strongly, "dataset doesn't require expert knowledge to curate"

<!-- TODO: ALIGN doesn't filter data as strongly -->

#### Florence

<!-- Intro -->
<!-- Object detection vs scene level -->
* More fine-grained, dynamic, multimodal representations
* Focus shift to finding _foundation model_ as CLIP turned out to be especially useful for that.
  * Pre-trained core
  * Flexible addition of modules
    * _Dynamic Head_ for object detection - citations coming later
    * _METER_ as a adapter for vision-language (e.g., visual question answering)
    * Adaptation to video recognition through _CoSwin_
* General trend in this direction, better and better predictions [CoCa\; @yu2022coca]
* Optimization inside image-label-description space
* Encoders
  * Uses CLIP pendant as the language encoder
  * Swin transformer as the image encoder
  * CoSwin for embedding

### Performance comparison

* As all of these models are orders of magnitudes too large for performing a benchmark, findings reported inside the papers are believed here
<!-- TODO: Other comparisons -->

### Resources

One can find the pre-trained CLIP models on [Github](https://github.com/openai/CLIP).
They even found their way into simple command line tools already.
For example there is an application named [rclip](https://github.com/yurijmikhalevich/rclip), which can be used for personal image retrieval, wrapping the _ViT-B/32_ CLIP architecture.
On my (mid-range) laptop I was able to find seemingly good matches for search terms tried out inside a folder with about 100 pictures.
After an initial caching one request took about ten seconds.
<!-- TODO: Look up if data if available -->

### Outlook

CLIP as building block
CLASP
LAION dataset
TODO: CLIP as module
