# Chapter 2.4 Text supporting computer vision models

*Author: Max Schneider*

*Supervisor: Jann Goschenhofer*

## Introduction

> "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.
> [\ldots] Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available.
> Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.
> [\ldots] One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great."
>
> --- @sutton2019bitterlesson

Most model choices presented in this chapter seem be directly inspired from this insight.
Each model can be seen as an attempt of its corresponding company to employ their available resources on the largest possible scale, with a particular focus on the size of the dataset.
This largely becomes feasible through the implementation of recent natural language processing (NLP) findings <!-- TODO: reference to Cem --> in the field of computer vision (CV). \
On the one hand, translations of architectural concepts firstly discovered in NLP are tried out for CV [see, e.g., the vision transformer\; @ImageT]. <!-- TODO: reference to Vladana -->
On the other hand, the power of these new NLP models, mostly of the transformer [@vaswani2017attention], is leveraged as text encoder building blocks inside bigger models, giving this chapter its name.
It has subchapters on the recent and relevant CV models CLIP [@radford2021learning], ALIGN [@jia2021scaling] and Florence [@solawetz2021florenceopen] and discusses some of their core concepts.
Their accuracy confirms the potential, hinted at by the impressive GPT-3 [@brown2020language], of improving scale using NLP concepts for CV.

## Concepts

### Web-scale data

The fuel for all other scaling is sample size.
Without enough datapoints especially deep neural networks are likely to overfit without extra care.
But preparing the model architecture for overfitting using expert knowledge could lead to a violation of the introductory quote.
"Micromanaging" a model a model could hinder its scaling.
A different solution --- based on scale --- is simply to increase the sample size, which is the approach ALIGN [@jia2021scaling] is taking explicitly and, e.g., is what CLIP is considering when making simplifications to its underlying architecture [@radford2021learning].
The immense numbers of observations

* 400 million [CLIP\; @radford2021learning]
* 900 million [Florence\; @yuan2021florence]
* 1.8 billion [ALIGN\; @jia2021scaling]

are achieved by tapping into the vast, readily available, numbers of image-text pairs produced and stored online.
The idea is to make use of the supervision contained in their co-occurrence.
Making use of their available resources and the aggressive parallelization capabilities of transformer architectures.
Cost and labor intensive manual labeling is completely avoided.
But this readily available web-scale data comes with some drawbacks.
Because of its extremely noisy nature, some form of pre-processing is needed, e.g., filtering for language, excluding graphic content and images with non-informative captions.
Still some degree of dataset curation necessitating arbitrary choices.
Social biases are reproduced.

<!-- TODO: ALIGN doesn't filter this strongly -->
<!-- TODO: Other chapters discussing web-scale data? -->

### Contrastive objective

The maximizing scale approach explains a lot of further design choices.
The so called contrastive loss turned out to be very suitable for that.
* Ref to CLIP inspiration from medical field with contrastive objective formula
* Pro: Efficient training <!-- citation e.g., ALIGN -->
* Pro: Out of box zero shot -> can serve as _foundation model_ [@bommasani2021opportunities] <!-- ref to zero shot chapter -->
* Contra: No longer a generative model, e.g., no flexible caption generation
* Extra paper - but also in ALIGN?

### Zero shooting and foundation models

<!-- But language models like BERT and GPT-3 have a big impact on another aspect of their field:
They become to serve as so called foundation models, future architectures use them as building blocks. (ref to foundation model paper, ref to DALLE oder so)
A trend which is observable for this new wave of CV models, too.
They show a large potential to be the CV counterparts to NLP foundation models. (ref Florence paper, ref models using CLIP as image encoder) -->

_Zero shooting_ is a paradigm coming from NLP research.
It means the previously fitted model is applied to a new, unseen dataset.
In a way each dataset can be seen as a different task and used to evaluate the models ability to perform it.
<!-- TODO: Name some datasets and their associated tasks. -->
This is done in order to avoid a bias in performance evaluation, where the model overfitted on the specific data-generating distribution.
<!-- TODO: Look up perfomance gain. -->
This is possible due to the flexible text encoding of CLIP.
<!-- TODO: Explain this better. -->
The model can readily function as a classifier by:

  1. Encoding all class labels.
  1. Predicting for an image, which encoded class label is most likely to come with it.

But in order to enhance performance by a margin of %d percent the prompts are engineered further.
They embed the class labels in sentence, e.g., "Picture of a (word)", which seemingly was necessary for the model to make full use of its learned parameters.

### Connecting image representations to language

* Semantic concepts
* Learn a representation _and_ connect it to language (-> NLP)
* Directly communicate visual concepts to the model like "picture" or "macro" or "drawing"

## Architectures

### CLIP

<!-- Intro -->
* Focus on _task learning_ (datasets as proxies to tasks) instead of _representation learning_
* Contrastive, Language, Image, Pre-training

Architecture

* Original transformer with modifications used for GPT family as a text encoder
* ResNet or vision transformer as a image encoder.
  * Vision transformer: much less compute
* High parallelization capabilities (transformer)
* Can CLIP be seen as a step closer to human-like AI?
  * No: performance drop from zero- to one-shot setting
  * No: contrastive objective?
  * Yes: visual representations connected to natural language

### ALIGN

<!-- Intro -->
* Over one billion image alt-text pairs
* Name comes from alignment of visual and language representations trough the beloved and known contrastive loss or, very intuitively, "A Large-scale ImaGe and Noisy-text embedding"
* Dual encoder architecture
* Image + text image retrieval (e.g., Image of Eiffel tower + "snow" -> snowy Eiffel tower)
* Key difference to CLIP: training data. ALIGN does not filter that strongly, "dataset doesn't require expert knowledge to curate"

### Florence

<!-- Intro -->
<!-- Object detection vs scene level -->
* More fine-grained, dynamic, multimodal representations
* Focus shift to finding _foundation model_ as CLIP turned out to be especially useful for that.
  * Pre-trained core
  * Flexible addition of modules
    * _Dynamic Head_ for object detection - citations coming later
    * _METER_ as a adapter for vision-language (e.g., visual question answering)
    * Adaptation to video recognition through _CoSwin_
* General trend in this direction, better and better predictions [CoCa\; @yu2022coca]
* Optimization inside image-label-description space
* Encoders
  * Uses CLIP pendant as the language encoder
  * Swin transformer as the image encoder
  * CoSwin for embedding

## Performance comparison

* As all of these models are orders of magnitudes too large for performing a benchmark, findings reported inside the papers are believed here
<!-- TODO: Other comparisons -->

## Resources

One can find the pre-trained CLIP models on [Github](https://github.com/openai/CLIP).
They even found their way into simple command line tools already.
For example there is an application named [rclip](https://github.com/yurijmikhalevich/rclip), which can be used for personal image retrieval, wrapping the _ViT-B/32_ CLIP architecture.
On my (mid-range) laptop I was able to find seemingly good matches for search terms tried out inside a folder with about 100 pictures.
After an initial caching one request took about ten seconds.
<!-- TODO: Look up if data if available -->

## Outlook

CLIP as building block
CLASP
LAION dataset
TODO: CLIP as module
