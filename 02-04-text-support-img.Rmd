## Text supporting computer vision models

*Chapter 2.4 / Topic 7*

*Author: Max Schneider*

*Supervisor: Jann Goschenhofer*

<!-- TODO: Check present tense -->
<!-- TODO: Check correct usage of "data" -->
### Introduction

> "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.
> [\ldots] Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available.
> Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.
> [\ldots] One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great."
>
> --- @sutton2019bitterlesson

Most model choices presented in this chapter seem be directly inspired from this insight.
Each model can be seen as an attempt of its corresponding company to employ their available resources on the largest possible scale, with a particular focus on the size of the dataset.
This largely becomes feasible through the implementation of recent natural language processing (NLP) findings <!-- TODO: reference to Cem --> in the field of computer vision (CV). \
On the one hand, translations of architectural concepts firstly discovered in NLP are tried out for CV [see, e.g., the vision transformer\; @ImageT]. <!-- TODO: reference to Vladana -->
On the other hand, the power of these new NLP models, mostly of the transformer [@vaswani2017attention], is leveraged as text encoder building blocks inside bigger models, giving this chapter its name.
It has subchapters on the recent and relevant CV models CLIP [@radford2021learning], ALIGN [@jia2021scaling] and Florence [@solawetz2021florenceopen] and discusses some of their core concepts.
Their accuracy confirms the potential, hinted at by the impressive GPT-3 [@brown2020language], of improving scale using NLP concepts for CV.

### Concepts

#### Web-scale data

A core problem bugging researchers is the lack of robustness of previous state-of-the-art CV models to distribution shifts, i.e., when a pre-trained model with good accuracy on the original dataset fails to generalize to new datasets with the same categories.
E.g., a ResNet101, trained on ImageNet to 76.2% accuracy, turns out to only have 32.6% accuracy when tested on ObjectNet [@radford2021learning].
This suggests that perhaps the model did not learn correct latent representations, but overfit to the specific data-generating process. \
One way to tackle this would be to make changes to the architecture and extend it in order to achieve better robustness.
But these kind of adaptations, using expert knowledge, seem to repeat the mistake pointed out by @sutton2019bitterlesson; "micromanaging" a model is likely to hinder its scaling.
<!-- E.g., introducing long short term memory to recurrent neural networks (RNN) improved them in the short term, but made them more computationally expensive, while not solving their core problem of poor parallelization, which resulted in it being outcompeted by the heavily parallelizable transformer architecture. | Is this really true?-->

A different solution, based on scale, would be to increase the sample size, which is the approach explicitly taken by @jia2021scaling and, e.g., what @radford2021learning consider when making simplifications to their underlying architecture.
The large numbers of observations

* 400 million [CLIP\; @radford2021learning]
* 900 million [Florence\; @yuan2021florence]
* 1.8 billion [ALIGN\; @jia2021scaling]

are achieved by tapping into the vast amount of image-text pairs produced and readily available on the internet.
Thus, error prone, cost and labor intensive (difficult to scale), manual labeling is avoided. \
But these web data have some<!-- kind of heavy --> drawbacks.
Because of their extremely noisy nature, some form of pre-processing is needed, e.g., filtering for English language, excluding graphic content and images with non-informative alt-texts.
I.e., still some degree of dataset curation is involved, necessitating arbitrary choices. \
Also the social biases inherent to the internet are reproduced and while the approach improves data efficiency somewhat (see subsection \@ref(contrObj)), the poor performance of deep learning in this area is not substantially enhanced and mainly just compensated for with its super scalable source of supervision [@radford2021learning].

<!-- TODO: Other chapters discussing web-scale data? -->

#### Contrastive objective {#contrObj}

The information contained in the co-occurrence of the image with its alt-text is made available through the framework of natural language supervision.
The architectures use two jointly trained sub-networks to encode the image and the text, respectively.
The vector encodings are then aligned in the latent representation space by minimizing a variant of the contrastive objective loss function \@ref(eq:contrLoss) [@tian2020contrastive].

\begin{equation}
  \ell_1^{V_\text{img}, V_\text{txt}} = - \underset{\{v_\text{img}^1, v_\text{txt}^1, \ldots, v_\text{txt}^N\}}{\mathbb{E}} \left( \log \frac{h_\theta(\{v_\text{img}^1,v_\text{txt}^1\})}{h_\theta(\{v_\text{img}^1,v_\text{txt}^1\}) + \sum_{k=2}^N h_\theta(\{v_\text{img}^1, v_\text{txt}^k\})} \right)
  (\#eq:contrLoss)
\end{equation}

$\ell_1^{V_\text{img}, V_\text{txt}}$ is the first half of the loss for the first image-text pair, $v_\text{img}^1$ and $v_\text{txt}^1$ are latent representations of image 1 and text 1 and $h_\theta(\cdot)$ is a similarity measure.
In order to guarantee symmetry, the total loss is the sum of $\ell_1^{V_\text{img}, V_\text{txt}}$ and $\ell_1^{V_\text{txt}, V_\text{img}}$, where the pairwise similarity of one text and all images is averaged instead of the other way around.
<!-- Question: Is the second part really necessary? If all columns are optimized, all elements are considered. Maybe only if the similarity measure really is asymmetrical?-->

Figure \@ref(fig:contr-viz) visualizes this.
Initially all images and texts of a given dataset are encoded by the respective sub-networks.
Using the resulting latent vectors, a similarity matrix with elements $h_\theta(\{v_\text{img}^i,v_\text{txt}^j\})$ can be calculated.
Loosely speaking, the contrastive objective aims to maximize diagonal elements and minimize the rest.

```{r contr-viz, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:contr-viz)'}
knitr::include_graphics('figures/02-04-text-support-img/contrastive-pre-training.png')
```
(ref:contr-viz) Visualization of a contrastive objective [@radford2021learning].

Figure \@ref(fig:contr-vs-pred-learn) compares contrastive learning to classical predictive learning, giving another interesting insight.
The exemplary task is to color an image given its B/W version.
Approach (a) encodes the B/W image and in turn decodes its latent representation to fitting colors.
The goodness of this fit is measured in the output space. \
Conversely, approach (b) measures the loss in the representation space.^[Note that contrastive learning easily works with other combinations of modalities than text and image; here B/W and colors.]
A reason for the good performance of contrastive learning could be that while common prediction losses (e.g., the $\mathcal{L}_2$ loss) penalize each prediction output dimension independently, approach (b) implies measurement in the intertwined representation space [@tian2020contrastive].

```{r contr-vs-pred-learn, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:contr-vs-pred-learn)'}
knitr::include_graphics('figures/02-04-text-support-img/tian-predictive-vs-contrastive.png')
```
(ref:contr-vs-pred-learn) Predictive vs. contrastive learning: Predictive losses are measured in the output space while contrastive losses are measured is in the representation space; indicated by red dotted boxes [@tian2020contrastive].

But, rather than theoretical considerations, the driving factor for using this objective is data efficiency.
As can be seen in figure \@ref(fig:data-efficiency), @radford2021learning start their search for an adequate pre-train model (more on this in subsection \@ref(foundMod)) by experimenting with a transformer based language model predicting the exact captions of an image.
This turns out to train three times slower, in terms of data efficiency, compared to a simpler baseline predicting a bag-of-words text encoding.
Switching to the contrastive objective of CLIP additionally improves data efficiency by a factor of four. \
But, the switch to contrastive learning comes with some limitations.
Especially its rigidity demands certain extra steps and forfeits the high flexibility of generative models.
The end of the next subsection discusses this in more detail.

```{r data-efficiency, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:data-efficiency)'}
knitr::include_graphics('figures/02-04-text-support-img/data-efficiency.png')
```
(ref:data-efficiency) Data efficiency of contrastive objective.
Development of zero-shot accuracy on ImageNet with increasing number of instances of training data seen by models.
The contrastive objective reaches same accuracies compared to the exact predictive objective while seeing only a seventh of the amount of data [@radford2021learning].

<!-- TODO: Learn a representation _and_ connect it to language (-> NLP) -->

#### Zero shooting and foundation models {#foundMod}

Both concepts are paradigms from NLP.
Foundation models [@bommasani2021opportunities] are intended to be used again, maybe as building blocks inside bigger models, e.g., CLIP is used inside DALL$\cdot$E 2 to embed images [@ramesh2022hierarchical].
Another application of CLIP is the creation of the LAION-400M dataset [@schuhmann2022laion].
To validate collected image-text pairs their similarity scores are calculated using CLIP and instances with a value too low are discarded.

One key concept and application for this is zero-shooting.
This means applying a pre-trained model to new, unseen datasets.
This is a form of more unbiased performance evaluation, as, e.g., overfitting to the data-generating distribution will not distort results. \
Figure \@ref(fig:zero-shooting) visualizes how this is implemented in contrastive learning.
For image classification, all available class names are embedded in a longer sentence prior to being encoded by the language model.
Afterwards the image to be classified is encoded and all similarity scores are returned, where the highest one can be retrieved as the decision.
Image retrieval would work the other way around.
After an initial encoding, the closest image to a encoded text in the representation space can be returned.

```{r zero-shooting, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:zero-shooting)'}
knitr::include_graphics('figures/02-04-text-support-img/zero-shooting.png')
```
(ref:zero-shooting) Visualization of zero-shooting [@radford2021learning].

<!-- But language models like BERT and GPT-3 have a big impact on another aspect of their field:
They become to serve as so called foundation models, future architectures use them as building blocks. (ref to foundation model paper, ref to DALLE oder so)
A trend which is observable for this new wave of CV models, too.
They show a large potential to be the CV counterparts to NLP foundation models. (ref Florence paper, ref models using CLIP as image encoder) -->

* Pro: Out of box zero shot -> can serve as _foundation model_ [@bommasani2021opportunities] <!-- ref to zero shot chapter -->
* Contra: No longer a generative model, e.g., no flexible caption generation, see CLIP Limitations

_Zero shooting_ is a paradigm coming from NLP research.
It means the previously fitted model is applied to a new, unseen dataset.
In a way each dataset can be seen as a different task and used to evaluate the models ability to perform it.
<!-- TODO: Name some datasets and their associated tasks. -->
This is done in order to avoid a bias in performance evaluation, where the model overfitted on the specific data-generating distribution.
This is possible due to the flexible text encoding of CLIP.

### Architectures

#### CLIP

<!-- TODO:
  I skipped over the text embedding in figure zero-shooting
  But in order to enhance performance by a margin of %d percent the prompts are engineered further.
  They embed the class labels in sentence, e.g., "Picture of a (word)", which seemingly was necessary for the model to make full use of its learned parameters.
  Directly communicate visual concepts to the model like "picture" or "macro" or "drawing"
  Look up performance gain. -->
<!-- mention parallelization capabilities of transformers -->

<!-- Intro -->
* Focus on _task learning_ (datasets as proxies to tasks) instead of _representation learning_
* Contrastive, Language, Image, Pre-training
* Ref to CLIP inspiration from medical field with contrastive objective formula

Architecture

* Original transformer with modifications used for GPT family as a text encoder
* ResNet or vision transformer as a image encoder.
  * Vision transformer: much less compute
* High parallelization capabilities (transformer)
* Can CLIP be seen as a step closer to human-like AI?
  * No: performance drop from zero- to one-shot setting
  * No: contrastive objective?
  * Yes: visual representations connected to natural language

#### ALIGN

<!-- Intro -->
* Over one billion image alt-text pairs
* Name comes from alignment of visual and language representations trough the beloved and known contrastive loss or, very intuitively, "A Large-scale ImaGe and Noisy-text embedding"
* Dual encoder architecture
* Image + text image retrieval (e.g., Image of Eiffel tower + "snow" -> snowy Eiffel tower)
* Key difference to CLIP: training data. ALIGN does not filter that strongly, "dataset doesn't require expert knowledge to curate"

<!-- TODO: ALIGN doesn't filter data as strongly -->

#### Florence

<!-- Intro -->
<!-- Object detection vs scene level -->
* More fine-grained, dynamic, multimodal representations
* Focus shift to finding _foundation model_ as CLIP turned out to be especially useful for that.
  * Pre-trained core
  * Flexible addition of modules
    * _Dynamic Head_ for object detection - citations coming later
    * _METER_ as a adapter for vision-language (e.g., visual question answering)
    * Adaptation to video recognition through _CoSwin_
* General trend in this direction, better and better predictions [CoCa\; @yu2022coca]
* Optimization inside image-label-description space
* Encoders
  * Uses CLIP pendant as the language encoder
  * Swin transformer as the image encoder
  * CoSwin for embedding

### Performance comparison

* As all of these models are orders of magnitudes too large for performing a benchmark, findings reported inside the papers are believed here
<!-- TODO: Other comparisons -->

### Resources

One can find the pre-trained CLIP models on [Github](https://github.com/openai/CLIP).
They even found their way into simple command line tools already.
For example there is an application named [rclip](https://github.com/yurijmikhalevich/rclip), which can be used for personal image retrieval, wrapping the _ViT-B/32_ CLIP architecture.
On my (mid-range) laptop I was able to find seemingly good matches for search terms tried out inside a folder with about 100 pictures.
After an initial caching one request took about ten seconds.
<!-- TODO: Look up if data if available -->

### Outlook

CLIP as building block
CLASP
LAION dataset
TODO: CLIP as module
