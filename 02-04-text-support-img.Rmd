## Text supporting computer vision models

*Chapter 2.4 / Topic 7*

*Author: Max Schneider*

*Supervisor: Jann Goschenhofer*

<!-- TODO: Check present tense -->
<!-- TODO: Check correct usage of "data" -->
<!-- TODO: Check consistent spelling of pre-train -->
<!-- TODO: Check consistent first letter capitalization of models -->
<!-- TODO: check "an" and "a" -->
### Introduction

> "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.
> [\ldots] Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available.
> Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.
> [\ldots] One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great."
>
> --- @sutton2019bitterlesson

This insight seems to directly inspire most model choices presented in this chapter.
Each network can be seen as an attempt of its company to employ their vast available resources on a large scale, with a particular focus on dataset sizes.
These mostly become feasible through the adaptation of recent findings in natural language processing (NLP) <!-- TODO: reference to Cem --> to computer vision (CV). \
On the one hand, architectural concepts firstly popularized in NLP are translated to CV [e.g., self-supervised learning or the Vision Transformer\; @ImageT <!-- TODO: reference to Vladana -->].
On the other hand, these powerful new NLP models, mostly Transformers [@vaswani2017attention], support bigger models from the inside as text encoding building blocks; hence the name of this chapter.
There are subsections on the recent and relevant CV models CLIP [@radford2021learning], ALIGN [@jia2021scaling] and Florence [@solawetz2021florenceopen] and a discussion of some relevant core concepts.
The strong performances confirm the potential, hinted at by the impressive GPT-3 [@brown2020language], of improving CV and increasing scale with the help of NLP.

### Concepts

#### Web-scale data {#webScaleData}

A core problem that troubles researchers is the lack of robustness of previous state-of-the-art CV models to distribution shifts.
I.e., when a model with good performance on its original dataset fails to generalize (transfer its knowledge) to new, more or less similar datasets.
E.g., @radford2021learning report that a ResNet101 which they trained on ImageNet to an accuracy of 76.2% maintains only an accuracy of 32.6% on ObjectNet.
This suggests that the model perhaps did not learn high quality latent representations, but instead overfit to the dataset-specific data-generating distribution. \
A <!-- TODO: naheliegend -->common way to tackle this would be to try out various changes on the architecture and the training algorithm of the network.
But this kind of adaptation, inscribing expert knowledge into the model, seems to repeat the mistake pointed out by @sutton2019bitterlesson; "micromanaging" a model is likely to thwart future scaling.
<!-- E.g., introducing long short term memory to recurrent neural networks (RNN) improved them in the short term, but made them more computationally expensive, while not solving their core problem of poor parallelization, which resulted in it being outcompeted by the heavily parallelizable transformer architecture. | Does this really make sense?-->

The researchers of CLIP, ALIGN and Florence follow a different approach, based on scale.
They try to increase sample size as much as possible and work with tremendous numbers of observations:

* 400 million [CLIP\; @radford2021learning]
* 900 million [Florence\; @yuan2021florence]
* 1.8 billion [ALIGN\; @jia2021scaling]

These are achieved with the vast amount of image-text pairs produced by and readily available on the internet.
Thus, error prone, cost and labor intensive (difficult to scale), manual labeling is avoided. \
Unfortunately, the models trained on web data also become vulnerable to their <!-- kind of heavy -->downsides.
Because of the extremely noisy nature of them, still some form of pre-processing is needed, e.g., filtering for English language, excluding graphic content and, optionally, removing images with non-informative alt-texts.
This makes some degree of dataset curation, and therefore arbitrary choices, necessary. \
Likewise, the social biases inherent to the internet are reproduced and furthermore, while the approach improves data efficiency to some degree (see next subsection \@ref(contrObj)), the poor performance of deep learning in this area is not substantially enhanced and mainly just compensated for with a super scalable source of supervision [@radford2021learning].

<!-- TODO: Other chapters discussing web-scale data? -->

#### Contrastive objective {#contrObj}

This source of supervision is the information contained in the co-occurrence of the image with its alt-text.
It is accessed through natural language supervision.
The architectures jointly train two sub-networks for image and text encoding, respectively.
During this, the vector encodings are aligned in the latent representation space through minimizing a variant of the contrastive loss function \@ref(eq:contrLoss) [@tian2020contrastive].
Half of the first image-text pair loss

\begin{equation}
  \ell_1^{V_\text{img}, V_\text{txt}} = - \underset{\{v_\text{img}^1, v_\text{txt}^1, \ldots, v_\text{txt}^N\}}{\mathbb{E}} \left( \log \frac{h_\theta(\{v_\text{img}^1,v_\text{txt}^1\})}{h_\theta(\{v_\text{img}^1,v_\text{txt}^1\}) + \sum_{k=2}^N h_\theta(\{v_\text{img}^1, v_\text{txt}^k\})} \right),
  (\#eq:contrLoss)
\end{equation}

where $v_\text{img}^1$ and $v_\text{txt}^1$ are vector encodings (latent representations) of image 1 and text 1 and $h_\theta(\cdot)$ is a similarity measure.
In order to guarantee symmetry, the total loss is formed by the sum of $\ell_1^{V_\text{img}, V_\text{txt}}$ and $\ell_1^{V_\text{txt}, V_\text{img}}$, where the pairwise similarities of one text and every image is calculated instead of the other way around.
<!-- Question: Is the second part really necessary? If all columns are optimized, all elements are considered. Maybe only if the similarity measure really is asymmetrical?-->

Figure \@ref(fig:contr-viz) visualizes this.
Initially all images and texts in the training data are encoded by the responsible sub-network.
Using the resulting encodings, a similarity matrix with elements $h_\theta(\{v_\text{img}^i,v_\text{txt}^j\})$ can be calculated.
Loosely speaking, the contrastive objective is to maximize elements on the diagonal and minimize the others.

```{r contr-viz, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:contr-viz)'}
knitr::include_graphics('figures/02-04-text-support-img/contrastive-pre-training.png')
```
(ref:contr-viz) Visualization of a contrastive objective [@radford2021learning]. After encoding the data, a similarity matrix for the images and texts is computed. The aim is that the N true image-text pairs score high in terms of similarity, while the $\text{N}^2 - \text{N}$ other possible combinations score low.

Contrastive learning can be contrasted with classical predictive learning.
Figure \@ref(fig:contr-vs-pred-learn) gives an interesting insight into the choice of space, where goodness of fit is measured.
The exemplary task is to color an image given its B/W version.
Approach (a) first encodes the B/W image and then decodes the interim latent representation to fitting colors.
The goodness of this fit is measured in the output space, meaning the estimated colors are compared to the true colors. \
Conversely, approach (b) measures the loss in the representation space.^[Note that contrastive learning easily works with other combinations of modalities than text and image; here B/W and colors.]
A reason for the good performance of contrastive learning could be that, while common prediction losses (e.g., the $\mathcal{L}_2$ loss) penalize each prediction output dimension independently, approach (b) implies measurement in the intertwined representation space [@tian2020contrastive].

```{r contr-vs-pred-learn, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:contr-vs-pred-learn)'}
knitr::include_graphics('figures/02-04-text-support-img/tian-predictive-vs-contrastive.png')
```
(ref:contr-vs-pred-learn) Predictive vs. contrastive learning: Predictive losses are measured in the output space while contrastive losses are measured is in the representation space, indicated by red dotted boxes [@tian2020contrastive].

But in the end, rather than theoretical considerations, the driving factor for using this objective is data efficiency.
As can be seen in figure \@ref(fig:data-efficiency), @radford2021learning start their search for an adequate pre-train model (more on this in subsection \@ref(foundMod)) by experimenting with a Transformer-based language model predicting the exact captions of an image.
It turns out that this approach trains three times slower, in terms of data efficiency, compared to a simpler baseline of predicting a bag-of-words text encoding.
Additionally, switching to the contrastive objective of CLIP improves data efficiency by a factor of four.

```{r data-efficiency, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:data-efficiency)'}
knitr::include_graphics('figures/02-04-text-support-img/data-efficiency.png')
```
(ref:data-efficiency) Data efficiency of contrastive objective. Development of zero-shot accuracy (see next subsection \@ref(foundMod)) on ImageNet with increasing number of instances of training data seen by models. The contrastive objective reaches same accuracies as the generative approach with only a seventh of the amount of data [@radford2021learning].

Nonetheless, the switch to contrastive learning leads to some limitations.
Its rigidity demands certain extra steps and forfeits the high flexibility of generative models.
In particular, this means contrastive models similar to CLIP are limited to choose from available options and cannot freely generate texts or images.
To extend the capabilities of those models additional network building blocks are necessary.

#### Foundation models and zero-shooting {#foundMod}

The first models which are considered foundation models today began to appear in NLP.
The term, later coined by @bommasani2021opportunities, refers to models that are noteworthy due to their large scale and ability to adapt to a wide variety of downstream tasks.
An early example is BERT [@Devlin2018]. \
A lot of the time, foundation models have an unfinished touch to them and the true scope of their capabilities cannot be sketched out clearly.
This generally is the case, because the desired abilities of neural networks are not designed for explicitly, but rather emerge during their implementation.
@bommasani2021opportunities cite GPT-3's ability to perform certain types of new tasks soley by confronting it with the right natural language prompt.
E.g., it is possible to get GPT-3 to summarize a paragraph by appending "TL;DR" (too long, didn't read) to the prompt, which is a common pattern on the internet to signal a following summery.
This is called "in-context learning" [@brown2020language]. \
It is apparent that one can make up plenty of unexpected ways to employ these models and it cannot be known whether there is a further way no one thought of yet.
This means possibly saving computational and data collection costs down the line, which ineptly is true for malicious use cases, e.g., surveillance, too.

Foundation models build on the concept of transfer-learning, i.e., pre-training a model on a task that is feasible to do and applying it to the desired task downstream.
In the context of this chapter this means pre-training on web-scale data (see subsection \@ref(webScaleData)) and evaluating performance on various common classification datasets.
E.g., @radford2021learning name the SVHN dataset as a proxy for the task "street number transcription" with the caveat "on the distribution of Google Street View photos", but they remark that a lot of datasets have no obvious, specific task associated, e.g., CIFAR-10.
They use these kind of datasets for measuring the "robustness to distribution shift and domain generation" of their model, which still is a topic of great interest as mentioned in subsection \@ref(webScaleData). \
When there is no further fine-tuning on the downstream task, i.e., no resuming of training on the new dataset, this is called zero-shooting.
It has the clear advantage of evaluating performance more unbiased, as processes like overfitting to the data-generating distribution will not distort results.

Figure \@ref(fig:zero-shooting) shows how contrastive models perform zero-shot transfer.
In the case of image classification all available classes are encoded by the language model.
Afterwards, the CV sub-network computes the encoding of the image to be classified and all pair-wise similarity scores are returned.
The pair with the best score can be retrieved as the decision. \
Image retrieval works the other way around:
After an initial encoding of all images, the ones most similar to the encoded natural language text prompt in the representation space can be returned.

```{r zero-shooting, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:zero-shooting)'}
knitr::include_graphics('figures/02-04-text-support-img/zero-shooting.png')
```
(ref:zero-shooting) Visualization of zero-shooting [@radford2021learning].

### Architectures

#### CLIP

The first of the large scale contrastive CV models that were published is CLIP, short for Contrastive Language-Image Pre-training [@radford2021learning].
The components of its name are explained in previous subsections \@ref(contrObj), \@ref(webScaleData) and \@ref(foundMod) and are the crucial concepts of ALIGN and Florence as well.
CLIP is a product of OpenAI, but its code is freely available and the different versions can be accessed as [python modules](https://github.com/openai/CLIP).
Not released is its dataset.

A lot of preliminary work stems from @zhang2020contrastive, whose paper firstly describes contrastive representation learning using image-text pairs.
Their implementation of the contrastive loss function \@ref(eq:contrLoss) is

\begin{equation}
  \ell_1^{V_\text{img}, V_\text{txt}} = - \log \frac{\exp(\langle v_\text{img}^1, v_\text{txt}^1 \rangle / \tau)}{\sum_{k=1}^{N} \exp(\langle v_\text{img}^1, v_\text{txt}^k \rangle / \tau)},
  (\#eq:contrLossCLIP)
\end{equation}

where $\langle v_\text{img}^1, v_\text{txt}^1 \rangle$ represents the cosine similarity, i.e., $v_\text{img}^{1 \top} v_\text{txt}^1 / (\|v_\text{img}^1\| \|v_\text{txt}^1\|)$, and $\tau \in \mathbb{R}^+$ is a temperature parameter, which is directly learned during training [@zhang2020contrastive].
CLIP adopts this.
$\ell_1^{V_\text{txt}, V_\text{img}}$, the counterpart to $\ell_1^{V_\text{img}, V_\text{txt}}$ for the total loss, is function \@ref(eq:contrLossCLIP) with switched arguments.
This can be viewed as a symmetric cross entropy loss over the cosine similarity of the embeddings [@radford2021learning].

**Architecture**

The text encoder for CLIP (see figure \@ref(fig:contr-vs-pred-learn)) is a modified Transformer [@vaswani2017attention], which was also used in GPT-2 [@radford2019language].
For the image encoder multiple sub-networks are tested:

* ResNets: ResNet-50, ResNet-101
* ResNets which follow EfficientNet-style model scaling: RN50x4, RN50x16, RN50x64
* Vision Transformers: ViT-B/32, ViT-B/16, ViT-L/14

The best performing sub-network was the ViT-L/14, for which in turn they continued training for an additional epoch with higher resolution images (336px), giving it the name ViT-L/14@336px.
If not indicated otherwise the performance was measured on this version of CLIP.
The EfficientNet-style ResNets use x4, x16 and x64 of the compute of a ResNet-50 and the largest model (the RN50x64) trained for 18 days on 592 V100 GPUs, while the ViT-L/14 only took 12 days on 256 GPUs.
The high capabilities in terms of parallelization of transformers seems to pay off.

When explaining zero-shooting (see subsection \@ref(foundMod)) initially, a part was skipped.
As can be seen in figure \@ref(fig:zero-shooting) there is an additional step before feeding image labels into the text encoder.
In order to enhance performance the prompts are further engineered, which seemingly is necessary for the model to understand the context of the words.
For this, the class labels are embedded in a sentence, e.g., "A photo of a {label}.", which increases ImageNet accuracy by 1.3 percentage points (pp).
When ensembling 80 different context prompts^[Prompts like: "A photo of a big {label}.", "A photo of a small {label}." [@radford2021learning]] ImageNet accuracy is improved by an additional 3.5pp, adding up to nearly 5pp.
The average gain across 36 datasets is reported to be 5pp. \
Another interesting aspect arising out of the connection of image and text representations is the ability to directly communicate visual concepts like "picture", "macro", "drawing" or even "dog" to the model.

**Performance**

Figure \@ref(fig:performance-clip) compares the performance of CLIP and a ResNet101 trained on ImageNet to the same accuracy as zero-shot CLIP.
The results of @radford2021learning show that CLIP constitutes an important step towards closing the robustness gap mentioned in subsection \@ref(webScaleData):
With datasets coming from more and more differing data-generating distributions to ImageNet the performance of ResNet101 deteriorates while CLIP remains fairly accurate. \
But these findings have to be taken with a grain of salt.
While @radford2021learning devote a subsection to data overlap analysis between training and zero-shot data, they do not give access their dataset, which makes their claims impossible to investigate by independent third parties.

```{r performance-clip, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:performance-clip)'}
knitr::include_graphics('figures/02-04-text-support-img/performance-clip.png')
```
(ref:performance-clip) Robustness of zero-shot CLIP to distribution shifts [@radford2021learning].

@shen2021much discover another curios fact.
They study performance improvements in Vision-and-Language models when the visual encoder is switched to CLIPs strong image encoder and discover that in this field of CV the ViT-B scores worse than the ResNets.
E.g., tests on image captioning reveal that the V&L model using ViT-B often is only half as good as the version using the RN50x4 -- the largest one used in this study. \
They propose that ViT-B lacks visual localization abilities due to its pooling strategies.
To test this @shen2021much perform some test which confirm their supposition, e.g., figure \@ref(fig:attention-ViT) which depicts a Grad-CAM Visualization of a V&L model with a ResNet-50 backbone and a ViT-B backbone for the question "What color is the woman's shirt on the left?".

```{r attention-ViT, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:attention-ViT)'}
knitr::include_graphics('figures/02-04-text-support-img/attention-of-ViT.png')
```
(ref:attention-ViT) Attention.

#### ALIGN

@jia2021scaling take an approach which is a little different.
They reiterate the necessity of large-scale vision datasets and assert that even CLIPs data collection process still involves a non-trivial data curation cost.
For this they explore employing a minimal amount of filtering and controlling the noise simply via sample size.
This results in a dataset with 1.8 billion image-text pairs.
They name their model ALIGN, short for "A Large-scale ImaGe and Noisy-text embedding", hinting at the contrastive loss, which aligns vector encodings in the representation space (see subsection \@ref(contrObj)).

**Architecture**

They use the dual encoder architecture with BERT-Large as the text and EfficientNet-L2 as the image encoder, which they train from scratch.
The model has around 800 million parameters [@alford2021alignparams].

They also mention a funny property of the access to aligned image and text encoders.
One can perform simple arithmetic operations, which they use for image + text retrieval.
E.g., encoding an image of the Eiffel tower and the word "snow" and adding up the vector encodings can retrieve a picture of a snowy Eiffel tower using cosine similarity, see firgure \@ref(fig:img-txt-addition).

<!-- TODO: Learn a representation _and_ connect it to language (-> NLP) -->

```{r img-txt-addition, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:img-txt-addition)'}
knitr::include_graphics('figures/02-04-text-support-img/align-word-and-image-addition.png')
```
(ref:img-txt-addition) Addition of word and image embedding.

**Diskussion**

* more expensive training

#### Florence

<!-- Intro -->
<!-- Object detection vs scene level -->
* More fine-grained, dynamic, multimodal representations
* Focus shift to finding _foundation model_ as CLIP turned out to be especially useful for that.
  * Pre-trained core
  * Flexible addition of modules
    * _Dynamic Head_ for object detection - citations coming later
    * _METER_ as a adapter for vision-language (e.g., visual question answering)
    * Adaptation to video recognition through _CoSwin_
* General trend in this direction, better and better predictions [CoCa\; @yu2022coca]
* Optimization inside image-label-description space
* Encoders
  * Uses CLIP pendant as the language encoder
  * Swin transformer as the image encoder
  * CoSwin for embedding

### Performance comparison

* As all of these models are orders of magnitudes too large for performing a benchmark, findings reported inside the papers are believed here

```{r table1, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:table1)'}
knitr::include_graphics('figures/02-04-text-support-img/table-imagenet.png')
```
(ref:table1) Top-1 Accuracy of zero-shot transfer of models to image classification on ImageNet and its variants.


```{r table2, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:table2)'}
knitr::include_graphics('figures/02-04-text-support-img/table-img-txt-retrieval.png')
```
(ref:table2) Zero-shot image and text retrieval [@yuan2021florence].

```{r table3, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:table3)'}
knitr::include_graphics('figures/02-04-text-support-img/table-misc-datasets.png')
```
(ref:table3) Top-1 Accuracy of CLIP, Florence and ALIGN on various datasets.

### Resources

One can find the pre-trained CLIP models on [Github](https://github.com/openai/CLIP).
They even found their way into simple command line tools already.
For example there is a CLI named [rclip](https://github.com/yurijmikhalevich/rclip), which can be used for personal image retrieval, wrapping the _ViT-B/32_ CLIP architecture.
On my (mid-range) laptop I was able to find seemingly good matches for search terms tried out inside a folder with about 100 pictures.
After an initial caching one request took about ten seconds.
<!-- Assumption that all data is in-distribution -->

### Outlook

CLIP is used inside DALL$\cdot$E 2 to embed images [@ramesh2022hierarchical].
Another application of CLIP is the creation of the LAION-400M dataset [@schuhmann2022laion].
To validate collected image-text pairs their similarity scores are calculated using CLIP and instances with a value too low are discarded.
<!-- CLASP -->

