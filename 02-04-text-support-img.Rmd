## Text supporting computer vision models

*Chapter 2.4 / Topic 7*

*Author: Max Schneider*

*Supervisor: Jann Goschenhofer*

<!-- TODO: Check present tense -->
<!-- TODO: Check correct usage of "data" -->
<!-- TODO: Check consistent spelling of pre-train -->
<!-- TODO: Check consistent first letter capitalization of models -->
<!-- TODO: check "an" and "a" -->
### Introduction

> "The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.
> [\ldots] Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available.
> Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation.
> [\ldots] One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great."
>
> --- @sutton2019bitterlesson

This insight seems to directly inspire most model choices presented in this chapter.
Each network can be seen as an attempt of its company to employ their vast available resources on a large scale, with a particular focus on dataset sizes.
These mostly become feasible through the adaptation of recent findings in natural language processing (NLP) <!-- TODO: reference to Cem --> to computer vision (CV). \
On the one hand, architectural concepts firstly popularized in NLP are translated to CV [e.g., self-supervised learning or the Vision Transformer\; @ImageT <!-- TODO: reference to Vladana -->].
On the other hand, these powerful new NLP models, mostly Transformers [@vaswani2017attention], support bigger models form the inside as text encoder building blocks; hence the name of this chapter.
There are subsections on the recent and relevant CV models CLIP [@radford2021learning], ALIGN [@jia2021scaling] and Florence [@solawetz2021florenceopen] and an overview of some relevant core concepts.
Their strong performances confirm the potential, hinted at by the impressive GPT-3 [@brown2020language], of improving CV and increasing scale with the help of NLP.

### Concepts

#### Web-scale data {#webScaleData}

A core problem that troubles researchers is the lack of robustness of previous state-of-the-art CV models to distribution shifts.
I.e., when a model with good performance on its original dataset fails to generalize (transfer its knowledge) to new, more or less similar datasets.
E.g., @radford2021learning report that a ResNet101 which they trained on ImageNet to an accuracy of 76.2% maintains only an accuracy of 32.6% on ObjectNet.
This suggests that the model perhaps did not learn high quality latent representations, but instead overfit to the dataset-specific data-generating distribution. \
A common way to tackle this would be to try out various changes on the architecture and the training algorithm of the network.
But this kind of adaptation, inscribing expert knowledge into the model, seems to repeat the mistake pointed out by @sutton2019bitterlesson; "micromanaging" a model is likely to thwart future scaling.
<!-- E.g., introducing long short term memory to recurrent neural networks (RNN) improved them in the short term, but made them more computationally expensive, while not solving their core problem of poor parallelization, which resulted in it being outcompeted by the heavily parallelizable transformer architecture. | Does this really make sense?-->

The researchers of CLIP, ALIGN and Florence follow a different approach, based on scale.
They try to increase sample size as much as possible and work with tremendous numbers of observations:

* 400 million [CLIP\; @radford2021learning]
* 900 million [Florence\; @yuan2021florence]
* 1.8 billion [ALIGN\; @jia2021scaling]

These are achieved with the vast amount of image-text pairs produced by and readily available on the internet.
Thus, error prone, cost and labor intensive (difficult to scale), manual labeling is avoided. \
Unfortunately the models also become vulnerable to the <!-- kind of heavy -->downsides of web data.
Because of their extremely noisy nature, still some form of pre-processing is needed, e.g., filtering for English language, excluding graphic content and removing images with non-informative alt-texts.
This makes some degree of dataset curation, and therefore arbitrary choices, necessary. \
Likewise, the social biases inherent to the internet are reproduced and furthermore, while the approach improves data efficiency to some degree (see subsection \@ref(contrObj)), the poor performance of deep learning in this area is not substantially enhanced and mainly just compensated for with a super scalable source of supervision [@radford2021learning].

<!-- TODO: Other chapters discussing web-scale data? -->

#### Contrastive objective {#contrObj}

The information contained in the co-occurrence of the image with its alt-text is made available through the framework of natural language supervision.
The architectures use two jointly trained sub-networks to encode the image and the text, respectively.
The vector encodings are then aligned in the latent representation space by minimizing a variant of the contrastive loss function \@ref(eq:contrLoss) [@tian2020contrastive].

\begin{equation}
  \ell_1^{V_\text{img}, V_\text{txt}} = - \underset{\{v_\text{img}^1, v_\text{txt}^1, \ldots, v_\text{txt}^N\}}{\mathbb{E}} \left( \log \frac{h_\theta(\{v_\text{img}^1,v_\text{txt}^1\})}{h_\theta(\{v_\text{img}^1,v_\text{txt}^1\}) + \sum_{k=2}^N h_\theta(\{v_\text{img}^1, v_\text{txt}^k\})} \right)
  (\#eq:contrLoss)
\end{equation}

$\ell_1^{V_\text{img}, V_\text{txt}}$ is the first half of the loss for the first image-text pair, $v_\text{img}^1$ and $v_\text{txt}^1$ are latent representations of image 1 and text 1 and $h_\theta(\cdot)$ is a similarity measure.
In order to guarantee symmetry, the total loss is the sum of $\ell_1^{V_\text{img}, V_\text{txt}}$ and $\ell_1^{V_\text{txt}, V_\text{img}}$, where the pairwise similarity of one text and all images is averaged instead of the other way around.
<!-- Question: Is the second part really necessary? If all columns are optimized, all elements are considered. Maybe only if the similarity measure really is asymmetrical?-->

Figure \@ref(fig:contr-viz) visualizes this.
Initially all images and texts of a given dataset are encoded by the respective sub-networks.
Using the resulting latent vectors, a similarity matrix with elements $h_\theta(\{v_\text{img}^i,v_\text{txt}^j\})$ can be calculated.
Loosely speaking, the contrastive objective aims to maximize diagonal elements and minimize the rest.

```{r contr-viz, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:contr-viz)'}
knitr::include_graphics('figures/02-04-text-support-img/contrastive-pre-training.png')
```
(ref:contr-viz) Visualization of a contrastive objective [@radford2021learning].

Figure \@ref(fig:contr-vs-pred-learn) compares contrastive learning to classical predictive learning, giving another interesting insight.
The exemplary task is to color an image given its B/W version.
Approach (a) encodes the B/W image and in turn decodes its latent representation to fitting colors.
The goodness of this fit is measured in the output space. \
Conversely, approach (b) measures the loss in the representation space.^[Note that contrastive learning easily works with other combinations of modalities than text and image; here B/W and colors.]
A reason for the good performance of contrastive learning could be that while common prediction losses (e.g., the $\mathcal{L}_2$ loss) penalize each prediction output dimension independently, approach (b) implies measurement in the intertwined representation space [@tian2020contrastive].

```{r contr-vs-pred-learn, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:contr-vs-pred-learn)'}
knitr::include_graphics('figures/02-04-text-support-img/tian-predictive-vs-contrastive.png')
```
(ref:contr-vs-pred-learn) Predictive vs. contrastive learning: Predictive losses are measured in the output space while contrastive losses are measured is in the representation space; indicated by red dotted boxes [@tian2020contrastive].

But, rather than theoretical considerations, the driving factor for using this objective is data efficiency.
As can be seen in figure \@ref(fig:data-efficiency), @radford2021learning start their search for an adequate pre-train model (more on this in subsection \@ref(foundMod)) by experimenting with a transformer based language model predicting the exact captions of an image.
This turns out to train three times slower, in terms of data efficiency, compared to a simpler baseline predicting a bag-of-words text encoding.
Switching to the contrastive objective of CLIP additionally improves data efficiency by a factor of four. \
But, the switch to contrastive learning comes with some limitations.
Especially its rigidity demands certain extra steps and forfeits the high flexibility of generative models.
The end of the next subsection discusses this in more detail.

```{r data-efficiency, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:data-efficiency)'}
knitr::include_graphics('figures/02-04-text-support-img/data-efficiency.png')
```
(ref:data-efficiency) Data efficiency of contrastive objective.
Development of zero-shot accuracy on ImageNet with increasing number of instances of training data seen by models.
The contrastive objective reaches same accuracies compared to the exact predictive objective while seeing only a seventh of the amount of data [@radford2021learning].

<!-- TODO: Contra: No longer a generative model, e.g., no flexible caption generation, see CLIP Limitations -->
<!-- TODO: Learn a representation _and_ connect it to language (-> NLP) -->

#### Foundation models and zero-shooting {#foundMod}

The first models, which are considered foundation models today, began to appear in NLP.
The term, coined by @bommasani2021opportunities later, refers to models that are noteworthy due to their large scale and ability to adapt to a wide variety of downstream tasks.
An early example would be BERT [@Devlin2018]. \
A lot of times foundation models have an unfinished touch to them and the true scope of their capabilities remains unknown.
This is because often times their desired abilities are not designed for explicitly, but emerge from their setup.
@bommasani2021opportunities cite GPT-3's ability to perform certain types of new tasks soley by describing to it what is wanted in natural language, called in-context learning, which just happened, so to speak [@brown2020language]. \
So, one can think of plenty of ways to employ these models and it never can be known if there is another one no one thought of yet.
While this could mean saving computation and data collection costs down the line, the same is valid for malicious use cases, e.g., surveillance.

The quintessential concept for foundation models is transfer-learning; pre-training a model on one task, which optimally is easy to do, and applying it downstream to the desired task.
For the models of this chapter this means pre-training on web-scale data (see subsection \@ref(webScaleData)) and evaluating performance on various common classification datasets.
@radford2021learning name the SVHN dataset as a proxy for the task "street number transcription" with the caveat "on the distribution of Google Street View photos" as an example, but they remark that a lot of datasets have no obvious, specific task associated, e.g., CIFAR-10.
On these kind of datasets they rather measure "robustness to distribution shift and domain generation", which still is of great interest like mentioned in subsection \@ref(webScaleData). \
When there is no further fine-tuning on the downstream task, i.e., no resuming of training on the new dataset, this is called zero-shooting.
<!-- TODO: Name some datasets and their associated tasks. -->
It has the clear advantage of evaluating performance more unbiased, as processes like overfitting to the data-generating distribution will not distort results. \
Figure \@ref(fig:zero-shooting) shows how contrastive models perform zero-shot transfer.
For image classification all available classes are encoded by the language model.
Afterwards the encoding of the image to be classified is computed by the CV sub-network and all similarity scores are returned, where the highest one can be retrieved as the decision. \
Image retrieval works the other way around:
After an initial encoding of all images, the closest ones to an encoded natural language text prompt in the representation space can be returned.

```{r zero-shooting, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:zero-shooting)'}
knitr::include_graphics('figures/02-04-text-support-img/zero-shooting.png')
```
(ref:zero-shooting) Visualization of zero-shooting [@radford2021learning].

### Architectures

#### CLIP

The first large scale contrastive CV model was CLIP, short for Contrastive Language-Image Pre-training [@radford2021learning].
The components of its name are explained in previous subsections \@ref(contrObj), \@ref(webScaleData) and \@ref(foundMod), respectively and are the defining concepts for ALIGN and Florence as well. \
CLIP is a product of OpenAI, but its code is freely available and the different versions can be accessed as python modules.
Not released is the corresponding dataset.

A lot of preliminary work is done by @zhang2020contrastive, whose paper firstly uses contrastive representation learning with image-text pairs.
CLIP version of the contrastive loss function \@ref(eq:contrLoss) is

\begin{equation}
  \ell_1^{V_\text{img}, V_\text{txt}} = - \log \frac{\exp(\langle v_\text{img}^1, v_\text{txt}^1 \rangle / \tau)}{\sum_{k=1}^{N} \exp(\langle v_\text{img}^1, v_\text{txt}^k \rangle / \tau)},
  (\#eq:contrLossCLIP)
\end{equation}

where $\langle v_\text{img}^1, v_\text{txt}^1 \rangle$ represents the cosine similarity, i.e., $v_\text{img}^{1 \top} v_\text{txt}^1 / (\|v_\text{img}^1\| \|v_\text{txt}^1\|)$, and $\tau \in \mathbb{R}^+$ represents a temperature parameter, which is directly learned during training [@zhang2020contrastive].
$\ell_1^{V_\text{txt}, V_\text{img}}$, the counterpart to $\ell_1^{V_\text{img}, V_\text{txt}}$ in the total loss, is function \@ref(eq:contrLossCLIP) with switched arguments.
This can be viewed as a symmetric cross entropy loss over the cosine similarity of the embeddings [@radford2021learning].

**Architecture**

For CLIP the text encoder shown in figure \@ref(fig:contr-vs-pred-learn) is a Transformer [@vaswani2017attention] with the same modifications as for GPT-2 [@radford2019language], while multiple sub-networks were tried out for the image encoder:

* ResNets: ResNet-50, ResNet-101
* ResNets which follow EfficientNet-style model scaling: RN50x4, RN50x16, RN50x64
* Vision Transformers: ViT-B/32, ViT-B/16, ViT-L/14

The best performing sub-network was the ViT-L/14, for which in turn they continued training for an additional epoch with higher resolution images (336px), giving it the name ViT-L/14@336px.
If not indicated otherwise the performance was measured on this version of CLIP.
The EfficientNet-style ResNets use x4, x16 and x64 of the compute of a ResNet-50 and the largest model (the RN50x64) trained for 18 days on 592 V100 GPUs, while the ViT-L/14 only took 12 days on 256 GPUs.
The high capabilities in terms of parallelization of transformers seems to pay off.

When explaining zero-shooting (see subsection \@ref(foundMod)) initially, a part was skipped.
As can be seen in figure \@ref(fig:zero-shooting) there is an additional step before feeding image labels into the text encoder.
In order to enhance performance the prompts are further engineered, which seemingly is necessary for the model to understand the context of the words.
For this, the class labels are embedded in a sentence, e.g., "A photo of a {label}.", which increases ImageNet accuracy by 1.3 percentage points (pp).
When ensembling 80 different context prompts^[Prompts like: "A photo of a big {label}.", "A photo of a small {label}." [@radford2021learning]] ImageNet accuracy is improved by an additional 3.5pp, adding up to nearly 5pp.
The average gain across 36 datasets is reported to be 5pp. \
Another interesting aspect arising out of the connection of image and text representations is the ability to directly communicate visual concepts like "picture", "macro", "drawing" or even "dog" to the model.

**Performance**

Figure \@ref(fig:performance-clip) compares the performance of CLIP and a ResNet101 trained on ImageNet to the same accuracy as zero-shot CLIP.
The results of @radford2021learning show that CLIP constitutes an important step towards closing the robustness gap mentioned in subsection \@ref(webScaleData):
With datasets coming from more and more differing data-generating distributions to ImageNet the performance of ResNet101 deteriorates while CLIP remains fairly accurate. \
But these findings have to be taken with a grain of salt.
While @radford2021learning devote a subsection to data overlap analysis between training and zero-shot data, they do not give access their dataset, which makes their claims impossible to investigate by independent third parties.

```{r performance-clip, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:performance-clip)'}
knitr::include_graphics('figures/02-04-text-support-img/performance-clip.png')
```
(ref:performance-clip) Robustness of zero-shot CLIP to distribution shifts [@radford2021learning].

@shen2021much discover another curios fact.
They study performance improvements in Vision-and-Language models when the visual encoder is switched to CLIPs strong image encoder and discover that in this field of CV the ViT-B scores worse than the ResNets.
E.g., tests on image captioning reveal that the V&L model using ViT-B often is only half as good as the version using the RN50x4 -- the largest one used in this study. \
They propose that ViT-B lacks visual localization abilities due to its pooling strategies.
To test this @shen2021much perform some test which confirm their supposition, e.g., figure \@ref(fig:attention-ViT) which depicts a Grad-CAM Visualization of a V&L model with a ResNet-50 backbone and a ViT-B backbone for the question "What color is the woman's shirt on the left?".

```{r attention-ViT, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:attention-ViT)'}
knitr::include_graphics('figures/02-04-text-support-img/attention-of-ViT.png')
```
(ref:attention-ViT) Attention.

#### ALIGN

@jia2021scaling take an approach which is a little different.
They reiterate the necessity of large-scale vision datasets and assert that even CLIPs data collection process still involves a non-trivial data curation cost.
For this they explore employing a minimal amount of filtering and controlling the noise simply via sample size.
This results in a dataset with 1.8 billion image-text pairs.
They name their model ALIGN, short for "A Large-scale ImaGe and Noisy-text embedding", hinting at the contrastive loss, which aligns vector encodings in the representation space (see subsection \@ref(contrObj)).

**Architecture**

They use the dual encoder architecture with BERT-Large as the text and EfficientNet-L2 as the image encoder, which they train from scratch.
The model has around 800 million parameters [@alford2021alignparams].

They also mention a funny property of the access to aligned image and text encoders.
One can perform simple arithmetic operations, which they use for image + text retrieval.
E.g., encoding an image of the Eiffel tower and the word "snow" and adding up the vector encodings can retrieve a picture of a snowy Eiffel tower using cosine similarity, see firgure \@ref(fig:img-txt-addition).

```{r img-txt-addition, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:img-txt-addition)'}
knitr::include_graphics('figures/02-04-text-support-img/align-word-and-image-addition.png')
```
(ref:img-txt-addition) Addition of word and image embedding.

**Diskussion**

* more expensive training

#### Florence

<!-- Intro -->
<!-- Object detection vs scene level -->
* More fine-grained, dynamic, multimodal representations
* Focus shift to finding _foundation model_ as CLIP turned out to be especially useful for that.
  * Pre-trained core
  * Flexible addition of modules
    * _Dynamic Head_ for object detection - citations coming later
    * _METER_ as a adapter for vision-language (e.g., visual question answering)
    * Adaptation to video recognition through _CoSwin_
* General trend in this direction, better and better predictions [CoCa\; @yu2022coca]
* Optimization inside image-label-description space
* Encoders
  * Uses CLIP pendant as the language encoder
  * Swin transformer as the image encoder
  * CoSwin for embedding

### Performance comparison

* As all of these models are orders of magnitudes too large for performing a benchmark, findings reported inside the papers are believed here

```{r table1, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:table1)'}
knitr::include_graphics('figures/02-04-text-support-img/table-imagenet.png')
```
(ref:table1) Top-1 Accuracy of zero-shot transfer of models to image classification on ImageNet and its variants.


```{r table2, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:table2)'}
knitr::include_graphics('figures/02-04-text-support-img/table-img-txt-retrieval.png')
```
(ref:table2) Zero-shot image and text retrieval [@yuan2021florence].

```{r table3, echo=FALSE, out.width='100%', fig.align='center', fig.cap='(ref:table3)'}
knitr::include_graphics('figures/02-04-text-support-img/table-misc-datasets.png')
```
(ref:table3) Top-1 Accuracy of CLIP, Florence and ALIGN on various datasets.

### Resources

One can find the pre-trained CLIP models on [Github](https://github.com/openai/CLIP).
They even found their way into simple command line tools already.
For example there is a CLI named [rclip](https://github.com/yurijmikhalevich/rclip), which can be used for personal image retrieval, wrapping the _ViT-B/32_ CLIP architecture.
On my (mid-range) laptop I was able to find seemingly good matches for search terms tried out inside a folder with about 100 pictures.
After an initial caching one request took about ten seconds.
<!-- Assumption that all data is in-distribution -->

### Outlook

CLIP is used inside DALL$\cdot$E 2 to embed images [@ramesh2022hierarchical].
Another application of CLIP is the creation of the LAION-400M dataset [@schuhmann2022laion].
To validate collected image-text pairs their similarity scores are calculated using CLIP and instances with a value too low are discarded.
<!-- CLASP -->

