# Strucutered + Unstrucutered Data

*Author: Rickmer Schulte*

*Supervisor: Daniel Schalk*

## Intro

While the previous chapter has extended the range of modalities considered in multimodal deep learning beyond image and text data, the focus remained on other sorts of unstructured data. This has neglected the broad class of structured data, which has been the basis for research in pre deep learning eras and which has given rise to many fundamental modeling approaches in statistics and classical machine learning. Hence, the following chapter will aim to give an overview of both data sources and outline the respective ways these have been used for modeling purposes as well as more recent attempts to model them jointly. 

Generally, structured and unstructured data substantially differ in certain aspects such as dimensionality and interpretability which have led to various modeling approaches that are particularly designed for the special characteristics of the data types, respectively. As shown in previous chapters, deep learning models such as neural networks are known to work well on unstructured data due to their ability to extract latent representation and to learn complex dependencies from unstructured data sources to achieve state-of-the art performance on many classification and prediction tasks. By contrast, classical statistical models are mostly applied to tabular data due the advantage of interpretability inherent to these models, which is commonly of great interest in many research fields. However, as more and more data has become available to researchers today, they often do not only have one sort of data modality at hand but both structured and unstructured data at the same time. Discarding one or the other data modality makes it likely to miss out on valuable insights and potential performance improvements.

Therefore, the following chapter will mainly investigate different proposed methods to model both data types jointly and examine similarities and differences between those. Besides classical methods such as feature engineering to integrate unstructured data via expert knowledge into the classical model framework, end-to-end learning techniques as well as different fusion procedures to integrate both types of modalities into common deep learning architectures are analyzed and evaluated. Especially the latter will be explored in detail by referring to numerous examples from survival analysis, finance and economics. 
Finally, the chapter will conclude with a critical assessment of recent research for combining structured and unstructured data in multimodal DL, highlighting lacking steps that are required by following research as well as giving an outlook on future developments in the field.


## Taxonomy: Structured vs. Unstructured Data

In order to have a clear setup for the remaining chapter, we will start off with a brief taxonomy of data types that will be encountered. Structured data, normally stored in some tabular form, has been the main research object in classical scientific fields. Whenever there was unstructured data involved, this was normally transformed into a structured form in a informed manner. Typically, this was done via expert-knowledge or data reduction techniques such as PCA prior to further statistical analysis. However, DL has enabled unsupervised extraction of features from unstructured data and thus to incorporate this kind of data in the models directly. Classical examples of unstructured data are image, text, video, and audio data as shown in Figure 1. Of these, the use of image and textual data together with tabular data will be examined along various examples later in the chapter. While the previous data types allowed for a clear distinction, lines can become increasingly blurred. For example, the record of few selected biomarkers or genes from patients would be regarded as structured data and normally be analyzed with classical statistical models. On the contrary, having the records of multiple thousand biomarkers or genes would rather be regarded as unstructured data and usually be analyzed via DL techniques. Thus, the distinction between structured and unstructured data does not only follow along the line of dimensionality but also concerns regarding the interpretability of single features within the data. 

<Figure1: Structured vs. Unstructured Data >


## Fusion Strategies

After we have classified the different data types that we will be dealing with, we will now proceed with fusion strategies that are used to merge data modalities into a single model. While there are potentially many ways to fuse data modalities, a distinction between three different strategies, namely early, joint and late fusion has been made in the literature. Here we follow along the taxonomy laid out by Huang et al. (2020) with a few generalizations as this is sufficient for our context. 

**Early fusion** refers to the procedure of merging data modalities into a feature vector prior to feeding it into the model. The data that is being fused can be raw or preprocessed data. The step of preprocessing usually involves dimensionality reduction to algin dimensions of the model input data. This can be done by training a separate DNN, using data driven transformations such as PCA or directly via expert knowledge. Besides using domain expertise to feed only regions of interest of e.g an image to the model, sampling from these regions is another common approach to further decrease dimensionality.

**Joint fusion** offers the flexibility to merge the modalities at different depths of the model and thereby to learn feature representations from the input data (within the model) before fusing the different modalities into a common layer. Thus, the important difference to early fusion is that latent feature representation learning is not separated from the subsequent model. Allowing the loss to backpropagate to the process of feature extraction from raw data. This process is also called end-to-end learning. Depending on the task, CNNs or LSTMs are usually acquired to learn latent feature representations. As depicted in Figure 2, learning lower dimensional feature representations does not have to be applied to all modalities and is often not done for structured data. A further distinction between models can be made regarding their model head, which can be a FCNN or a classical statistical model (linear, logistic, GAM). While the former can be desirable to capture possible interactions between modalities, the latter is frequently used as it preserves interpretability.

**Late fusion** or sometimes also called decision level fusion is the procedure of fusing the predictions of multiple models that have been trained on each modality separately. The idea originates from ensemble classifiers, where each model is assumed to inform the final prediction separately. Outcomes from the models can be aggregated in various ways such as averaging or majority voting.

We will refer to numerous examples of both early and joint fusion in the following sections. While the former two are frequently applied and easily comparable, late fusion is less common and different in nature and thus not further investigated here.

<Figure2: Data modality fusion strategies>

## Applications

The following section will discuss various examples of this specific kind of multimodal DL by referring to different publications and their proposed methods. The publications originate from very different scientific fields, which is why methods are target for their respective use case. Hence, allowing the reader to follow along the development of methods as well as the progress in the field of multimodal DL (Struc. + Unstruc.) and obtaining a good overview of current and potential areas of applications. As there are various publications related to this kind of multimodal DL, the investigation is narrowed down to publications which either introduce new methodical approaches or did pioneering work in their field by facilitating multimodal DL. 

## Multimodal DL in Survival

Especially in the field of survival analysis, many interesting ideas were proposed with regards to multimodal deep learning which also incorporate structured data. While clinical patient data such as electronic health records (EHR) were traditionally used for modelling risks and hazards in survival analysis, recent research has started to incorporate image data such as body scans and other modalities such as gene expression data in the modelling framework. Before examining these procedures in detail, we will briefly revisit the classical modelling setup of survival analysis by referring to the well-known Cox Proportional Hazard Model (CPH).

### Traditional Survival Analysis (CPH Model)
Survival Analysis generally studies the time duration until a certain event occurs. While many methods have been developed to analyse the effect
of certain variables on the survival time, the Cox Proportional Hazard Model (CPH) remains the most prominent one. The CPH models the hazard rate, which is the probability of a certain event occurring in the next moment given it has not so far:
$$
h(t|x) = h_0(t) * e^{x\beta}
$$
where $h_0(t)$ denotes the baseline hazard rate and $x\beta$ the linear effects of the covariates. The fundamental
assumption underlying the traditional CPH is that covariates influence the hazard rate proportionally and multiplicatively. This stems from the fact that the effects in the so called risk function $f(x) = x\beta$ are assumed to be linear. Although, this has the advantage of being easily interpretable, it does the limit the flexibility of the model and thus to capture the full dynamics at hand. 

### Multimodal DL Survival Analysis
Overcoming the limitations of the classical CPH model, @Katzman_2018 were among the first to incorporate neural networks into the CPH and thereby replacing the linear effect assumption. While this helped to capture interactions and non-linearities of covariates, it only allowed modelling of structured data. Giving rise to the idea of DeepConvSurv proposed by ref_DeepConvSurv, which uses CNNs to predict risk of patients using pathological image data pathological only. Learning features form images via CNNs in an end-to-end fashion showed to outperform hand-crafted features regarding the survival prediction. Building on the idea of DeepConvSurv, the authors extend the model by adding further modalities. Besides pathological images, their proposed DeepCorrSurv model also includes molecular data of cancer patients. The name of the model stems from the fact, that separate subnetworks are applied to each modality and that the correlation between the output of these modality specific subnetworks are maximized before fine-tuning the learned feature embedding to perform well on the survival task. With this procedure the authors aim to remove the discrepancy between modalities and argue that the procedure is beneficial in small sample settings as it may reduce the impact of noise inherent to a single modality that is unrelated to the survival prediction task. 

The idea that the different modalities of multimodal data may contain both complementary and as well as common information shared by all modalities was quite far reaching and also adopted by other researchers. ref_tong_etal for example introduced the usage of auto encoders (AE) in this context, by proposing models that extract the lower dimensional hidden features of the AE applied to each modality. While their first model trains AEs on each modality separately before concatenating the learned features (ConcatAE), their second model obtains cross-modality AEs that are trained to recover both modalities from each modality respectively (CrossAE). The concept of complementary information of modalities informing survival prediction separately gives rise to the first model, whereas the concept of retrieving common information inherent across modalities gives rise to the latter. Although, theoretically both models could also handle  classical tabular EHR data, they were only applied to multi-omics data such as gene expressions of breast cancer patients and are therefore less comparable with the others models, which is why they are not further investigated here.

Similar to Tong et al. (2020), Cheerla and Gevaert (2019) also derive the setup of their model from the idea of common information that is shared by all modalities. Besides, having specialized subnetworks for each modality to learn latent feature embeddings they also introduce a similarity loss that is added to the classical cox loss from the survival prediction. This similarity loss aims at learning modality invariant latent feature embeddings, which are disirable not only in the light of noise reduction but also in cases of missing data. While previous research often applied their models only on subsets of the large cancer genome atlas program (TCGA), Cheerla and Gevaert (2019) analyze 20 different cancer types of the TCGA using four different data modalities. As this expands the problem of data missigness, they specifically target the problem by introducing a variation of regular dropout, which they refer to as multimodal dropout. Hereby, they drop entire modalities during training in order to make models less dependent on one single data source and to better handle missing data during inference time. Opposed to Tong et al. (2020), the model is trained in an end-to-end manner and thus enables the survival loss to guide the latent feature learning. More impressive than the overall prediction performance were the results of T-SNE-mappings that were obtained from the learned latent feature embeddings. One sample mapping is displayed in Figure X and clearly shows the clustering of patients with regards to cancer types although the model was not trained on this variable. Besides, being useful for accurate survival prediction, such feature mappings can directly be used for patient profiling and are thus an contribution to the research itself. 

<Figure 3.1: Architecture with Similarity Loss> <Figure 3.2: T-SNE-mapped representations of feature vectors>

(Vale-Silva and Rohr, 2021) extend the previous work by utilizing up to six different data modalities and 33 cancer types of the TCGA dataset. For their model MultiSurv they proposed a straightforward architecture, which applies separate subnetworks to each modality and has another FCNN as model head. Testing their modular model on different combinations of data inputs, they find the best model performance for the combination of clinical and mRNA data, while including further modalities lead to slight performance reductions. Although, the best performing model (clinical+mRNA) outperformed all single modality models, the largest model which included all six modalities was not able to beat the classical CPH model based on clinical data only. Additionally, high variability of performance for different cancer types is found, which seems unsurprising, incorporating the fact that tissue appearance differ vastly between cancer types and that for some of these only up to 20 samples were present in the data. Although, state-of-the-art performance is claimed by the others, the before mentioned aspects do raise concerns regarding the robustness of the results. Besides, facing serious data quantity issue for some cancer types, results could be driven simply by the analysis setup of testing the model repeatedly on different combinations of data modalities. Additionally, the study nicely showcases that the most relevant information can often be retrieved from classical structured clinical data and that including further modalities can by contrast even distort model training when sample sizes are low compared to the variability within the data. While these concerns could certainly have been raised for the other studies as well, they simply became more apparent in (Vale-Silva and Rohr, 2021) due their comprehensive and transparent analysis.

Lastly we will discuss survival models, that incorporate the idea of Wide & Deep NN (WDNN). Wide & Deep NN were first introduced by ref_chengetal and constitute that data inputs are not only feed to either a linear or FCNN model part, but both at the same time. The initial assumption was that models need to be able to memorize as well as generalize and that these aspects could be covered by the linear and FCNN part, respectively. 

<Figure 4: Wide & Deep NN>

The idea of Wide & Deep NN were applied in the context of multimodal DL survival by (Pölsterl et al., 2019) and (Kopper et al., 2022). Similar to previous studies (Pölsterl et al., 2019) used them to overcome the linearity assumption in the CPH model as described above. By contrast, (Kopper et al., 2022) applied them on piecewise exponential additive mixed model (PAMM) in order to overcome not only the linearity but also the proportionality constraint in the classical CPH. By dropping the proportionality assumption, the models yields piecewise constant hazard rates for different time intervals. Particularly interesting is the additive structure in the last layer of both models as the linear model head preserves interpretability. This of great value in most research applications, as often not only the final prediction but also the contribution of single variables to that prediction is of interest.

Although, Wide & Deep NN are advantageous due to their flexibility and interpretability, special care needs to be take regarding the possible feature overlap between the linear and NN part as it can dramatically impact the interpretability. Considering the case that a certain feature $x$ is fed to the linear as well as the FCNN model part. Because of the universal approximation property of neural networks, the FCNN part could potentially model any arbitrary relation between the dependent and independent variable ($d(x)$). This however raises an identifiablity issue as the coefficients ($\beta$) of the linear part could theoretically be altered arbitrarily ($\widetilde{\beta}$) without changing the overall prediction when the weights of the NN ($\widetilde{d}(x)$) are adjusted accordingly.


$$
x\beta + d(x) = x\widetilde{\beta} + d(x) + f(x) = x\widetilde{\beta} + \widetilde{d}(x)
$$
Generally, there are two ways to deal with this identifiablity problem. The first possibility would be to apply a two-stage procedure by first estimating only the linear effects and then applying the DL model part on the obtained residuals. An alternative would be to incorporate orthogonalization within the model, thereby allowing for efficient end-to-end training. The latter was proposed by (Rügamer, Kolb and Klein, 2020) and utilized in the DeepPAMM model by (Kopper et al., 2022). The next section will go in more detail about the two possibilities to solve the described identifiablity issue and will discuss further applications of multimodal DL in other scientific fields. 

### Multimodal DL in Other Scientific Fields

After having seen multiple applications of multimodal DL in survival analysis which predominately occur in the biomedical context, we will now extend the scope of the chapter by discussing further applications of multimodal DL related to the field of economics and finance. While structured data has traditionally been the main studied data source in these fields, recent research has not only focused on combining both structured and unstrucutered data, but also on ways to replace costly collected and sometimes scarce structured data with freely available and up-to-date unstructured data using remote sensing data. Before discussing these approaches, we will first go in more detail about the above mentioned method proposed by (Rügamer, Kolb and Klein, 2020).

As previous research exclusively focused on mean prediction, uncertainty quantification has often received less attention. (Rügamer, Kolb and Klein, 2020) approach this by extending structured additive distributional regression (SADR) to the DL context. Instead of learning a single parameter e.g. the mean, SADR offers the flexibility to directly learn multiple distributional parameters and thereby natively providing uncertainty quantification. It is nevertheless possible to just model the mean of the distribution, which is why SADR can be regraded as generalization of classical mean prediction. (Rügamer, Kolb and Klein, 2020) then proposed a model that models the distributional parameter as a function of covariates via a linear, generalized additive (GAM) or NN model. All distributional parameters are resembled in a distributional layer (output layer) and as the linear and GAM channels are directly connected to this output layer, interpretability of the coefficients is preserved. An illustration of their so called Semi-Structured Deep Distributional (SSDDR) is given in the figure below. 

<Figure 5: Architecture of SSDDR>

If the mean is now modeled by a linear and DNN part and the same feature inputs are fed to both model parts, we are in the setting of Wide & Deep NN. As illustrated above, such feature overlaps give rise to a identifiablity problem. To overcome this issue (Rügamer, Kolb and Klein, 2020) key idea was to integrate an orthogonalization cell in the model, that orthogonalizes the latent features of deep network with respect to the coefficients of the linear and GAM model if feature overlaps are present. In case $\boldsymbol{X}$ contains the inputs, that are part of the feature overlap, the projection matrix $\boldsymbol{\mathcal{P}^{\perp}}$ projects into the respective orthogonal complement of the linear projection onto the column space spanned by $\boldsymbol{X}$. This allows backpropagation of the loss through the orthogonalization cell and therefore end-to-end learning.

Another way of orthogonalizing feature representations is by applying a two-stage procedure as described above. (Law, Paige and Russell, 2019) utilizes this procedure to make their latent feature representations retrieved from unstructured data orthogonal to their linear effect estimates from structured data. More specifically, they try to accurately predict house prices in London using multiodal DL on street and aerial view images as well as tabular housing attributes. And therefore aiming at learning latent feature representations from the images that extract only features which are orthogonal to the housing attributes. Conducting a series of experiments, they find that including image data next to the tabular housing data does improve the prediction performance albeit structured data remaining the most relevant data source. Additionally, they tested models with different model heads. Although, a DNN as model head offers more modelling flexibility it showed only slight performance gains over the additive linear model head, which preserves the often desirable interpretability. They therefore argue that it is indeed possible to generate interpretable models without losing too much on the performance side.

In the last part of this section, we will allude to several other promising approaches that make use of unstructured data sources such as remote sensing data. Although, structured data is not included in all of the proposed models, they are still covered in this chapter to give the reader a broad overview of current research in the field. Moreover, structured data could easily be added to each of these models, but often studies specifically avoid the use of this data source as it is sometimes scarcely available due to the cost of data collection. Besides availability, strucutered data such as household surveys are unregularly conducted and often differ vastly between countries, making large scale studies impossible. Therefore, different studies tried to provide alternatives to classical surveys by applying DL methods on freely available unstructured data sources. While (Jean et al., 2016) use night and daylight satellite images to predict poverty in several African countries, (Gebru et al., 2017) use Google street view images to estimate socioeconomic attributes in the US. The results of (You et al., 2017) which use deep Gaussian processes to predict corn yield or the data sources created by (Sirko et al., 2021) which map buildings in many African countries from remote sensing data using DL, could easily be combined with other structured data sources and thereby constitute another form of multimodal DL.

### Critical Assessment

### Conclusion and Outlook
-Achievements: Different ways to incorporate multi modal data using DL
-General Observations:
tabular data often carries the most important information (noisy image data and small sample size)
end-to-end learning may improve performance of predictions
Joint fusion with head classical statistical model can preserve interpretability
-Major challenges: Small sample sizes particularly for images from patients,
making it hard for DL to extract valuable information from images so that structured data sources mostly carry the most relevant information,
insufficient benchmarking between proposed models as well as with the most important benchmark of single modality models (especially tabular data only models), DL has many tunable parameters, which makes it easy to achieve small improvements for some configurations
not clear on which data and which fields multi-modal works best, not clear which DL archtiectures as well as fusion strategies work best (joint fusion with interpretable or NN as head)
strong publication bias
-Outlook: Do we need multi-modal deep learning in regular scientific context (outside classical Computer vision tasks) where good and interpretable structured data is available? - In current setup it might seem questionable, but with increasing data sizes 
However: Missing Data might be more easily handled if different data sources contain not only complementary but also consensus information

