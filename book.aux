\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{apalike}
\HyPL@Entry{0<</S/r>>}
\@writefile{toc}{\contentsline {fm}{Preface}{v}{chapter*.1}\protected@file@percent }
\newlabel{preface}{{}{v}{Preface}{chapter*.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Creative Commons License\relax }}{v}{figure.caption.2}\protected@file@percent }
\HyPL@Entry{6<</S/D>>}
\@writefile{toc}{\contentsline {fm}{Foreword}{1}{chapter*.3}\protected@file@percent }
\newlabel{foreword}{{}{1}{Foreword}{chapter*.3}{}}
\newlabel{citation}{{}{1}{Citation}{section*.4}{}}
\citation{rlang}
\newlabel{technical-setup}{{}{2}{Technical Setup}{section*.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{3}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction to Multimodal Deep Learning}{3}{section.1.1}\protected@file@percent }
\newlabel{introduction-to-multimodal-deep-learning}{{1.1}{3}{Introduction to Multimodal Deep Learning}{section.1.1}{}}
\citation{BERT}
\citation{attention}
\citation{ResNet}
\citation{EfficientNet}
\citation{SimCLR}
\citation{BYOL}
\citation{COCO}
\citation{meshed_memory}
\citation{GAN}
\citation{VAE}
\citation{DALLE}
\citation{GLIDE}
\citation{CLIP}
\citation{ALIGN}
\citation{yuan2021florence}
\citation{CLIP}
\citation{VilBert}
\citation{Flamingo}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Outline of the Booklet}{4}{section.1.2}\protected@file@percent }
\newlabel{outline-of-the-booklet}{{1.2}{4}{Outline of the Booklet}{section.1.2}{}}
\citation{Pathways}
\citation{DALLE}
\citation{Mikolov2013}
\citation{Bojanowski2016}
\citation{Sutskever2014}
\citation{Bahdanau2014}
\citation{vaswani2017attention}
\citation{Devlin2018}
\citation{Raffel2019}
\citation{brown2020language}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Introducing the modalities}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{c01-00-intro-modalities}{{2}{7}{Introducing the modalities}{chapter.2}{}}
\citation{ResNet}
\citation{EfficientNet}
\citation{SimCLR}
\citation{SwAV}
\citation{BYOL}
\citation{ImageT}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}State-of-the-art in NLP}{9}{section.2.1}\protected@file@percent }
\newlabel{c01-01-sota-nlp}{{2.1}{9}{State-of-the-art in NLP}{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}State-of-the-art in Computer Vision}{9}{section.2.2}\protected@file@percent }
\newlabel{c01-02-sota-cv}{{2.2}{9}{State-of-the-art in Computer Vision}{section.2.2}{}}
\citation{history1}
\citation{history2}
\citation{alexnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}History}{10}{subsection.2.2.1}\protected@file@percent }
\newlabel{history}{{2.2.1}{10}{History}{subsection.2.2.1}{}}
\citation{supervised}
\citation{unsupervised}
\citation{selfsup}
\citation{selfsup2}
\citation{contrastive}
\citation{resnet}
\citation{width}
\citation{gpipe}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Supervised and unsupervised learning}{11}{subsection.2.2.2}\protected@file@percent }
\newlabel{supervised-and-unsupervised-learning}{{2.2.2}{11}{Supervised and unsupervised learning}{subsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Scaling networks}{11}{subsection.2.2.3}\protected@file@percent }
\newlabel{scaling-networks}{{2.2.3}{11}{Scaling networks}{subsection.2.2.3}{}}
\citation{esnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Deep residual networks}{12}{subsection.2.2.4}\protected@file@percent }
\newlabel{deep-residual-networks}{{2.2.4}{12}{Deep residual networks}{subsection.2.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4.1}Deep Residual Learning}{12}{subsubsection.2.2.4.1}\protected@file@percent }
\newlabel{deep-residual-learning}{{2.2.4.1}{12}{Deep Residual Learning}{subsubsection.2.2.4.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.4.1.1}Residual Learning}{12}{paragraph.2.2.4.1.1}\protected@file@percent }
\newlabel{residual-learning}{{2.2.4.1.1}{12}{Residual Learning}{paragraph.2.2.4.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.4.1.2}Identity Mapping by Shortcuts}{13}{paragraph.2.2.4.1.2}\protected@file@percent }
\newlabel{identity-mapping-by-shortcuts}{{2.2.4.1.2}{13}{Identity Mapping by Shortcuts}{paragraph.2.2.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Figure 1. Building block of residual learning\relax }}{13}{figure.caption.7}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ch01-figure01}{{2.1}{13}{Figure 1. Building block of residual learning\relax }{figure.caption.7}{}}
\citation{vgg}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4.2}Network Architectures}{14}{subsubsection.2.2.4.2}\protected@file@percent }
\newlabel{network-architectures}{{2.2.4.2}{14}{Network Architectures}{subsubsection.2.2.4.2}{}}
\citation{effecient}
\citation{depthwidth}
\citation{mobilenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}EfficientNet}{15}{subsection.2.2.5}\protected@file@percent }
\newlabel{efficientnet}{{2.2.5}{15}{EfficientNet}{subsection.2.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.1}Compound Model Scaling}{15}{subsubsection.2.2.5.1}\protected@file@percent }
\newlabel{compound-model-scaling}{{2.2.5.1}{15}{Compound Model Scaling}{subsubsection.2.2.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.5.1.1}Problem Formulation}{15}{paragraph.2.2.5.1.1}\protected@file@percent }
\newlabel{problem-formulation}{{2.2.5.1.1}{15}{Problem Formulation}{paragraph.2.2.5.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Figure 2. Architecture of ResNet\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{fig:ch01-figure02}{{2.2}{16}{Figure 2. Architecture of ResNet\relax }{figure.caption.9}{}}
\citation{gpipe}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Figure 3. Model scaling\relax }}{17}{figure.caption.10}\protected@file@percent }
\newlabel{fig:ch01-figure03}{{2.3}{17}{Figure 3. Model scaling\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.5.1.2}Scaling Dimensions}{17}{paragraph.2.2.5.1.2}\protected@file@percent }
\newlabel{scaling-dimensions}{{2.2.5.1.2}{17}{Scaling Dimensions}{paragraph.2.2.5.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.5.1.3}Compound Scaling}{18}{paragraph.2.2.5.1.3}\protected@file@percent }
\newlabel{compound-scaling}{{2.2.5.1.3}{18}{Compound Scaling}{paragraph.2.2.5.1.3}{}}
\gdef \LT@i {\LT@entry 
    {2}{72.21002pt}\LT@entry 
    {1}{103.47002pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5.2}EfficientNet Architecture}{19}{subsubsection.2.2.5.2}\protected@file@percent }
\newlabel{efficientnet-architecture}{{2.2.5.2}{19}{EfficientNet Architecture}{subsubsection.2.2.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Results and comparison of the networks}{19}{subsection.2.2.6}\protected@file@percent }
\newlabel{results-and-comparison-of-the-networks}{{2.2.6}{19}{Results and comparison of the networks}{subsection.2.2.6}{}}
\gdef \LT@ii {\LT@entry 
    {2}{60.73003pt}\LT@entry 
    {1}{52.77002pt}\LT@entry 
    {1}{47.77002pt}}
\gdef \LT@iii {\LT@entry 
    {2}{132.94003pt}\LT@entry 
    {1}{52.77002pt}\LT@entry 
    {1}{47.77002pt}}
\citation{simclr}
\citation{byol}
\citation{NNCLR}
\citation{ORE}
\citation{SwAV}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Contrastive learning}{20}{subsection.2.2.7}\protected@file@percent }
\newlabel{contrastive-learning}{{2.2.7}{20}{Contrastive learning}{subsection.2.2.7}{}}
\citation{simclr}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}A Simple Framework for Contrastive Learning of Visual Representations}{21}{subsection.2.2.8}\protected@file@percent }
\newlabel{a-simple-framework-for-contrastive-learning-of-visual-representations}{{2.2.8}{21}{A Simple Framework for Contrastive Learning of Visual Representations}{subsection.2.2.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.8.1}The Contrastive Learning Framework}{22}{subsubsection.2.2.8.1}\protected@file@percent }
\newlabel{the-contrastive-learning-framework}{{2.2.8.1}{22}{The Contrastive Learning Framework}{subsubsection.2.2.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Figure 4. A simple framework for contrastive learning of visual representations\relax }}{22}{figure.caption.15}\protected@file@percent }
\newlabel{fig:ch01-figure04}{{2.4}{22}{Figure 4. A simple framework for contrastive learning of visual representations\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.8.1.1}Stochastic data augmentation module}{22}{paragraph.2.2.8.1.1}\protected@file@percent }
\newlabel{stochastic-data-augmentation-module}{{2.2.8.1.1}{22}{Stochastic data augmentation module}{paragraph.2.2.8.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Figure 5. Augmentation texhniques\relax }}{23}{figure.caption.16}\protected@file@percent }
\newlabel{fig:ch01-figure05}{{2.5}{23}{Figure 5. Augmentation texhniques\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.8.1.2}Neural network base encoder}{23}{paragraph.2.2.8.1.2}\protected@file@percent }
\newlabel{neural-network-base-encoder}{{2.2.8.1.2}{23}{Neural network base encoder}{paragraph.2.2.8.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.8.1.3}Small neural network projection head}{23}{paragraph.2.2.8.1.3}\protected@file@percent }
\newlabel{small-neural-network-projection-head}{{2.2.8.1.3}{23}{Small neural network projection head}{paragraph.2.2.8.1.3}{}}
\citation{byol}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.8.1.4}Contrastive loss function}{24}{paragraph.2.2.8.1.4}\protected@file@percent }
\newlabel{contrastive-loss-function}{{2.2.8.1.4}{24}{Contrastive loss function}{paragraph.2.2.8.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}Bootstrap Your Own Latent}{24}{subsection.2.2.9}\protected@file@percent }
\newlabel{bootstrap-your-own-latent}{{2.2.9}{24}{Bootstrap Your Own Latent}{subsection.2.2.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.9.1}Description of method}{25}{subsubsection.2.2.9.1}\protected@file@percent }
\newlabel{description-of-method}{{2.2.9.1}{25}{Description of method}{subsubsection.2.2.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Figure 6. Bootstrap Your Own Latent\relax }}{25}{figure.caption.18}\protected@file@percent }
\newlabel{fig:ch01-figure06}{{2.6}{25}{Figure 6. Bootstrap Your Own Latent\relax }{figure.caption.18}{}}
\gdef \LT@iv {\LT@entry 
    {2}{42.50002pt}\LT@entry 
    {2}{81.29002pt}\LT@entry 
    {1}{59.06001pt}\LT@entry 
    {1}{52.77002pt}\LT@entry 
    {1}{47.77002pt}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.10}Comparison of contrastive learning frameworks}{26}{subsection.2.2.10}\protected@file@percent }
\newlabel{comparison-of-contrastive-learning-frameworks}{{2.2.10}{26}{Comparison of contrastive learning frameworks}{subsection.2.2.10}{}}
\citation{TRANSFORMERS_NLP}
\citation{wang}
\citation{selfa}
\citation{vit}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.11}Transformers in Computer Vision}{27}{subsection.2.2.11}\protected@file@percent }
\newlabel{transformers-in-computer-vision}{{2.2.11}{27}{Transformers in Computer Vision}{subsection.2.2.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.11.1}Vision Transformers}{27}{subsubsection.2.2.11.1}\protected@file@percent }
\newlabel{vision-transformers}{{2.2.11.1}{27}{Vision Transformers}{subsubsection.2.2.11.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.11.1.1}Method}{27}{paragraph.2.2.11.1.1}\protected@file@percent }
\newlabel{method}{{2.2.11.1.1}{27}{Method}{paragraph.2.2.11.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Figure 7. Vision Transformer\relax }}{28}{figure.caption.23}\protected@file@percent }
\newlabel{fig:ch01-figure7}{{2.7}{28}{Figure 7. Vision Transformer\relax }{figure.caption.23}{}}
\citation{noisy}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.2.11.1.2}Experiments}{29}{paragraph.2.2.11.1.2}\protected@file@percent }
\newlabel{experiments}{{2.2.11.1.2}{29}{Experiments}{paragraph.2.2.11.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.12}Conclusion}{29}{subsection.2.2.12}\protected@file@percent }
\newlabel{conclusion}{{2.2.12}{29}{Conclusion}{subsection.2.2.12}{}}
\citation{darkMatter}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Resources and Benchmarks for NLP, CV and multimodal tasks}{30}{section.2.3}\protected@file@percent }
\newlabel{c01-03-benchmarks}{{2.3}{30}{Resources and Benchmarks for NLP, CV and multimodal tasks}{section.2.3}{}}
\citation{gao2020pile}
\citation{brown2020language}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Datasets}{31}{subsection.2.3.1}\protected@file@percent }
\newlabel{datasets}{{2.3.1}{31}{Datasets}{subsection.2.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.1}Natural Language Processing Datasets}{31}{subsubsection.2.3.1.1}\protected@file@percent }
\newlabel{natural-language-processing-datasets}{{2.3.1.1}{31}{Natural Language Processing Datasets}{subsubsection.2.3.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.1.1}Common Crawl}{31}{paragraph.2.3.1.1.1}\protected@file@percent }
\newlabel{common-crawl}{{2.3.1.1.1}{31}{Common Crawl}{paragraph.2.3.1.1.1}{}}
\citation{rosset2020turing}
\citation{gao2020pile}
\citation{gao2020pile}
\citation{koehn2005europarl}
\citation{Gokaslan2019OpenWeb}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.1.2}The Pile}{32}{paragraph.2.3.1.1.2}\protected@file@percent }
\newlabel{the-pile}{{2.3.1.1.2}{32}{The Pile}{paragraph.2.3.1.1.2}{}}
\citation{gao2020pile}
\citation{wenzek2019ccnet}
\citation{mayer2014creating}
\citation{xue2020mt5}
\citation{xue2020mt5}
\citation{rosset2020turing}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.1.3}Multilingual Datasets}{33}{paragraph.2.3.1.1.3}\protected@file@percent }
\newlabel{multilingual-datasets}{{2.3.1.1.3}{33}{Multilingual Datasets}{paragraph.2.3.1.1.3}{}}
\citation{zhu2015aligning}
\citation{zhu2015aligning}
\citation{bandy2021addressing}
\citation{fellbaum2010wordnet}
\citation{krizhevsky2009learning}
\citation{krizhevsky2009learning}
\citation{deng2009imagenet}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.1.4}BooksCorpus}{34}{paragraph.2.3.1.1.4}\protected@file@percent }
\newlabel{bookscorpus}{{2.3.1.1.4}{34}{BooksCorpus}{paragraph.2.3.1.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.2}Computer Vision Dataset}{34}{subsubsection.2.3.1.2}\protected@file@percent }
\newlabel{computer-vision-dataset}{{2.3.1.2}{34}{Computer Vision Dataset}{subsubsection.2.3.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.2.1}ImageNet}{34}{paragraph.2.3.1.2.1}\protected@file@percent }
\newlabel{imagenet}{{2.3.1.2.1}{34}{ImageNet}{paragraph.2.3.1.2.1}{}}
\citation{hinton2015distilling}
\citation{sun2017revisiting}
\citation{sun2017revisiting}
\citation{gao2017knowledge}
\citation{shao2019objects365}
\citation{deng2009imagenet}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.2.2}Joint-Foto-Tree (JFT) \& Entity-Foto-Tree (EFT)}{35}{paragraph.2.3.1.2.2}\protected@file@percent }
\newlabel{joint-foto-tree-jft-entity-foto-tree-eft}{{2.3.1.2.2}{35}{Joint-Foto-Tree (JFT) \& Entity-Foto-Tree (EFT)}{paragraph.2.3.1.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.2.3}Objects365}{35}{paragraph.2.3.1.2.3}\protected@file@percent }
\newlabel{objects365}{{2.3.1.2.3}{35}{Objects365}{paragraph.2.3.1.2.3}{}}
\citation{lin2014microsoft}
\citation{lin2014microsoft}
\citation{torralba2011unbiased}
\citation{schuhmann2021laion}
\citation{schuhmann2021laion}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.2.4}Microsoft Common Objects in Context (COCO)}{36}{paragraph.2.3.1.2.4}\protected@file@percent }
\newlabel{microsoft-common-objects-in-context-coco}{{2.3.1.2.4}{36}{Microsoft Common Objects in Context (COCO)}{paragraph.2.3.1.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.3}Multi Modal Datasets}{36}{subsubsection.2.3.1.3}\protected@file@percent }
\newlabel{multi-modal-datasets}{{2.3.1.3}{36}{Multi Modal Datasets}{subsubsection.2.3.1.3}{}}
\citation{pont2020connecting}
\citation{lin2014microsoft}
\citation{zhou2017scene}
\citation{young2014image}
\citation{kuznetsova2020open}
\citation{krishna2017visual,kuznetsova2020open,lin2014microsoft}
\citation{saharia2022photorealistic}
\citation{parti}
\citation{LocNarWeb}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.3.1}LAION 400M \& 5B}{37}{paragraph.2.3.1.3.1}\protected@file@percent }
\newlabel{laion-400m-5b}{{2.3.1.3.1}{37}{LAION 400M \& 5B}{paragraph.2.3.1.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.3.2}Localized Narratives}{37}{paragraph.2.3.1.3.2}\protected@file@percent }
\newlabel{localized-narratives}{{2.3.1.3.2}{37}{Localized Narratives}{paragraph.2.3.1.3.2}{}}
\citation{yuan2022wudaomm}
\citation{srinivasan2021wit}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.3.3}WuDaoMM}{38}{paragraph.2.3.1.3.3}\protected@file@percent }
\newlabel{wudaomm}{{2.3.1.3.3}{38}{WuDaoMM}{paragraph.2.3.1.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.1.3.4}Wikipedia Image Text (WIT)}{38}{paragraph.2.3.1.3.4}\protected@file@percent }
\newlabel{wikipedia-image-text-wit}{{2.3.1.3.4}{38}{Wikipedia Image Text (WIT)}{paragraph.2.3.1.3.4}{}}
\citation{yuan2022wudaomm}
\citation{redditUsers}
\citation{sheng2019woman}
\gdef \LT@v {\LT@entry 
    {1}{92.89235pt}\LT@entry 
    {1}{243.60765pt}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1.4}Bias In Datasets}{39}{subsubsection.2.3.1.4}\protected@file@percent }
\newlabel{bias-in-datasets}{{2.3.1.4}{39}{Bias In Datasets}{subsubsection.2.3.1.4}{}}
\citation{dhamala2021bold}
\citation{prabhu2020large}
\citation{prabhu2020large}
\citation{guo2016ms}
\citation{radford2021learning}
\citation{birhane2021multimodal}
\citation{darkMatter}
\citation{unsupBrain}
\citation{zhuang2021unsupervised}
\citation{darkMatter}
\citation{darkMatter}
\citation{darkMatter}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Pre-Training Tasks}{41}{subsection.2.3.2}\protected@file@percent }
\newlabel{pre-training-tasks}{{2.3.2}{41}{Pre-Training Tasks}{subsection.2.3.2}{}}
\citation{bromley1993signature}
\citation{grill2020bootstrap}
\citation{grill2020bootstrap}
\citation{he2022masked}
\citation{radford2021learning}
\citation{jia2021scaling}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Benchmarks}{43}{subsection.2.3.3}\protected@file@percent }
\newlabel{benchmarks}{{2.3.3}{43}{Benchmarks}{subsection.2.3.3}{}}
\citation{rajpurkar2018know}
\citation{recht2019imagenet}
\citation{radford2021learning}
\citation{parti}
\citation{brown2020language}
\citation{radford2021learning}
\citation{mahajan2018exploring}
\citation{kolesnikov2019large}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.1}Natural Language Processing Benchmarks}{44}{subsubsection.2.3.3.1}\protected@file@percent }
\newlabel{natural-language-processing-benchmarks}{{2.3.3.1}{44}{Natural Language Processing Benchmarks}{subsubsection.2.3.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.1.1}(Super)GLUE}{44}{paragraph.2.3.3.1.1}\protected@file@percent }
\newlabel{superglue}{{2.3.3.1.1}{44}{(Super)GLUE}{paragraph.2.3.3.1.1}{}}
\citation{rajpurkar2016squad}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces taken from \url  {https://mccormickml.com}\relax }}{46}{figure.caption.28}\protected@file@percent }
\citation{rajpurkar2018know}
\citation{rajpurkar2018know}
\citation{srivastava2022beyond}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.1.2}Stanford Question Answering Dataset (SQuAD) (1.0 \& 2.0)}{47}{paragraph.2.3.3.1.2}\protected@file@percent }
\newlabel{stanford-question-answering-dataset-squad-1.0-2.0}{{2.3.3.1.2}{47}{Stanford Question Answering Dataset (SQuAD) (1.0 \& 2.0)}{paragraph.2.3.3.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.1.3}Beyond the Imitation Game Benchmark (BIG-bench)}{47}{paragraph.2.3.3.1.3}\protected@file@percent }
\newlabel{beyond-the-imitation-game-benchmark-big-bench}{{2.3.3.1.3}{47}{Beyond the Imitation Game Benchmark (BIG-bench)}{paragraph.2.3.3.1.3}{}}
\citation{bowman2021will}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.1.4}WMT}{48}{paragraph.2.3.3.1.4}\protected@file@percent }
\newlabel{wmt}{{2.3.3.1.4}{48}{WMT}{paragraph.2.3.3.1.4}{}}
\citation{ribeiro2020beyond}
\citation{goodfellow2014explaining}
\citation{recht2019imagenet}
\citation{recht2019imagenet}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.1.5}CheckList}{49}{paragraph.2.3.3.1.5}\protected@file@percent }
\newlabel{checklist}{{2.3.3.1.5}{49}{CheckList}{paragraph.2.3.3.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.2}Computer Vision Benchmarks}{49}{subsubsection.2.3.3.2}\protected@file@percent }
\newlabel{computer-vision-benchmarks}{{2.3.3.2}{49}{Computer Vision Benchmarks}{subsubsection.2.3.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.2.1}ImageNet Versions}{49}{paragraph.2.3.3.2.1}\protected@file@percent }
\newlabel{imagenet-versions}{{2.3.3.2.1}{49}{ImageNet Versions}{paragraph.2.3.3.2.1}{}}
\citation{beyer2020we}
\citation{beyer2020we}
\citation{beyer2020we}
\citation{beyer2020we}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.2.2}MS-COCO \& Object365}{50}{paragraph.2.3.3.2.2}\protected@file@percent }
\newlabel{ms-coco-object365}{{2.3.3.2.2}{50}{MS-COCO \& Object365}{paragraph.2.3.3.2.2}{}}
\citation{coco_eval}
\citation{zhou2017scene}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.2.3}ADE20k}{51}{paragraph.2.3.3.2.3}\protected@file@percent }
\newlabel{ade20k}{{2.3.3.2.3}{51}{ADE20k}{paragraph.2.3.3.2.3}{}}
\citation{zellers2019recognition}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces taken from \url  {https://learnopencv.com}\relax }}{52}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.3}Multi-Modal Benchmarks}{52}{subsubsection.2.3.3.3}\protected@file@percent }
\newlabel{multi-modal-benchmarks}{{2.3.3.3}{52}{Multi-Modal Benchmarks}{subsubsection.2.3.3.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.3.1}Visual Commonsense Reasoning (VCR)}{52}{paragraph.2.3.3.3.1}\protected@file@percent }
\newlabel{visual-commonsense-reasoning-vcr}{{2.3.3.3.1}{52}{Visual Commonsense Reasoning (VCR)}{paragraph.2.3.3.3.1}{}}
\citation{antol2015vqa}
\citation{antol2015vqa}
\citation{zhang2016yin}
\citation{zhang2016yin}
\citation{zhang2016yin}
\citation{goyal2017making}
\citation{goyal2017making}
\citation{antol2015vqa}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.3.2}Visual Question Answering 1.0 \& 2.0 (VQA)}{53}{paragraph.2.3.3.3.2}\protected@file@percent }
\newlabel{visual-question-answering-1.0-2.0-vqa}{{2.3.3.3.2}{53}{Visual Question Answering 1.0 \& 2.0 (VQA)}{paragraph.2.3.3.3.2}{}}
\citation{hudson2019gqa}
\citation{parti}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.4}GQA}{54}{subsubsection.2.3.3.4}\protected@file@percent }
\newlabel{gqa}{{2.3.3.4}{54}{GQA}{subsubsection.2.3.3.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.4.1}Generative Benchmarks}{54}{paragraph.2.3.3.4.1}\protected@file@percent }
\newlabel{generative-benchmarks}{{2.3.3.4.1}{54}{Generative Benchmarks}{paragraph.2.3.3.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces taken from Parti Paper\relax }}{55}{figure.caption.30}\protected@file@percent }
\citation{shekhar2017foil}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.4.2}PartiPrompts, DrawBench, Localized Narratives}{56}{paragraph.2.3.3.4.2}\protected@file@percent }
\newlabel{partiprompts-drawbench-localized-narratives}{{2.3.3.4.2}{56}{PartiPrompts, DrawBench, Localized Narratives}{paragraph.2.3.3.4.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.4.3}FOIL it!}{56}{paragraph.2.3.3.4.3}\protected@file@percent }
\newlabel{foil-it}{{2.3.3.4.3}{56}{FOIL it!}{paragraph.2.3.3.4.3}{}}
\citation{parcalabescu-etal-2022-valse}
\citation{strubell2019energy}
\citation{strubell2019energy}
\citation{lottick2019energy}
\citation{henderson2020towards}
\@writefile{toc}{\contentsline {paragraph}{\numberline {2.3.3.4.4}VALSE}{57}{paragraph.2.3.3.4.4}\protected@file@percent }
\newlabel{valse}{{2.3.3.4.4}{57}{VALSE}{paragraph.2.3.3.4.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3.5}Other Benchmarks}{57}{subsubsection.2.3.3.5}\protected@file@percent }
\newlabel{other-benchmarks}{{2.3.3.5}{57}{Other Benchmarks}{subsubsection.2.3.3.5}{}}
\citation{mccoco,cornia2020m2}
\citation{mccoco}
\citation{cornia2020m2}
\citation{ramesh2021dalle}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Multimodal architectures}{59}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{c02-00-multimodal}{{3}{59}{Multimodal architectures}{chapter.3}{}}
\citation{nichol2021glide}
\citation{harnad1990symbol}
\citation{silberer2012grounded}
\citation{bordes2020incorporating}
\citation{radford2021learning}
\citation{brown2020language}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Left, Silberer et al., 2014: stacked autoencoders to learn higher-level embeddings from textual and visual modalities, encoded as vectors of attributes. Right, Bordes et al., 2020: textual and visual information fused in an Intermediate space denoted as “grounded space”; the “grounding objective function” is not applied directly on sentence embeddings but trained on this intermediate space, on which sentence embeddings are projected.\relax }}{61}{figure.caption.31}\protected@file@percent }
\newlabel{fig:unnamed-chunk-1}{{3.1}{61}{Left, Silberer et al., 2014: stacked autoencoders to learn higher-level embeddings from textual and visual modalities, encoded as vectors of attributes. Right, Bordes et al., 2020: textual and visual information fused in an Intermediate space denoted as “grounded space”; the “grounding objective function” is not applied directly on sentence embeddings but trained on this intermediate space, on which sentence embeddings are projected.\relax }{figure.caption.31}{}}
\citation{baevski2022data2vec}
\citation{lu2019vilbert}
\citation{alayrac2022flamingo}
\citation{baevski2022data2vec}
\citation{vaswani2017attention}
\citation{dosovitskiy2020image}
\citation{jaegle2021perceiver}
\citation{mccoco}
\citation{deng2009imagenet}
\citation{sun}
\citation{pascalvoc}
\citation{deng2009imagenet}
\citation{mccoco}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Img2text}{63}{section.3.1}\protected@file@percent }
\newlabel{c02-01-img2text}{{3.1}{63}{Img2text}{section.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Microsoft COCO: Common Objects in Context}{63}{subsection.3.1.1}\protected@file@percent }
\newlabel{microsoft-coco-common-objects-in-context}{{3.1.1}{63}{Microsoft COCO: Common Objects in Context}{subsection.3.1.1}{}}
\citation{deng2009imagenet}
\citation{mccoco}
\citation{pascalvoc}
\citation{mccoco}
\citation{mccoco}
\citation{sun}
\citation{WordNet}
\citation{mccoco}
\citation{pascalvoc}
\citation{mccoco}
\citation{pascalvoc}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.1}Image Collection and Annotation for MS COCO}{64}{subsubsection.3.1.1.1}\protected@file@percent }
\newlabel{image-collection-and-annotation-for-ms-coco}{{3.1.1.1}{64}{Image Collection and Annotation for MS COCO}{subsubsection.3.1.1.1}{}}
\citation{mccoco}
\citation{mccoco}
\citation{mccoco}
\citation{mccoco}
\citation{mccoco}
\citation{mccoco}
\citation{mccoco}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Type of images \citep  {mccoco}.\relax }}{65}{figure.caption.32}\protected@file@percent }
\newlabel{fig:imagetype}{{3.2}{65}{Type of images \citep {mccoco}.\relax }{figure.caption.32}{}}
\citation{mccoco}
\citation{deng2009imagenet}
\citation{pascalvoc}
\citation{sun}
\citation{mccoco}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Annotation pipeline for COCO \citep  {mccoco}.\relax }}{66}{figure.caption.33}\protected@file@percent }
\newlabel{fig:cocoannotation}{{3.3}{66}{Annotation pipeline for COCO \citep {mccoco}.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.2}Comparison with other Datasets}{66}{subsubsection.3.1.1.2}\protected@file@percent }
\newlabel{comparison-with-other-datasets}{{3.1.1.2}{66}{Comparison with other Datasets}{subsubsection.3.1.1.2}{}}
\citation{mccoco}
\citation{mccoco}
\citation{mccoco}
\citation{mccoco}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Comparison COCO with other PASCAL VOC, SUN and ImageNet \citep  {mccoco}.\relax }}{67}{figure.caption.34}\protected@file@percent }
\newlabel{fig:cococomparison}{{3.4}{67}{Comparison COCO with other PASCAL VOC, SUN and ImageNet \citep {mccoco}.\relax }{figure.caption.34}{}}
\citation{mccoco}
\citation{cornia2020m2}
\citation{Socher10connectingmodalities}
\citation{5487377}
\citation{vinyals}
\citation{karpthy1}
\citation{yao1}
\citation{xu1}
\citation{cornia2020m2}
\citation{NIPS2017_3f5ee243}
\citation{devlin-etal-2019-bert}
\citation{cornia2020m2}
\citation{HerdadeKBS19}
\citation{huang1}
\citation{cornia2020m2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.3}Discussion}{68}{subsubsection.3.1.1.3}\protected@file@percent }
\newlabel{discussion}{{3.1.1.3}{68}{Discussion}{subsubsection.3.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Models for Image captioning}{68}{subsection.3.1.2}\protected@file@percent }
\newlabel{models-for-image-captioning}{{3.1.2}{68}{Models for Image captioning}{subsection.3.1.2}{}}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Meshed-Memory Transformer for Image Captioning (\(M^2\))}{69}{subsection.3.1.3}\protected@file@percent }
\newlabel{meshed-memory-transformer-for-image-captioning-m2}{{3.1.3}{69}{\texorpdfstring {Meshed-Memory Transformer for Image Captioning (\(M^2\))}{Meshed-Memory Transformer for Image Captioning (M\^{}2)}}{subsection.3.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \(M^2\) Transformer \citep  {cornia2020m2}.\relax }}{69}{figure.caption.35}\protected@file@percent }
\newlabel{fig:m2arc1}{{3.5}{69}{\(M^2\) Transformer \citep {cornia2020m2}.\relax }{figure.caption.35}{}}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3.1}\(M^2\) Transformer Architecture}{70}{subsubsection.3.1.3.1}\protected@file@percent }
\newlabel{m2-transformer-architecture}{{3.1.3.1}{70}{\texorpdfstring {\(M^2\) Transformer Architecture}{M\^{}2 Transformer Architecture}}{subsubsection.3.1.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \(M^2\) Transformer Architecture \citep  {cornia2020m2}.\relax }}{70}{figure.caption.36}\protected@file@percent }
\newlabel{fig:m2arc2}{{3.6}{70}{\(M^2\) Transformer Architecture \citep {cornia2020m2}.\relax }{figure.caption.36}{}}
\newlabel{eq:binom}{{3.1}{70}{\texorpdfstring {\(M^2\) Transformer Architecture}{M\^{}2 Transformer Architecture}}{equation.3.1.1}{}}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.1.3.1.1}Memory-Augmented Encoder}{71}{paragraph.3.1.3.1.1}\protected@file@percent }
\newlabel{memory-augmented-encoder}{{3.1.3.1.1}{71}{Memory-Augmented Encoder}{paragraph.3.1.3.1.1}{}}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.1.3.1.2}Meshed Decoder}{72}{paragraph.3.1.3.1.2}\protected@file@percent }
\newlabel{meshed-decoder}{{3.1.3.1.2}{72}{Meshed Decoder}{paragraph.3.1.3.1.2}{}}
\citation{mccoco}
\citation{cornia2020m2}
\citation{karpthy1}
\citation{papineni-etal-2002-bleu}
\citation{meteor}
\citation{lin-2004-rouge}
\citation{cider}
\citation{spice}
\citation{cornia2020m2}
\citation{huang1}
\citation{cornia2020m2}
\citation{cornia2020m2}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.1.3.1.3}Comparison with other models on COCO Datasets}{73}{paragraph.3.1.3.1.3}\protected@file@percent }
\newlabel{comparison-with-other-models-on-coco-datasets}{{3.1.3.1.3}{73}{Comparison with other models on COCO Datasets}{paragraph.3.1.3.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Comparison with Transformer-based alternatives \citep  {cornia2020m2}\relax }}{73}{figure.caption.37}\protected@file@percent }
\newlabel{fig:compare1}{{3.7}{73}{Comparison with Transformer-based alternatives \citep {cornia2020m2}\relax }{figure.caption.37}{}}
\citation{cornia2020m2}
\citation{8099614}
\citation{8578734}
\citation{renet}
\citation{GCN-LSTM}
\citation{Yang_2019_CVPR}
\citation{huang1}
\citation{HerdadeKBS19}
\citation{cornia2020m2}
\citation{cornia2020m2}
\citation{cornia2020m2}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Comparison with the state of the art on the ``Karpathy'' test split, in single-model setting \citep  {cornia2020m2}\relax }}{74}{figure.caption.38}\protected@file@percent }
\newlabel{fig:compare2}{{3.8}{74}{Comparison with the state of the art on the ``Karpathy'' test split, in single-model setting \citep {cornia2020m2}\relax }{figure.caption.38}{}}
\citation{cornia2020m2}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces ``Examples of captions generated by M\^{}2\$ Transformer and the original Transformer model, as well as the corresponding ground-truths \citep  {cornia2020m2}\relax }}{75}{figure.caption.39}\protected@file@percent }
\newlabel{fig:example2}{{3.9}{75}{``Examples of captions generated by M\^{}2\$ Transformer and the original Transformer model, as well as the corresponding ground-truths \citep {cornia2020m2}\relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Text-2-image}{75}{section.3.2}\protected@file@percent }
\newlabel{c02-02-text2img}{{3.2}{75}{Text-2-image}{section.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Seeking objectivity}{76}{subsection.3.2.1}\protected@file@percent }
\newlabel{seeking-objectivity}{{3.2.1}{76}{Seeking objectivity}{subsection.3.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.1}Datasets}{76}{subsubsection.3.2.1.1}\protected@file@percent }
\newlabel{datasets-1}{{3.2.1.1}{76}{Datasets}{subsubsection.3.2.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1.2}Measures}{76}{subsubsection.3.2.1.2}\protected@file@percent }
\newlabel{measures}{{3.2.1.2}{76}{Measures}{subsubsection.3.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Generative Adversarial Networks}{76}{subsection.3.2.2}\protected@file@percent }
\newlabel{generative-adversarial-networks}{{3.2.2}{76}{Generative Adversarial Networks}{subsection.3.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.1}Vanilla GAN for Image Generation}{76}{subsubsection.3.2.2.1}\protected@file@percent }
\newlabel{vanilla-gan-for-image-generation}{{3.2.2.1}{76}{Vanilla GAN for Image Generation}{subsubsection.3.2.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.2}Conditioning on Text}{76}{subsubsection.3.2.2.2}\protected@file@percent }
\newlabel{conditioning-on-text}{{3.2.2.2}{76}{Conditioning on Text}{subsubsection.3.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.3}Stacking generators}{76}{subsubsection.3.2.2.3}\protected@file@percent }
\newlabel{stacking-generators}{{3.2.2.3}{76}{Stacking generators}{subsubsection.3.2.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.4}Is attention all you need?}{77}{subsubsection.3.2.2.4}\protected@file@percent }
\newlabel{is-attention-all-you-need}{{3.2.2.4}{77}{Is attention all you need?}{subsubsection.3.2.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2.5}Variational Autoencoder}{77}{subsubsection.3.2.2.5}\protected@file@percent }
\newlabel{variational-autoencoder}{{3.2.2.5}{77}{Variational Autoencoder}{subsubsection.3.2.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Dall-E starting post-GAN era}{77}{subsection.3.2.3}\protected@file@percent }
\newlabel{dall-e-starting-post-gan-era}{{3.2.3}{77}{Dall-E starting post-GAN era}{subsection.3.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}GLIDE}{77}{subsection.3.2.4}\protected@file@percent }
\newlabel{glide}{{3.2.4}{77}{GLIDE}{subsection.3.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Dall-E 2}{77}{subsection.3.2.5}\protected@file@percent }
\newlabel{dall-e-2}{{3.2.5}{77}{Dall-E 2}{subsection.3.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Imagen}{77}{subsection.3.2.6}\protected@file@percent }
\newlabel{imagen}{{3.2.6}{77}{Imagen}{subsection.3.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.7}Parti}{77}{subsection.3.2.7}\protected@file@percent }
\newlabel{parti}{{3.2.7}{77}{Parti}{subsection.3.2.7}{}}
\citation{harnad1990symbol}
\citation{mikolov2013efficient}
\citation{harris1954distributional}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.8}Open-Source Community}{78}{subsection.3.2.8}\protected@file@percent }
\newlabel{open-source-community}{{3.2.8}{78}{Open-Source Community}{subsection.3.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.9}Discussion}{78}{subsection.3.2.9}\protected@file@percent }
\newlabel{discussion-1}{{3.2.9}{78}{Discussion}{subsection.3.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Images supporting language models}{78}{section.3.3}\protected@file@percent }
\newlabel{c02-03-img-support-text}{{3.3}{78}{Images supporting language models}{section.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Words In (Non-Symbolic) Contexts}{78}{subsection.3.3.1}\protected@file@percent }
\newlabel{words-in-non-symbolic-contexts}{{3.3.1}{78}{Words In (Non-Symbolic) Contexts}{subsection.3.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Historical evolution of models which integrate visual information into pure language models. \relax }}{79}{figure.caption.40}\protected@file@percent }
\newlabel{fig:img-hist}{{3.10}{79}{Historical evolution of models which integrate visual information into pure language models. \relax }{figure.caption.40}{}}
\citation{mikolov2013efficient}
\citation{pennington2014glove}
\citation{devlin2018bert}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Word-Embeddings: Survival-Kit}{80}{subsection.3.3.2}\protected@file@percent }
\newlabel{word-embeddings-survival-kit}{{3.3.2}{80}{Word-Embeddings: Survival-Kit}{subsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}The Beginning: Sequential Multimodal Embeddings}{80}{subsection.3.3.3}\protected@file@percent }
\newlabel{the-beginning-sequential-multimodal-embeddings}{{3.3.3}{80}{The Beginning: Sequential Multimodal Embeddings}{subsection.3.3.3}{}}
\citation{bruni2014multimodal}
\citation{bosch2007image}
\citation{kiela2014learning}
\citation{krizhevsky2012imagenet}
\citation{deng2009imagenet}
\citation{mikolov2013efficient}
\citation{silberer2014learning}
\citation{lazaridou2015combining}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces From @kiela2014learning. Textual and visual features vectors are concatenated.\relax }}{82}{figure.caption.41}\protected@file@percent }
\newlabel{fig:img-kiela2014-01}{{3.11}{82}{From @kiela2014learning. Textual and visual features vectors are concatenated.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}The Grounded Space}{82}{subsection.3.3.4}\protected@file@percent }
\newlabel{the-grounded-space}{{3.3.4}{82}{The Grounded Space}{subsection.3.3.4}{}}
\citation{kottur2016visual}
\citation{collell2017imagined}
\citation{deng2009imagenet}
\citation{hill2014learning}
\citation{devereux2014centre}
\citation{ailem2018probabilistic}
\citation{kiros2018illustrative}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces From @lazaridou2015combining. The linguistic embedding of the word 'cat' is mapped to a visual space, such that the similarity of vector representations of words and associated images is maximized.\relax }}{83}{figure.caption.42}\protected@file@percent }
\newlabel{fig:img-lazaridou2015-01}{{3.12}{83}{From @lazaridou2015combining. The linguistic embedding of the word 'cat' is mapped to a visual space, such that the similarity of vector representations of words and associated images is maximized.\relax }{figure.caption.42}{}}
\citation{kiela2017learning}
\citation{hochreiter1997long}
\citation{pennington2014glove}
\citation{silberer2014learning}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.5}The Transformers Era}{84}{subsection.3.3.5}\protected@file@percent }
\newlabel{the-transformers-era}{{3.3.5}{84}{The Transformers Era}{subsection.3.3.5}{}}
\citation{singh2022flava}
\citation{hu2021unit}
\citation{dosovitskiy2020image}
\citation{tan2020vokenization}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5.1}Vokenization}{86}{subsubsection.3.3.5.1}\protected@file@percent }
\newlabel{vokenization}{{3.3.5.1}{86}{Vokenization}{subsubsection.3.3.5.1}{}}
\citation{lin2014microsoft}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces From @tan2020vokenization. Visually supervised the language model with token-related images, called Vokens.\relax }}{87}{figure.caption.43}\protected@file@percent }
\newlabel{fig:img-tan2020-04}{{3.13}{87}{From @tan2020vokenization. Visually supervised the language model with token-related images, called Vokens.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces From @tan2020vokenization. Statistics of image-captioning dataset and other natural language corpora. VG, CC, Eng Wiki, and CNN/DM denote Visual Genome, Conceptual Captions, English Wikipedia, and CNN/Daily Mail, respectively. JSD represents Jensen–Shannon divergence to the English Wikipedia corpus.\relax }}{87}{figure.caption.44}\protected@file@percent }
\newlabel{fig:img-tan2020-01}{{3.14}{87}{From @tan2020vokenization. Statistics of image-captioning dataset and other natural language corpora. VG, CC, Eng Wiki, and CNN/DM denote Visual Genome, Conceptual Captions, English Wikipedia, and CNN/Daily Mail, respectively. JSD represents Jensen–Shannon divergence to the English Wikipedia corpus.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces From @tan2020vokenization. The Vokenization process. A contextualized image (visual token, Voken) is retrieved for every token in a sentence and with this visual token, visual supervision is performed.\relax }}{88}{figure.caption.45}\protected@file@percent }
\newlabel{fig:img-tan2020-05}{{3.15}{88}{From @tan2020vokenization. The Vokenization process. A contextualized image (visual token, Voken) is retrieved for every token in a sentence and with this visual token, visual supervision is performed.\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {3.3.5.1.1}The Relevance Score Function: Model, Training, Inference}{89}{paragraph.3.3.5.1.1}\protected@file@percent }
\newlabel{the-relevance-score-function-model-training-inference}{{3.3.5.1.1}{89}{The Relevance Score Function: Model, Training, Inference}{paragraph.3.3.5.1.1}{}}
\citation{lu2022imagination}
\citation{esser2021taming}
\citation{radford2021learning}
\citation{wang2018glue}
\citation{zellers2018swag}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5.2}One Step Further: The Power Of Imagination}{90}{subsubsection.3.3.5.2}\protected@file@percent }
\newlabel{one-step-further-the-power-of-imagination}{{3.3.5.2}{90}{One Step Further: The Power Of Imagination}{subsubsection.3.3.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces From @lu2022imagination. The generator $G$ visualize imaginations close to the encoded texts by minimizing $\symcal {L}_{GAN}$. The cross-modal encoder $E_c$ learns imagination-augmented language representation. Two-step learning procedure consists of: 1) pre-train a Transformer with visual supervision from large-scale language corpus and image set, 2) fine-tune the visually supervised pre-trained Transformer and the imagination-augmented cross-modal encoder on downstream tasks.\relax }}{91}{figure.caption.46}\protected@file@percent }
\newlabel{fig:img-lu2022-01}{{3.16}{91}{From @lu2022imagination. The generator $G$ visualize imaginations close to the encoded texts by minimizing $\mathcal {L}_{GAN}$. The cross-modal encoder $E_c$ learns imagination-augmented language representation. Two-step learning procedure consists of: 1) pre-train a Transformer with visual supervision from large-scale language corpus and image set, 2) fine-tune the visually supervised pre-trained Transformer and the imagination-augmented cross-modal encoder on downstream tasks.\relax }{figure.caption.46}{}}
\citation{bruni2014multimodal}
\citation{agirre2009study}
\citation{hill2015simlex}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.6}Was It Worth?}{92}{subsection.3.3.6}\protected@file@percent }
\newlabel{was-it-worth}{{3.3.6}{92}{Was It Worth?}{subsection.3.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.6.1}Evaluation In The Pre-Transformers Era}{92}{subsubsection.3.3.6.1}\protected@file@percent }
\newlabel{evaluation-in-the-pre-transformers-era}{{3.3.6.1}{92}{Evaluation In The Pre-Transformers Era}{subsubsection.3.3.6.1}{}}
\citation{kiela2017learning}
\citation{collell2017imagined}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Pipeline for intrisinsic evaluation of semantic representations. In the first step, the cosine similarity between two word embeddings w1 and w2 is used as similariry measure and in a second step, the correlation with human speakers'assessment is computed to gauge the quality of the embeddings. The higher the correlation, the better the embeddings.\relax }}{93}{figure.caption.47}\protected@file@percent }
\newlabel{fig:img-eval01}{{3.17}{93}{Pipeline for intrisinsic evaluation of semantic representations. In the first step, the cosine similarity between two word embeddings w1 and w2 is used as similariry measure and in a second step, the correlation with human speakers'assessment is computed to gauge the quality of the embeddings. The higher the correlation, the better the embeddings.\relax }{figure.caption.47}{}}
\citation{wang2018glue}
\citation{rajpurkar2016squad}
\citation{zellers2018swag}
\citation{tan2020vokenization}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces From @hill2014learning: Each bar represents a different model settings and the dashed line indicates the pure linguistic benchmark model.\relax }}{94}{figure.caption.48}\protected@file@percent }
\newlabel{fig:img-2014hill-01}{{3.18}{94}{From @hill2014learning: Each bar represents a different model settings and the dashed line indicates the pure linguistic benchmark model.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.6.2}Evaluation In The Post-Transformers Era}{94}{subsubsection.3.3.6.2}\protected@file@percent }
\newlabel{evaluation-in-the-post-transformers-era}{{3.3.6.2}{94}{Evaluation In The Post-Transformers Era}{subsubsection.3.3.6.2}{}}
\citation{pezzelle2021word}
\citation{brysbaert2014concreteness}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces From @tan2020vokenization. Results of vision-and-language pre-trained models (universal models) on GLUE tasks compared to baseline models (BERT).\relax }}{95}{figure.caption.49}\protected@file@percent }
\newlabel{fig:img-tan2020-02}{{3.19}{95}{From @tan2020vokenization. Results of vision-and-language pre-trained models (universal models) on GLUE tasks compared to baseline models (BERT).\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces From @tan2020vokenization. Fine-tuning results of different pre-trained models w/ or w/o the voken classification task (denoted as“Voken-cls”).\relax }}{95}{figure.caption.50}\protected@file@percent }
\newlabel{fig:img-tan2020-03}{{3.20}{95}{From @tan2020vokenization. Fine-tuning results of different pre-trained models w/ or w/o the voken classification task (denoted as“Voken-cls”).\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces From @pezzelle2021word. Spearman’s rank correlation between similarities computed with representations by all tested models and human similarity judgments in the five evaluation benchmarks.\relax }}{96}{figure.caption.51}\protected@file@percent }
\newlabel{fig:img-pezzele2021-01}{{3.21}{96}{From @pezzelle2021word. Spearman’s rank correlation between similarities computed with representations by all tested models and human similarity judgments in the five evaluation benchmarks.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces From @pezzelle2021word. Correlation between model and human similarity ratings on WordSim353, SimLex999 and MEN. Each barplot reports results on both the whole benchmark and the most concrete subset of it.\relax }}{96}{figure.caption.52}\protected@file@percent }
\newlabel{fig:img-pezzele2021-02}{{3.22}{96}{From @pezzelle2021word. Correlation between model and human similarity ratings on WordSim353, SimLex999 and MEN. Each barplot reports results on both the whole benchmark and the most concrete subset of it.\relax }{figure.caption.52}{}}
\citation{lu2022imagination}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces From @lu2022imagination. Model-agnostic improvement in Few-shot Setting with GLUE benchmark.\relax }}{97}{figure.caption.53}\protected@file@percent }
\newlabel{fig:img-lu2022-02}{{3.23}{97}{From @lu2022imagination. Model-agnostic improvement in Few-shot Setting with GLUE benchmark.\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.7}The End Of This Story}{97}{subsection.3.3.7}\protected@file@percent }
\newlabel{the-end-of-this-story}{{3.3.7}{97}{The End Of This Story}{subsection.3.3.7}{}}
\citation{sun2021multimodal}
\citation{ive2019distilling}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.8}Appendix: Selected Models - Summary}{98}{subsection.3.3.8}\protected@file@percent }
\newlabel{appendix-selected-models---summary}{{3.3.8}{98}{Appendix: Selected Models - Summary}{subsection.3.3.8}{}}
\gdef \LT@vi {\LT@entry 
    {1}{5.27081pt}\LT@entry 
    {1}{29.79567pt}\LT@entry 
    {1}{24.59381pt}\LT@entry 
    {1}{15.81871pt}\LT@entry 
    {1}{32.97397pt}\LT@entry 
    {1}{25.45514pt}\LT@entry 
    {1}{94.57263pt}\LT@entry 
    {1}{35.68585pt}\LT@entry 
    {1}{25.45514pt}\LT@entry 
    {1}{46.93088pt}}
\citation{sutton2019bitterlesson}
\citation{ImageT}
\citation{vaswani2017attention}
\citation{radford2021learning}
\citation{jia2021scaling}
\citation{yuan2021florence}
\citation{brown2020language}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Text supporting computer vision models}{113}{section.3.4}\protected@file@percent }
\newlabel{c02-04-text-support-img}{{3.4}{113}{Text supporting computer vision models}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Introduction}{113}{subsection.3.4.1}\protected@file@percent }
\newlabel{introduction-1}{{3.4.1}{113}{Introduction}{subsection.3.4.1}{}}
\citation{radford2021learning}
\citation{sutton2019bitterlesson}
\citation{radford2021learning}
\citation{yuan2021florence}
\citation{jia2021scaling}
\citation{radford2021learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Concepts}{114}{subsection.3.4.2}\protected@file@percent }
\newlabel{concepts}{{3.4.2}{114}{Concepts}{subsection.3.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.1}Web-scale data}{114}{subsubsection.3.4.2.1}\protected@file@percent }
\newlabel{webScaleData}{{3.4.2.1}{114}{Web-scale data}{subsubsection.3.4.2.1}{}}
\citation{tian2020contrastive}
\citation{radford2021learning}
\citation{tian2020contrastive}
\citation{tian2020contrastive}
\citation{radford2021learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.2}Contrastive objective}{115}{subsubsection.3.4.2.2}\protected@file@percent }
\newlabel{contrObj}{{3.4.2.2}{115}{Contrastive objective}{subsubsection.3.4.2.2}{}}
\newlabel{eq:contrLoss}{{3.10}{115}{Contrastive objective}{equation.3.4.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces Visualization of a contrastive objective \citep  {radford2021learning}. After encoding the data, a similarity matrix for the images and texts is computed. The aim is that the N true image-text pairs score high in terms of similarity, while the \(\text  {N}^2 - \text  {N}\) other possible combinations score low.\relax }}{116}{figure.caption.54}\protected@file@percent }
\newlabel{fig:contr-viz}{{3.24}{116}{Visualization of a contrastive objective \citep {radford2021learning}. After encoding the data, a similarity matrix for the images and texts is computed. The aim is that the N true image-text pairs score high in terms of similarity, while the \(\text {N}^2 - \text {N}\) other possible combinations score low.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.25}{\ignorespaces Predictive vs.\nobreakspace  {}contrastive learning: Predictive losses are measured in the output space while contrastive losses are measured is in the representation space, indicated by red dotted boxes \citep  {tian2020contrastive}.\relax }}{116}{figure.caption.55}\protected@file@percent }
\newlabel{fig:contr-vs-pred-learn}{{3.25}{116}{Predictive vs.~contrastive learning: Predictive losses are measured in the output space while contrastive losses are measured is in the representation space, indicated by red dotted boxes \citep {tian2020contrastive}.\relax }{figure.caption.55}{}}
\citation{radford2021learning}
\citation{bommasani2021opportunities}
\citation{Devlin2018}
\citation{bommasani2021opportunities}
\citation{brown2020language}
\@writefile{lof}{\contentsline {figure}{\numberline {3.26}{\ignorespaces Data efficiency of contrastive objective. Development of zero-shot accuracy (see next subsection \ref  {foundMod}) on ImageNet with increasing number of instances of training data seen by models. The contrastive objective reaches same accuracies as the generative approach with only a seventh of the amount of data \citep  {radford2021learning}.\relax }}{117}{figure.caption.56}\protected@file@percent }
\newlabel{fig:data-efficiency}{{3.26}{117}{Data efficiency of contrastive objective. Development of zero-shot accuracy (see next subsection \ref {foundMod}) on ImageNet with increasing number of instances of training data seen by models. The contrastive objective reaches same accuracies as the generative approach with only a seventh of the amount of data \citep {radford2021learning}.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2.3}Foundation models and zero-shooting}{117}{subsubsection.3.4.2.3}\protected@file@percent }
\newlabel{foundMod}{{3.4.2.3}{117}{Foundation models and zero-shooting}{subsubsection.3.4.2.3}{}}
\citation{radford2021learning}
\citation{radford2021learning}
\citation{radford2021learning}
\citation{zhang2020contrastive}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Architectures}{118}{subsection.3.4.3}\protected@file@percent }
\newlabel{architectures}{{3.4.3}{118}{Architectures}{subsection.3.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3.1}CLIP}{118}{subsubsection.3.4.3.1}\protected@file@percent }
\newlabel{clip}{{3.4.3.1}{118}{CLIP}{subsubsection.3.4.3.1}{}}
\citation{zhang2020contrastive}
\citation{radford2021learning}
\citation{vaswani2017attention}
\citation{radford2019language}
\@writefile{lof}{\contentsline {figure}{\numberline {3.27}{\ignorespaces Visualization of zero-shooting \citep  {radford2021learning}.\relax }}{119}{figure.caption.57}\protected@file@percent }
\newlabel{fig:zero-shooting}{{3.27}{119}{Visualization of zero-shooting \citep {radford2021learning}.\relax }{figure.caption.57}{}}
\newlabel{eq:contrLossCLIP}{{3.11}{119}{CLIP}{equation.3.4.11}{}}
\citation{radford2021learning}
\citation{radford2021learning}
\citation{radford2021learning}
\citation{radford2021learning}
\citation{shen2021much}
\citation{shen2021much}
\citation{jia2021scaling}
\@writefile{lof}{\contentsline {figure}{\numberline {3.28}{\ignorespaces Robustness of zero-shot CLIP to distribution shifts \citep  {radford2021learning}.\relax }}{121}{figure.caption.58}\protected@file@percent }
\newlabel{fig:performance-clip}{{3.28}{121}{Robustness of zero-shot CLIP to distribution shifts \citep {radford2021learning}.\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.29}{\ignorespaces Grad-CAM Visualizations for the prompt ``What color is the woman's shirt on the left?''.\relax }}{121}{figure.caption.59}\protected@file@percent }
\newlabel{fig:attention-ViT}{{3.29}{121}{Grad-CAM Visualizations for the prompt ``What color is the woman's shirt on the left?''.\relax }{figure.caption.59}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3.2}ALIGN}{121}{subsubsection.3.4.3.2}\protected@file@percent }
\newlabel{align}{{3.4.3.2}{121}{ALIGN}{subsubsection.3.4.3.2}{}}
\citation{zhang2020contrastive}
\citation{radford2021learning}
\citation{alford2021alignparams}
\citation{yuan2021florence}
\citation{yuan2021florence}
\@writefile{lof}{\contentsline {figure}{\numberline {3.30}{\ignorespaces Multimodal image retrieval via arithmetic operations on word and image embeddings.\relax }}{122}{figure.caption.60}\protected@file@percent }
\newlabel{fig:img-txt-addition}{{3.30}{122}{Multimodal image retrieval via arithmetic operations on word and image embeddings.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3.3}Florence}{122}{subsubsection.3.4.3.3}\protected@file@percent }
\newlabel{florence}{{3.4.3.3}{122}{Florence}{subsubsection.3.4.3.3}{}}
\citation{radford2021learning}
\citation{jia2021scaling}
\citation{yuan2021florence}
\citation{yuan2021florence}
\@writefile{lof}{\contentsline {figure}{\numberline {3.31}{\ignorespaces Florence' approach to foundation models: A general purpose vision system for all tasks.\relax }}{123}{figure.caption.61}\protected@file@percent }
\newlabel{fig:florence-dimensions}{{3.31}{123}{Florence' approach to foundation models: A general purpose vision system for all tasks.\relax }{figure.caption.61}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Performance comparison}{123}{subsection.3.4.4}\protected@file@percent }
\newlabel{performanceComp}{{3.4.4}{123}{Performance comparison}{subsection.3.4.4}{}}
\citation{yuan2021florence}
\citation{jia2021scaling}
\citation{yuan2021florence}
\@writefile{lof}{\contentsline {figure}{\numberline {3.32}{\ignorespaces Modular architecture of Florence.\relax }}{124}{figure.caption.62}\protected@file@percent }
\newlabel{fig:florence-architecture}{{3.32}{124}{Modular architecture of Florence.\relax }{figure.caption.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.33}{\ignorespaces Top-1 Accuracy of zero-shot transfer of models to image classification on ImageNet and its variants.\relax }}{124}{figure.caption.63}\protected@file@percent }
\newlabel{fig:table1}{{3.33}{124}{Top-1 Accuracy of zero-shot transfer of models to image classification on ImageNet and its variants.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.34}{\ignorespaces Zero-shot image and text retrieval \citep  {yuan2021florence}.\relax }}{124}{figure.caption.64}\protected@file@percent }
\newlabel{fig:table2}{{3.34}{124}{Zero-shot image and text retrieval \citep {yuan2021florence}.\relax }{figure.caption.64}{}}
\citation{ramesh2022hierarchical}
\citation{schuhmann2022laion}
\@writefile{lof}{\contentsline {figure}{\numberline {3.35}{\ignorespaces Top-1 Accuracy of CLIP, Florence and ALIGN on various datasets.\relax }}{125}{figure.caption.65}\protected@file@percent }
\newlabel{fig:table3}{{3.35}{125}{Top-1 Accuracy of CLIP, Florence and ALIGN on various datasets.\relax }{figure.caption.65}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Resources}{125}{subsection.3.4.5}\protected@file@percent }
\newlabel{resources}{{3.4.5}{125}{Resources}{subsection.3.4.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Text + Image}{125}{section.3.5}\protected@file@percent }
\newlabel{c02-05-text-plus-img}{{3.5}{125}{Text + Image}{section.3.5}{}}
\citation{uppal2022multimodal}
\citation{baevski2022data2vec}
\@writefile{lof}{\contentsline {figure}{\numberline {3.36}{\ignorespaces Uppal et al. (2022): VisLang Paper Trends (previous 2 years)\relax }}{127}{figure.caption.66}\protected@file@percent }
\newlabel{fig:vltasks}{{3.36}{127}{Uppal et al. (2022): VisLang Paper Trends (previous 2 years)\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Data2vec}{127}{subsection.3.5.1}\protected@file@percent }
\newlabel{data2vec}{{3.5.1}{127}{Data2vec}{subsection.3.5.1}{}}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {3.37}{\ignorespaces Baevski et al. (2022): Data2vec Architecture - a teacher model creates contextualized latent targets on the basis of its top K layers (blue) as prediction task to train the student model\relax }}{128}{figure.caption.67}\protected@file@percent }
\newlabel{fig:data2vecoverview}{{3.37}{128}{Baevski et al. (2022): Data2vec Architecture - a teacher model creates contextualized latent targets on the basis of its top K layers (blue) as prediction task to train the student model\relax }{figure.caption.67}{}}
\citation{vaswani2017attention}
\citation{bao2021beit}
\citation{liu2019roberta}
\citation{sennrich2015neural}
\citation{baevski2020wav2vec}
\citation{wang2018glue}
\citation{chen2021empirical}
\citation{caron2021emerging}
\citation{he2022masked}
\citation{wei2022masked}
\@writefile{lof}{\contentsline {figure}{\numberline {3.38}{\ignorespaces Dosovitskiy et al. (2021)\relax }}{130}{figure.caption.68}\protected@file@percent }
\newlabel{fig:visiontransformer}{{3.38}{130}{Dosovitskiy et al. (2021)\relax }{figure.caption.68}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.39}{\ignorespaces Baevski et al. (2022): data2vec performance (vision)\relax }}{130}{figure.caption.69}\protected@file@percent }
\newlabel{fig:data2vecresults1}{{3.39}{130}{Baevski et al. (2022): data2vec performance (vision)\relax }{figure.caption.69}{}}
\citation{wang2018glue}
\@writefile{lof}{\contentsline {figure}{\numberline {3.40}{\ignorespaces Baevski et al. (2022): data2vec results (language)\relax }}{131}{figure.caption.70}\protected@file@percent }
\newlabel{fig:data2vecresults2}{{3.40}{131}{Baevski et al. (2022): data2vec results (language)\relax }{figure.caption.70}{}}
\citation{jaegle2021perceiver}
\citation{lu2019vilbert}
\citation{devlin2018bert}
\citation{ren2015faster}
\citation{krishnavisualgenome}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Vision-and-Language Bert (VilBert)}{133}{subsection.3.5.2}\protected@file@percent }
\newlabel{vision-and-language-bert-vilbert}{{3.5.2}{133}{Vision-and-Language Bert (VilBert)}{subsection.3.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.41}{\ignorespaces Lu et al. (2019): VilBert's Dual Stream Architecture: dashed transformer modules can be repeated, co-attention modules allow sparse interaction between modalities.\relax }}{133}{figure.caption.71}\protected@file@percent }
\newlabel{fig:vilbertarc}{{3.41}{133}{Lu et al. (2019): VilBert's Dual Stream Architecture: dashed transformer modules can be repeated, co-attention modules allow sparse interaction between modalities.\relax }{figure.caption.71}{}}
\citation{lu2019vilbert}
\citation{sikarwar2022efficacy}
\citation{das2017human}
\citation{sikarwar2022efficacy}
\@writefile{lof}{\contentsline {figure}{\numberline {3.42}{\ignorespaces Lu et al. (2019): Cross-Attention in VilBert\relax }}{134}{figure.caption.72}\protected@file@percent }
\newlabel{fig:vilbertattention}{{3.42}{134}{Lu et al. (2019): Cross-Attention in VilBert\relax }{figure.caption.72}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.43}{\ignorespaces Sikarwar et al. (2022): (Left to Right) Picture, Human Attention, 36 Regions, 72 Regions, 108 Regions. Similarity between human and model attention is measured using rank correlation.\relax }}{135}{figure.caption.73}\protected@file@percent }
\newlabel{fig:vilbertmaps}{{3.43}{135}{Sikarwar et al. (2022): (Left to Right) Picture, Human Attention, 36 Regions, 72 Regions, 108 Regions. Similarity between human and model attention is measured using rank correlation.\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.44}{\ignorespaces Lu et al. (2019): VilBert Performance\relax }}{136}{figure.caption.74}\protected@file@percent }
\newlabel{fig:vilbertresults}{{3.44}{136}{Lu et al. (2019): VilBert Performance\relax }{figure.caption.74}{}}
\citation{alayrac2022flamingo}
\citation{hoffmann2022training}
\citation{brown2020language}
\citation{jaegle2021perceiver}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Flamingo}{137}{subsection.3.5.3}\protected@file@percent }
\newlabel{flamingo}{{3.5.3}{137}{Flamingo}{subsection.3.5.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.45}{\ignorespaces Alayrac et al. (2022): Flamingo Prompt-Output-Examples\relax }}{138}{figure.caption.75}\protected@file@percent }
\newlabel{fig:flamingoexamples}{{3.45}{138}{Alayrac et al. (2022): Flamingo Prompt-Output-Examples\relax }{figure.caption.75}{}}
\citation{jia2021scaling}
\@writefile{lof}{\contentsline {figure}{\numberline {3.46}{\ignorespaces Alayrac et al. (2022): Flamingo Model Structure\relax }}{139}{figure.caption.76}\protected@file@percent }
\newlabel{fig:flamingoarc}{{3.46}{139}{Alayrac et al. (2022): Flamingo Model Structure\relax }{figure.caption.76}{}}
\citation{jaegle2021perceiver}
\citation{radford2021learning}
\@writefile{lof}{\contentsline {figure}{\numberline {3.47}{\ignorespaces Alayrac et al. (2022): Flamingo Perceiver-Resampler\relax }}{141}{figure.caption.77}\protected@file@percent }
\newlabel{fig:perceiver}{{3.47}{141}{Alayrac et al. (2022): Flamingo Perceiver-Resampler\relax }{figure.caption.77}{}}
\citation{perez2021true}
\citation{perez2021true}
\@writefile{lof}{\contentsline {figure}{\numberline {3.48}{\ignorespaces Alayrac et al. (2022): Flamingo Gated Cross-Attention\relax }}{142}{figure.caption.78}\protected@file@percent }
\newlabel{fig:flamingoattention}{{3.48}{142}{Alayrac et al. (2022): Flamingo Gated Cross-Attention\relax }{figure.caption.78}{}}
\citation{brown2020language}
\@writefile{lof}{\contentsline {figure}{\numberline {3.49}{\ignorespaces Alayrac et al. (2022): Flamingo Datasets (Table2, p.19)\relax }}{143}{figure.caption.79}\protected@file@percent }
\newlabel{fig:flamingodatasets}{{3.49}{143}{Alayrac et al. (2022): Flamingo Datasets (Table2, p.19)\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.50}{\ignorespaces Alayrac et al. (2022): Flamingo Results without Fine-Tuning\relax }}{143}{figure.caption.80}\protected@file@percent }
\newlabel{fig:flamingoresult}{{3.50}{143}{Alayrac et al. (2022): Flamingo Results without Fine-Tuning\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.51}{\ignorespaces Alayrac et al. (2022): Flamingo Results with Fine-Tuning\relax }}{144}{figure.caption.81}\protected@file@percent }
\newlabel{fig:flamingfinetune}{{3.51}{144}{Alayrac et al. (2022): Flamingo Results with Fine-Tuning\relax }{figure.caption.81}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Discussion}{144}{subsection.3.5.4}\protected@file@percent }
\newlabel{discussion-2}{{3.5.4}{144}{Discussion}{subsection.3.5.4}{}}
\citation{zeng2022socratic}
\citation{perez2021true}
\citation{chowdhery2022palm}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Further Topics}{147}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{c03-00-further}{{4}{147}{Further Topics}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Including Further Modalities}{147}{section.4.1}\protected@file@percent }
\newlabel{c03-01-further-modalities}{{4.1}{147}{Including Further Modalities}{section.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Intro}{147}{subsection.4.1.1}\protected@file@percent }
\newlabel{intro}{{4.1.1}{147}{Intro}{subsection.4.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Motivation}{148}{subsection.4.1.2}\protected@file@percent }
\newlabel{motivation}{{4.1.2}{148}{Motivation}{subsection.4.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Taxonomy of Multimodal Challenges}{149}{subsection.4.1.3}\protected@file@percent }
\newlabel{taxonomy-of-multimodal-challenges}{{4.1.3}{149}{Taxonomy of Multimodal Challenges}{subsection.4.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3.1}Multimodal Representation Learning}{149}{subsubsection.4.1.3.1}\protected@file@percent }
\newlabel{multimodal-representation-learning}{{4.1.3.1}{149}{Multimodal Representation Learning}{subsubsection.4.1.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.1.3.1.1}Joint Representations}{149}{paragraph.4.1.3.1.1}\protected@file@percent }
\newlabel{joint-representations}{{4.1.3.1.1}{149}{Joint Representations}{paragraph.4.1.3.1.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.1.3.1.2}Coordinated Representation}{149}{paragraph.4.1.3.1.2}\protected@file@percent }
\newlabel{coordinated-representation}{{4.1.3.1.2}{149}{Coordinated Representation}{paragraph.4.1.3.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3.2}Multimodal Translation}{150}{subsubsection.4.1.3.2}\protected@file@percent }
\newlabel{multimodal-translation}{{4.1.3.2}{150}{Multimodal Translation}{subsubsection.4.1.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3.3}Multimodal Alignment}{150}{subsubsection.4.1.3.3}\protected@file@percent }
\newlabel{multimodal-alignment}{{4.1.3.3}{150}{Multimodal Alignment}{subsubsection.4.1.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3.4}Multimodal Fusion}{150}{subsubsection.4.1.3.4}\protected@file@percent }
\newlabel{multimodal-fusion}{{4.1.3.4}{150}{Multimodal Fusion}{subsubsection.4.1.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}General Multimodal Architectures}{150}{subsection.4.1.4}\protected@file@percent }
\newlabel{general-multimodal-architectures}{{4.1.4}{150}{General Multimodal Architectures}{subsection.4.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Multimodal Training Paradigms}{151}{subsection.4.1.5}\protected@file@percent }
\newlabel{multimodal-training-paradigms}{{4.1.5}{151}{Multimodal Training Paradigms}{subsection.4.1.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5.1}Modality-Agnostic Uni-Modal SSL}{151}{subsubsection.4.1.5.1}\protected@file@percent }
\newlabel{modality-agnostic-uni-modal-ssl}{{4.1.5.1}{151}{Modality-Agnostic Uni-Modal SSL}{subsubsection.4.1.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5.2}Generalized Cross-Modal SSL}{151}{subsubsection.4.1.5.2}\protected@file@percent }
\newlabel{generalized-cross-modal-ssl}{{4.1.5.2}{151}{Generalized Cross-Modal SSL}{subsubsection.4.1.5.2}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.1.5.2.1}Contrastive Methods}{151}{paragraph.4.1.5.2.1}\protected@file@percent }
\newlabel{contrastive-methods}{{4.1.5.2.1}{151}{Contrastive Methods}{paragraph.4.1.5.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.1.5.2.2}Non-Contrastive Methods}{151}{paragraph.4.1.5.2.2}\protected@file@percent }
\newlabel{non-contrastive-methods}{{4.1.5.2.2}{151}{Non-Contrastive Methods}{paragraph.4.1.5.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Combining General Architectures and Training Paradigms}{151}{subsection.4.1.6}\protected@file@percent }
\newlabel{combining-general-architectures-and-training-paradigms}{{4.1.6}{151}{Combining General Architectures and Training Paradigms}{subsection.4.1.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Structured + Unstructured Data}{151}{section.4.2}\protected@file@percent }
\newlabel{c03-02-structured-unstructured}{{4.2}{151}{Structured + Unstructured Data}{section.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Intro}{151}{subsection.4.2.1}\protected@file@percent }
\newlabel{intro-1}{{4.2.1}{151}{Intro}{subsection.4.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Taxonomy: Structured vs.\nobreakspace  {}Unstructured Data}{152}{subsection.4.2.2}\protected@file@percent }
\newlabel{taxonomy-structured-vs.-unstructured-data}{{4.2.2}{152}{Taxonomy: Structured vs.~Unstructured Data}{subsection.4.2.2}{}}
\citation{HuangFusion2020}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Structured vs. Unstructured Data\relax }}{153}{figure.caption.82}\protected@file@percent }
\newlabel{fig:struc-vs-unstrc}{{4.1}{153}{Structured vs. Unstructured Data\relax }{figure.caption.82}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Fusion Strategies}{153}{subsection.4.2.3}\protected@file@percent }
\newlabel{fusion-strategies}{{4.2.3}{153}{Fusion Strategies}{subsection.4.2.3}{}}
\citation{HuangFusion2020}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Data Modality Fusion Strategies \citep  [Adopted from][]{HuangFusion2020}.\relax }}{154}{figure.caption.83}\protected@file@percent }
\newlabel{fig:fusion-strategies}{{4.2}{154}{Data Modality Fusion Strategies \citep [Adopted from][]{HuangFusion2020}.\relax }{figure.caption.83}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Applications}{154}{subsection.4.2.4}\protected@file@percent }
\newlabel{applications}{{4.2.4}{154}{Applications}{subsection.4.2.4}{}}
\citation{Katzman2018}
\citation{DeepConvSurv}
\citation{DeepCorrSurv}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.1}Multimodal DL in Survival}{155}{subsubsection.4.2.4.1}\protected@file@percent }
\newlabel{multimodal-dl-in-survival}{{4.2.4.1}{155}{Multimodal DL in Survival}{subsubsection.4.2.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.2}Traditional Survival Analysis (CPH Model)}{155}{subsubsection.4.2.4.2}\protected@file@percent }
\newlabel{traditional-survival-analysis-cph-model}{{4.2.4.2}{155}{Traditional Survival Analysis (CPH Model)}{subsubsection.4.2.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.3}Multimodal DL Survival Analysis}{155}{subsubsection.4.2.4.3}\protected@file@percent }
\newlabel{multimodal-dl-survival-analysis}{{4.2.4.3}{155}{Multimodal DL Survival Analysis}{subsubsection.4.2.4.3}{}}
\citation{TongAE}
\citation{TongAE}
\citation{Cheerla2019}
\citation{Cheerla2019}
\citation{TongAE}
\citation{Cheerla2019}
\citation{MultiSurv2021}
\citation{MultiSurv2021}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces a) Architecture with Similarity Loss b) T-SNE-Mapped Representations of Latent Features (Colored by Cancer Type) \citep  {Cheerla2019}.\relax }}{157}{figure.caption.84}\protected@file@percent }
\newlabel{fig:cheerla-model}{{4.3}{157}{a) Architecture with Similarity Loss b) T-SNE-Mapped Representations of Latent Features (Colored by Cancer Type) \citep {Cheerla2019}.\relax }{figure.caption.84}{}}
\citation{WideDeepNN2016}
\citation{WideDeepNN2016}
\citation{Poelsterl2020}
\citation{DeepPAMM2022}
\citation{Poelsterl2020}
\citation{DeepPAMM2022}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Illustration of Wide \& Deep Neural Networks \citep  {WideDeepNN2016}.\relax }}{158}{figure.caption.85}\protected@file@percent }
\newlabel{fig:wide-deep-nn}{{4.4}{158}{Illustration of Wide \& Deep Neural Networks \citep {WideDeepNN2016}.\relax }{figure.caption.85}{}}
\citation{SSDDR2020}
\citation{DeepPAMM2022}
\citation{SSDDR2020}
\citation{SSDDR2020}
\citation{SSDDR2020}
\citation{SSDDR2020}
\citation{SSDDR2020}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.4}Multimodal DL in Other Scientific Fields}{159}{subsubsection.4.2.4.4}\protected@file@percent }
\newlabel{multimodal-dl-in-other-scientific-fields}{{4.2.4.4}{159}{Multimodal DL in Other Scientific Fields}{subsubsection.4.2.4.4}{}}
\citation{Law2019}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Architecture of SSDDR (X+Z (Struct.) and U (Unstruct.) Data) \citep  {SSDDR2020}.\relax }}{160}{figure.caption.86}\protected@file@percent }
\newlabel{fig:SSDDR}{{4.5}{160}{Architecture of SSDDR (X+Z (Struct.) and U (Unstruct.) Data) \citep {SSDDR2020}.\relax }{figure.caption.86}{}}
\citation{Law2019}
\citation{Jean2016}
\citation{Gebru2017}
\citation{DeepGPYou2017}
\citation{Sirko2021}
\citation{DeepGPYou2017}
\citation{Sirko2021}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Fully Nonlinear and Semi-Interpretable Models (X (Struct.) and S+A (Unstruct.) Data) \citep  {Law2019}.\relax }}{161}{figure.caption.87}\protected@file@percent }
\newlabel{fig:model-heads}{{4.6}{161}{Fully Nonlinear and Semi-Interpretable Models (X (Struct.) and S+A (Unstruct.) Data) \citep {Law2019}.\relax }{figure.caption.87}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Conclusion and Outlook}{162}{subsection.4.2.5}\protected@file@percent }
\newlabel{conclusion-and-outlook}{{4.2.5}{162}{Conclusion and Outlook}{subsection.4.2.5}{}}
\citation{vaswani2017attention}
\citation{Devlin2018}
\citation{dosovitskiy2020image}
\citation{Dean21}
\citation{Crawshaw2020}
\citation{Baltrusaitis2019}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Multi-Purpose Models}{163}{section.4.3}\protected@file@percent }
\newlabel{c03-03-multi-purpose}{{4.3}{163}{Multi-Purpose Models}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Intro}{163}{subsection.4.3.1}\protected@file@percent }
\newlabel{intro-2}{{4.3.1}{163}{Intro}{subsection.4.3.1}{}}
\citation{Kaiser2017}
\citation{Jacobs1991}
\citation{Jordan1994}
\citation{Shaazer2017}
\citation{mccoco}
\citation{ImageNet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1.1}Multi-Purpose Models}{164}{subsubsection.4.3.1.1}\protected@file@percent }
\newlabel{multi-purpose-models}{{4.3.1.1}{164}{Multi-Purpose Models}{subsubsection.4.3.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Previous Work}{164}{subsection.4.3.2}\protected@file@percent }
\newlabel{previous-work}{{4.3.2}{164}{Previous Work}{subsection.4.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.1}MultiModel}{164}{subsubsection.4.3.2.1}\protected@file@percent }
\newlabel{multimodel}{{4.3.2.1}{164}{MultiModel}{subsubsection.4.3.2.1}{}}
\citation{Hu2021}
\citation{vaswani2017attention}
\citation{Devlin2018}
\citation{Carion2020}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.2}Unified Transformer (UniT)}{165}{subsubsection.4.3.2.2}\protected@file@percent }
\newlabel{unified-transformer-unit}{{4.3.2.2}{165}{Unified Transformer (UniT)}{subsubsection.4.3.2.2}{}}
\citation{Wang2022}
\citation{dosovitskiy2020image}
\citation{sennrich-etal-2016-neural}
\citation{pmlr-v139-ramesh21a}
\citation{lewis-etal-2020-bart}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Architecture of *MultiModel*. The outer boxes without text are the modality nets. From @Kaiser2017.\relax }}{166}{figure.caption.88}\protected@file@percent }
\newlabel{fig:multimodel}{{4.7}{166}{Architecture of *MultiModel*. The outer boxes without text are the modality nets. From @Kaiser2017.\relax }{figure.caption.88}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.3}OFA - Once For All}{166}{subsubsection.4.3.2.3}\protected@file@percent }
\newlabel{ofa---once-for-all}{{4.3.2.3}{166}{OFA - Once For All}{subsubsection.4.3.2.3}{}}
\citation{Reed2022}
\citation{ResNet}
\citation{kudo-richardson-2018-sentencepiece}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Modified transformer for UniT. The decoder follows the implementation of DETR [@Carion2020]. From @Hu2021.\relax }}{167}{figure.caption.89}\protected@file@percent }
\newlabel{fig:unit}{{4.8}{167}{Modified transformer for UniT. The decoder follows the implementation of DETR [@Carion2020]. From @Hu2021.\relax }{figure.caption.89}{}}
\citation{atari}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces *OFA*, the different input and output concepts can be seen here. From @Wang2022.\relax }}{168}{figure.caption.90}\protected@file@percent }
\newlabel{fig:ofa}{{4.9}{168}{*OFA*, the different input and output concepts can be seen here. From @Wang2022.\relax }{figure.caption.90}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.4}Gato - A Generalist Agent}{168}{subsubsection.4.3.2.4}\protected@file@percent }
\newlabel{gato---a-generalist-agent}{{4.3.2.4}{168}{Gato - A Generalist Agent}{subsubsection.4.3.2.4}{}}
\citation{Dean21}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2.5}Comparison}{169}{subsubsection.4.3.2.5}\protected@file@percent }
\newlabel{comparison}{{4.3.2.5}{169}{Comparison}{subsubsection.4.3.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Pathway Proposal}{169}{subsection.4.3.3}\protected@file@percent }
\newlabel{pathway-proposal}{{4.3.3}{169}{Pathway Proposal}{subsection.4.3.3}{}}
\citation{Dean20}
\citation{Lewkowycz2022}
\citation{parti}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces Concept of pathways. Different tasks follow different paths to different expert models. From @Dean21, [Screenshot August 31th 2022](https://www.youtube.com/watch?v=Nf-d9CcEZ2w).\relax }}{170}{figure.caption.91}\protected@file@percent }
\newlabel{fig:pathways}{{4.10}{170}{Concept of pathways. Different tasks follow different paths to different expert models. From @Dean21, [Screenshot August 31th 2022](https://www.youtube.com/watch?v=Nf-d9CcEZ2w).\relax }{figure.caption.91}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3.1}Idea}{170}{subsubsection.4.3.3.1}\protected@file@percent }
\newlabel{idea}{{4.3.3.1}{170}{Idea}{subsubsection.4.3.3.1}{}}
\citation{Fernando2017}
\citation{Baeck1993}
\citation{Doerr2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Related Work to Pathways}{171}{subsection.4.3.4}\protected@file@percent }
\newlabel{related-work-to-pathways}{{4.3.4}{171}{Related Work to Pathways}{subsection.4.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4.1}PathNet}{171}{subsubsection.4.3.4.1}\protected@file@percent }
\newlabel{pathnet}{{4.3.4.1}{171}{PathNet}{subsubsection.4.3.4.1}{}}
\citation{Mustafa2022}
\citation{kudo-richardson-2018-sentencepiece}
\citation{dosovitskiy2020image}
\citation{Riquelme2021}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4.2}LIMoE}{172}{subsubsection.4.3.4.2}\protected@file@percent }
\newlabel{limoe}{{4.3.4.2}{172}{LIMoE}{subsubsection.4.3.4.2}{}}
\citation{radford2021learning}
\citation{deng2009imagenet}
\citation{mccoco}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Training *PathNet* on two tasks. At first random paths are initialized (1), then trained (2-3) and fixed (4). The same procedure is repeated for the next paths using the previously fixed paths and new parameters in all other nodes (5-9). From @Fernando2017.\relax }}{173}{figure.caption.92}\protected@file@percent }
\newlabel{fig:pathnet}{{4.11}{173}{Training *PathNet* on two tasks. At first random paths are initialized (1), then trained (2-3) and fixed (4). The same procedure is repeated for the next paths using the previously fixed paths and new parameters in all other nodes (5-9). From @Fernando2017.\relax }{figure.caption.92}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Architecture of *LIMoE*. From @Mustafa2022.\relax }}{174}{figure.caption.93}\protected@file@percent }
\newlabel{fig:LIMoE}{{4.12}{174}{Architecture of *LIMoE*. From @Mustafa2022.\relax }{figure.caption.93}{}}
\citation{Gesmundo2022a}
\citation{Dean21}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4.3}muNet (Multitask Network)}{175}{subsubsection.4.3.4.3}\protected@file@percent }
\newlabel{munet-multitask-network}{{4.3.4.3}{175}{muNet (Multitask Network)}{subsubsection.4.3.4.3}{}}
\citation{Rebuffi2017}
\citation{Steiner2021}
\citation{Dean21}
\citation{Chowdhery2022}
\citation{brown2020language}
\citation{Hinton2015}
\citation{Reed2022}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Conclusion Pathways}{177}{subsection.4.3.5}\protected@file@percent }
\newlabel{conclusion-pathways}{{4.3.5}{177}{Conclusion Pathways}{subsection.4.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Discussion}{177}{subsection.4.3.6}\protected@file@percent }
\newlabel{discussion-3}{{4.3.6}{177}{Discussion}{subsection.4.3.6}{}}
\citation{galanter2016generative}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Generative Art}{178}{section.4.4}\protected@file@percent }
\newlabel{c03-04-usecase}{{4.4}{178}{Generative Art}{section.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces LMU logo in style of Van Gogh's Sunflower painting\relax }}{178}{figure.caption.94}\protected@file@percent }
\newlabel{fig:Logo}{{4.13}{178}{LMU logo in style of Van Gogh's Sunflower painting\relax }{figure.caption.94}{}}
\citation{mordvintsev_2015}
\citation{StyleTransfer}
\citation{NIPS2014_5ca3e9b1}
\citation{karras2019style}
\citation{8477754}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Historical Overview}{179}{subsection.4.4.1}\protected@file@percent }
\newlabel{historical-overview}{{4.4.1}{179}{Historical Overview}{subsection.4.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Picture of a Labrador processed by DeepDream (\href  {https://www.tensorflow.org/tutorials/generative/deepdream}{Google Colab})\relax }}{179}{figure.caption.95}\protected@file@percent }
\newlabel{fig:DeepDream}{{4.14}{179}{Picture of a Labrador processed by DeepDream (\href {https://www.tensorflow.org/tutorials/generative/deepdream}{Google Colab})\relax }{figure.caption.95}{}}
\citation{DALLE}
\citation{CLIP}
\citation{StyleGAN}
\citation{DiffusionModels}
\citation{GLIDE}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Picture of a Labrador with Kandinsky style \href  {https://www.tensorflow.org/tutorials/generative/style_transfer}{(Google Colab)}\relax }}{180}{figure.caption.96}\protected@file@percent }
\newlabel{fig:StyleTransfer2}{{4.15}{180}{Picture of a Labrador with Kandinsky style \href {https://www.tensorflow.org/tutorials/generative/style_transfer}{(Google Colab)}\relax }{figure.caption.96}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Fake face generated by \href  {https://thispersondoesnotexist.com/}{StyleGAN}\relax }}{180}{figure.caption.97}\protected@file@percent }
\newlabel{fig:GAN}{{4.16}{180}{Fake face generated by \href {https://thispersondoesnotexist.com/}{StyleGAN}\relax }{figure.caption.97}{}}
\citation{unrealEngine}
\citation{ruDALLE}
\citation{DALLEmini}
\citation{DALLEpytorch}
\citation{LAION}
\citation{GLIDE}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}How to use models?}{181}{subsection.4.4.2}\protected@file@percent }
\newlabel{how-to-use-models}{{4.4.2}{181}{How to use models?}{subsection.4.4.2}{}}
\citation{qiao2022initial}
\citation{GLIDE}
\citation{GLIDE}
\citation{WZRD}
\citation{3D}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Comparison of different models with prompt ``fall landscape with a small cottage next to a lake''\relax }}{182}{figure.caption.98}\protected@file@percent }
\newlabel{fig:comparison1}{{4.17}{182}{Comparison of different models with prompt ``fall landscape with a small cottage next to a lake''\relax }{figure.caption.98}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Different tasks and modalities}{182}{subsection.4.4.3}\protected@file@percent }
\newlabel{different-tasks-and-modalities}{{4.4.3}{182}{Different tasks and modalities}{subsection.4.4.3}{}}
\citation{GLIDE}
\citation{GLIDE}
\citation{bias}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Comparison of different models with prompt ``panda mad scientist mixing sparkling chemicals, artstation''\relax }}{183}{figure.caption.99}\protected@file@percent }
\newlabel{fig:comparison2}{{4.18}{183}{Comparison of different models with prompt ``panda mad scientist mixing sparkling chemicals, artstation''\relax }{figure.caption.99}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Discussion and prospects}{183}{subsection.4.4.4}\protected@file@percent }
\newlabel{discussion-and-prospects}{{4.4.4}{183}{Discussion and prospects}{subsection.4.4.4}{}}
\citation{misconduct}
\citation{bias_ML}
\citation{bias}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Text-conditional image inpainting examples with GLIDE \citep  {GLIDE}\relax }}{184}{figure.caption.100}\protected@file@percent }
\newlabel{fig:inpainting}{{4.19}{184}{Text-conditional image inpainting examples with GLIDE \citep {GLIDE}\relax }{figure.caption.100}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Text-conditional edit from user scratch with GLIDE \citep  {GLIDE}\relax }}{184}{figure.caption.101}\protected@file@percent }
\newlabel{fig:sketch}{{4.20}{184}{Text-conditional edit from user scratch with GLIDE \citep {GLIDE}\relax }{figure.caption.101}{}}
\citation{NFT}
\citation{DALLE2}
\citation{explainaility}
\citation{bias}
\citation{explainaility}
\citation{Radford2019LanguageMA}
\citation{GPT3}
\citation{environment}
\citation{environment}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{187}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{conclusion-1}{{5}{187}{Conclusion}{chapter.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Epilogue}{189}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{epilogue}{{6}{189}{Epilogue}{chapter.6}{}}
\bibdata{book.bib,packages.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Acknowledgements}{191}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{acknowledgements}{{7}{191}{Acknowledgements}{chapter.7}{}}
\bibcite{agirre2009study}{{Agirre et~al.}{}{{}}{{}}}
\bibcite{ailem2018probabilistic}{{Ailem et~al.}{}{{}}{{}}}
\bibcite{alayrac2022flamingo}{{3}{a}{{Alayrac et~al.}}{{}}}
\bibcite{Flamingo}{{4}{b}{{Alayrac et~al.}}{{}}}
\bibcite{alford2021alignparams}{{Alford}{}{{}}{{}}}
\bibcite{spice}{{6}{a}{{Anderson et~al.}}{{}}}
\bibcite{8578734}{{7}{b}{{Anderson et~al.}}{{}}}
\bibcite{antol2015vqa}{{Antol et~al.}{}{{}}{{}}}
\bibcite{unrealEngine}{{Aran}{}{{}}{{}}}
\bibcite{baevski2022data2vec}{{10}{a}{{Baevski et~al.}}{{}}}
\@writefile{toc}{\contentsline {fm}{Bibliography}{193}{chapter*.102}\protected@file@percent }
\bibcite{baevski2020wav2vec}{{11}{b}{{Baevski et~al.}}{{}}}
\bibcite{Baltrusaitis2019}{{Baltrušaitis et~al.}{}{{}}{{}}}
\bibcite{bandy2021addressing}{{Bandy and Vincent}{}{{}}{{}}}
\bibcite{meteor}{{Banerjee and Lavie}{}{{}}{{}}}
\bibcite{bao2021beit}{{Bao et~al.}{}{{}}{{}}}
\bibcite{Pathways}{{Barham et~al.}{}{{}}{{}}}
\bibcite{atari}{{Bellemare et~al.}{}{{}}{{}}}
\bibcite{beyer2020we}{{Beyer et~al.}{}{{}}{{}}}
\bibcite{birhane2021multimodal}{{Birhane et~al.}{}{{}}{{}}}
\bibcite{DALLEmini}{{Boris}{}{{}}{{}}}
\bibcite{bosch2007image}{{Bosch et~al.}{}{{}}{{}}}
\bibcite{bowman2021will}{{Bowman and Dahl}{}{{}}{{}}}
\bibcite{bromley1993signature}{{Bromley et~al.}{}{{}}{{}}}
\bibcite{brown2020language}{{24}{a}{{Brown et~al.}}{{}}}
\bibcite{GPT3}{{25}{b}{{Brown et~al.}}{{}}}
\bibcite{bruni2014multimodal}{{Bruni et~al.}{}{{}}{{}}}
\bibcite{brysbaert2014concreteness}{{Brysbaert et~al.}{}{{}}{{}}}
\bibcite{Baeck1993}{{Bäck and Schwefel}{}{{}}{{}}}
\bibcite{Carion2020}{{Carion et~al.}{}{{}}{{}}}
\bibcite{caron2021emerging}{{Caron et~al.}{}{{}}{{}}}
\bibcite{SimCLR}{{31}{a}{{Chen et~al.}}{{}}}
\bibcite{chen2021empirical}{{32}{b}{{Chen et~al.}}{{}}}
\bibcite{chowdhery2022palm}{{33}{a}{{Chowdhery et~al.}}{{}}}
\bibcite{Chowdhery2022}{{34}{b}{{Chowdhery et~al.}}{{}}}
\bibcite{collell2017imagined}{{Collell et~al.}{}{{}}{{}}}
\bibcite{meshed_memory}{{Cornia et~al.}{}{{}}{{}}}
\bibcite{Crawshaw2020}{{Crawshaw}{}{{}}{{}}}
\bibcite{das2017human}{{Das et~al.}{}{{}}{{}}}
\bibcite{Dean20}{{39}{a}{{Dean}}{{}}}
\bibcite{Dean21}{{40}{b}{{Dean}}{{}}}
\bibcite{misconduct}{{Dehouche}{}{{}}{{}}}
\bibcite{deng2009imagenet}{{Deng et~al.}{}{{}}{{}}}
\bibcite{devereux2014centre}{{Devereux et~al.}{}{{}}{{}}}
\bibcite{BERT}{{44}{a}{{Devlin et~al.}}{{}}}
\bibcite{Devlin2018}{{45}{b}{{Devlin et~al.}}{{}}}
\bibcite{devlin-etal-2019-bert}{{46}{c}{{Devlin et~al.}}{{}}}
\bibcite{devlin2018bert}{{47}{d}{{Devlin et~al.}}{{}}}
\bibcite{dhamala2021bold}{{Dhamala et~al.}{}{{}}{{}}}
\bibcite{DiffusionModels}{{Dhariwal and Nichol}{}{{}}{{}}}
\bibcite{Doerr2021}{{Doerr and Neumann}{}{{}}{{}}}
\bibcite{dosovitskiy2020image}{{Dosovitskiy et~al.}{}{{}}{{}}}
\bibcite{bias}{{52}{a}{{Esser et~al.}}{{}}}
\bibcite{esser2021taming}{{53}{b}{{Esser et~al.}}{{}}}
\bibcite{pascalvoc}{{Everingham et~al.}{}{{}}{{}}}
\bibcite{WordNet}{{Fellbaum}{}{{}}{{}}}
\bibcite{Fernando2017}{{Fernando et~al.}{}{{}}{{}}}
\bibcite{galanter2016generative}{{Galanter}{}{{}}{{}}}
\bibcite{gao2017knowledge}{{Gao et~al.}{}{{}}{{}}}
\bibcite{StyleTransfer}{{Gatys et~al.}{}{{}}{{}}}
\bibcite{Gesmundo2022a}{{Gesmundo and Dean}{}{{}}{{}}}
\bibcite{Gokaslan2019OpenWeb}{{Gokaslan and Cohen}{}{{}}{{}}}
\bibcite{NIPS2014_5ca3e9b1}{{62}{a}{{Goodfellow et~al.}}{{}}}
\bibcite{GAN}{{63}{b}{{Goodfellow et~al.}}{{}}}
\bibcite{goodfellow2014explaining}{{64}{c}{{Goodfellow et~al.}}{{}}}
\bibcite{goyal2017making}{{Goyal et~al.}{}{{}}{{}}}
\bibcite{grill2020bootstrap}{{66}{a}{{Grill et~al.}}{{}}}
\bibcite{BYOL}{{67}{b}{{Grill et~al.}}{{}}}
\bibcite{guo2016ms}{{Guo et~al.}{}{{}}{{}}}
\bibcite{harnad1990symbol}{{Harnad}{}{{}}{{}}}
\bibcite{harris1954distributional}{{Harris et~al.}{}{{}}{{}}}
\bibcite{he2022masked}{{71}{a}{{He et~al.}}{{}}}
\bibcite{ResNet}{{72}{b}{{He et~al.}}{{}}}
\bibcite{henderson2020towards}{{Henderson et~al.}{}{{}}{{}}}
\bibcite{HerdadeKBS19}{{Herdade et~al.}{}{{}}{{}}}
\bibcite{hill2014learning}{{Hill and Korhonen}{}{{}}{{}}}
\bibcite{hill2015simlex}{{Hill et~al.}{}{{}}{{}}}
\bibcite{Hinton2015}{{Hinton et~al.}{}{{}}{{}}}
\bibcite{hochreiter1997long}{{Hochreiter and Schmidhuber}{}{{}}{{}}}
\bibcite{hoffmann2022training}{{Hoffmann et~al.}{}{{}}{{}}}
\bibcite{hu2021unit}{{80}{a}{{Hu and Singh}}{{}}}
\bibcite{Hu2021}{{81}{b}{{Hu and Singh}}{{}}}
\bibcite{huang1}{{Huang et~al.}{}{{}}{{}}}
\bibcite{hudson2019gqa}{{Hudson and Manning}{}{{}}{{}}}
\bibcite{ive2019distilling}{{Ive et~al.}{}{{}}{{}}}
\bibcite{Jacobs1991}{{Jacobs et~al.}{}{{}}{{}}}
\bibcite{jaegle2021perceiver}{{Jaegle et~al.}{}{{}}{{}}}
\bibcite{ALIGN}{{87}{a}{{Jia et~al.}}{{}}}
\bibcite{jia2021scaling}{{88}{b}{{Jia et~al.}}{{}}}
\bibcite{Jordan1994}{{Jordan and Jacobs}{}{{}}{{}}}
\bibcite{explainaility}{{Joshi et~al.}{}{{}}{{}}}
\bibcite{Kaiser2017}{{Kaiser et~al.}{}{{}}{{}}}
\bibcite{karpthy1}{{Karpathy and Fei-Fei}{}{{}}{{}}}
\bibcite{karras2019style}{{Karras et~al.}{}{{}}{{}}}
\bibcite{kiela2014learning}{{Kiela and Bottou}{}{{}}{{}}}
\bibcite{kiela2017learning}{{Kiela et~al.}{}{{}}{{}}}
\bibcite{VAE}{{Kingma and Welling}{}{{}}{{}}}
\bibcite{kiros2018illustrative}{{Kiros et~al.}{}{{}}{{}}}
\bibcite{koehn2005europarl}{{Koehn}{}{{}}{{}}}
\bibcite{kolesnikov2019large}{{Kolesnikov et~al.}{}{{}}{{}}}
\bibcite{kottur2016visual}{{Kottur et~al.}{}{{}}{{}}}
\bibcite{krishnavisualgenome}{{Krishna et~al.}{}{{}}{{}}}
\bibcite{krizhevsky2012imagenet}{{Krizhevsky et~al.}{}{{}}{{}}}
\bibcite{kudo-richardson-2018-sentencepiece}{{Kudo and Richardson}{}{{}}{{}}}
\bibcite{lazaridou2015combining}{{Lazaridou et~al.}{}{{}}{{}}}
\bibcite{lewis-etal-2020-bart}{{Lewis et~al.}{}{{}}{{}}}
\bibcite{Lewkowycz2022}{{Lewkowycz et~al.}{}{{}}{{}}}
\bibcite{lin-2004-rouge}{{Lin}{}{{}}{{}}}
\bibcite{COCO}{{108}{a}{{Lin et~al.}}{{}}}
\bibcite{lin2014microsoft}{{109}{b}{{Lin et~al.}}{{}}}
\bibcite{mccoco}{{110}{c}{{Lin et~al.}}{{}}}
\bibcite{liu2019roberta}{{Liu et~al.}{}{{}}{{}}}
\bibcite{lottick2019energy}{{Lottick et~al.}{}{{}}{{}}}
\bibcite{VilBert}{{113}{a}{{Lu et~al.}}{{}}}
\bibcite{lu2019vilbert}{{114}{b}{{Lu et~al.}}{{}}}
\bibcite{lu2022imagination}{{115}{c}{{Lu et~al.}}{{}}}
\bibcite{mahajan2018exploring}{{Mahajan et~al.}{}{{}}{{}}}
\bibcite{mayer2014creating}{{Mayer and Cysouw}{}{{}}{{}}}
\bibcite{3D}{{Mccormack and Gambardella}{}{{}}{{}}}
\bibcite{redditUsers}{{MICHAEL~BARTHEL and MITCHELL}{}{{}}{{}}}
\bibcite{mikolov2013efficient}{{Mikolov et~al.}{}{{}}{{}}}
\bibcite{unsupBrain}{{Mineault}{}{{}}{{}}}
\bibcite{coco_eval}{{Mircosoft}{}{{}}{{}}}
\bibcite{mordvintsev_2015}{{Mordvintsev}{}{{}}{{}}}
\bibcite{Mustafa2022}{{Mustafa et~al.}{}{{}}{{}}}
\bibcite{GLIDE}{{Nichol et~al.}{}{{}}{{}}}
\bibcite{DALLEpytorch}{{OpenAI}{}{{}}{{}}}
\bibcite{papineni-etal-2002-bleu}{{Papineni et~al.}{}{{}}{{}}}
\bibcite{parcalabescu-etal-2022-valse}{{Parcalabescu et~al.}{}{{}}{{}}}
\bibcite{StyleGAN}{{Patashnik et~al.}{}{{}}{{}}}
\bibcite{pennington2014glove}{{Pennington et~al.}{}{{}}{{}}}
\bibcite{perez2021true}{{Perez et~al.}{}{{}}{{}}}
\bibcite{pezzelle2021word}{{Pezzelle et~al.}{}{{}}{{}}}
\bibcite{prabhu2020large}{{Prabhu and Birhane}{}{{}}{{}}}
\bibcite{qiao2022initial}{{Qiao et~al.}{}{{}}{{}}}
\bibcite{rlang}{{R Core Team}{}{{}}{{}}}
\bibcite{radford2021learning}{{136}{a}{{Radford et~al.}}{{}}}
\bibcite{CLIP}{{137}{b}{{Radford et~al.}}{{}}}
\bibcite{Radford2019LanguageMA}{{138}{c}{{Radford et~al.}}{{}}}
\bibcite{radford2019language}{{139}{d}{{Radford et~al.}}{{}}}
\bibcite{rajpurkar2018know}{{140}{a}{{Rajpurkar et~al.}}{{}}}
\bibcite{rajpurkar2016squad}{{141}{b}{{Rajpurkar et~al.}}{{}}}
\bibcite{DALLE2}{{142}{a}{{Ramesh et~al.}}{{}}}
\bibcite{DALLE}{{143}{b}{{Ramesh et~al.}}{{}}}
\bibcite{pmlr-v139-ramesh21a}{{144}{c}{{Ramesh et~al.}}{{}}}
\bibcite{Rebuffi2017}{{Rebuffi et~al.}{}{{}}{{}}}
\bibcite{recht2019imagenet}{{Recht et~al.}{}{{}}{{}}}
\bibcite{Reed2022}{{Reed et~al.}{}{{}}{{}}}
\bibcite{ren2015faster}{{Ren et~al.}{}{{}}{{}}}
\bibcite{8099614}{{Rennie et~al.}{}{{}}{{}}}
\bibcite{ribeiro2020beyond}{{Ribeiro et~al.}{}{{}}{{}}}
\bibcite{Riquelme2021}{{Riquelme et~al.}{}{{}}{{}}}
\bibcite{ImageNet}{{Russakovsky et~al.}{}{{}}{{}}}
\bibcite{schuhmann2022laion}{{Schuhmann}{}{{}}{{}}}
\bibcite{LAION}{{Schuhmann et~al.}{}{{}}{{}}}
\bibcite{sennrich2015neural}{{155}{a}{{Sennrich et~al.}}{{}}}
\bibcite{sennrich-etal-2016-neural}{{156}{b}{{Sennrich et~al.}}{{}}}
\bibcite{shao2019objects365}{{Shao et~al.}{}{{}}{{}}}
\bibcite{Shaazer2017}{{Shazeer et~al.}{}{{}}{{}}}
\bibcite{shekhar2017foil}{{Shekhar et~al.}{}{{}}{{}}}
\bibcite{shen2021much}{{Shen et~al.}{}{{}}{{}}}
\bibcite{sheng2019woman}{{Sheng et~al.}{}{{}}{{}}}
\bibcite{ruDALLE}{{Shonenkov}{}{{}}{{}}}
\bibcite{sikarwar2022efficacy}{{Sikarwar and Kreiman}{}{{}}{{}}}
\bibcite{silberer2012grounded}{{164}{a}{{Silberer and Lapata}}{{}}}
\bibcite{silberer2014learning}{{165}{b}{{Silberer and Lapata}}{{}}}
\bibcite{singh2022flava}{{Singh et~al.}{}{{}}{{}}}
\bibcite{Socher10connectingmodalities}{{Socher and Fei-fei}{}{{}}{{}}}
\bibcite{8477754}{{Soderlund and Blair}{}{{}}{{}}}
\bibcite{srinivasan2021wit}{{Srinivasan et~al.}{}{{}}{{}}}
\bibcite{bias_ML}{{Srinivasan and Uchino}{}{{}}{{}}}
\bibcite{srivastava2022beyond}{{Srivastava et~al.}{}{{}}{{}}}
\bibcite{Steiner2021}{{Steiner et~al.}{}{{}}{{}}}
\bibcite{strubell2019energy}{{173}{a}{{Strubell et~al.}}{{}}}
\bibcite{environment}{{174}{b}{{Strubell et~al.}}{{}}}
\bibcite{sun2021multimodal}{{Sun et~al.}{}{{}}{{}}}
\bibcite{sutton2019bitterlesson}{{Sutton}{}{{}}{{}}}
\bibcite{tan2020vokenization}{{Tan and Bansal}{}{{}}{{}}}
\bibcite{EfficientNet}{{Tan and Le}{}{{}}{{}}}
\bibcite{uppal2022multimodal}{{Uppal et~al.}{}{{}}{{}}}
\bibcite{attention}{{180}{a}{{Vaswani et~al.}}{{}}}
\bibcite{vaswani2017attention}{{181}{b}{{Vaswani et~al.}}{{}}}
\bibcite{NIPS2017_3f5ee243}{{182}{c}{{Vaswani et~al.}}{{}}}
\bibcite{cider}{{Vedantam et~al.}{}{{}}{{}}}
\bibcite{vinyals}{{Vinyals et~al.}{}{{}}{{}}}
\bibcite{wang2018glue}{{185}{a}{{Wang et~al.}}{{}}}
\bibcite{Wang2022}{{186}{b}{{Wang et~al.}}{{}}}
\bibcite{NFT}{{187}{c}{{Wang et~al.}}{{}}}
\bibcite{wei2022masked}{{Wei et~al.}{}{{}}{{}}}
\bibcite{wenzek2019ccnet}{{Wenzek et~al.}{}{{}}{{}}}
\bibcite{WZRD}{{WZRD}{}{{}}{{}}}
\bibcite{sun}{{Xiao et~al.}{}{{}}{{}}}
\bibcite{xu1}{{Xu et~al.}{}{{}}{{}}}
\bibcite{xue2020mt5}{{Xue et~al.}{}{{}}{{}}}
\bibcite{Yang_2019_CVPR}{{Yang et~al.}{}{{}}{{}}}
\bibcite{darkMatter}{{Yann and Ishan}{}{{}}{{}}}
\bibcite{5487377}{{196}{a}{{Yao et~al.}}{{}}}
\bibcite{yao1}{{197}{b}{{Yao et~al.}}{{}}}
\bibcite{GCN-LSTM}{{198}{c}{{Yao et~al.}}{{}}}
\bibcite{parti}{{Yu et~al.}{}{{}}{{}}}
\bibcite{yuan2021florence}{{200}{a}{{Yuan et~al.}}{{}}}
\bibcite{yuan2022wudaomm}{{201}{b}{{Yuan et~al.}}{{}}}
\bibcite{zellers2019recognition}{{202}{a}{{Zellers et~al.}}{{}}}
\bibcite{zellers2018swag}{{203}{b}{{Zellers et~al.}}{{}}}
\bibcite{zeng2022socratic}{{Zeng et~al.}{}{{}}{{}}}
\bibcite{zhang2016yin}{{205}{a}{{Zhang et~al.}}{{}}}
\bibcite{zhang2020contrastive}{{206}{b}{{Zhang et~al.}}{{}}}
\bibcite{zhuang2021unsupervised}{{Zhuang et~al.}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\gdef \@abspage@last{214}
