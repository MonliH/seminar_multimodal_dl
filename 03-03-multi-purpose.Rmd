## Multi-purpose Models

*Author: Philipp Koch*

*Supervisor: Rasmus Hvingelby*

### Intro

After we describe further modalities in the previous sections, we will look at truly multipurpose models.
Multitask multimodal models have already been proposed, like UniT, which extends the transformer architecture
to deal with different modalities and tasks. However, previous multitask multimodal models remain limited in
different aspects, which we will describe and discuss further. To become genuinely multipurpose, however, a model
must be able to solve different tasks without fine-tuning and must be capable of dealing with different modalities.
Thus, it must be able to transfer knowledge in-between tasks but must also be able to allocate capabilities for
different modalities.
The recently introduced deep learning architecture Pathways is designed to be multipurpose. Pathways builds on
newly designed hardware and software dedicated to addressing the challenges of contemporary deep learning models,
which are ever-growing, where GPT-3 might be the most prominent example. We will discuss previous drawbacks and
describe how Pathways aims to solve these issues. Besides the hardware aspect, Pathways provides a large neural
network constructed as a directed acyclic graph (DAG). The input is passed through the network on different paths.
Each node of the network is itself a neural network aimed at solving a specific aspect of a task. Using these
different neural networks inside the model allows the model to be multitask and transfer knowledge in-between tasks.
Another important aspect of this architecture is the obtained sparsity. When computed, just necessary nodes are computed,
resulting in higher overall performance.
Furthermore, the model is intended to absorb different modalities as input, where no implementation has been found.
Multimodality is further used hypothetically in the initial blog post. However, the similar model PathNet also achieves
multimodality. The only model based on Pathways is the language model (PaLM), which is multilingual and capable of
understanding code and solving mathematical tasks. However, the multimodality here remains questionable. Future
Pathways-based models might provide more insight if the claim to step further toward artificial general intelligence (AGI)
of the authors of Pathways and PathNet is true or not. Eventually, we will discuss the impact of the new Pathways
multipurpose model since it might have a large impact on deep learning in the upcoming future. Broader applicable
models will become feasible yet also centralize the usage, thus reducing accessibility and subsequently research on these models.


### Introduction
In recent years and months, more and more focus has shifted to transformers being used in a multimodal setting e.g. [@Lu2020].
However, with the introduction of ViT [@dosovitskiy2020image], it became clear that these models are not just appropriate for NLP.
Recent developments have proven transformers to be general models as long as input can be tokenized and presented to them. Although
transformers have been successful in a multimodal and multitask learning setting, other models were also around too, and transformers
might be further enhanced by using so-called Mixture-of-Expert layers as done in [@Fedus2021] and recently [@Mustafa2022].
In this chapter, multi-purpose models will be surveyed. At first, the term multi-purpose will be clarified since there is, to our best
knowledge, no standard definition for this term. Different approaches from recent work will be presented, and eventually, the chapter
will diverge to future developments, as outlined in [@Dean2021]. Jeff Dean proposed a promising architecture for future multi-purpose models,
which will be further examined on how this proposal has already been implemented. The chapter will conclude with an outlook and a discussion
on how the field will likely evolve in the following years.



<!-- ### TODO
 - Broader survey on previous work (also VisualBERT, VilBERT etc.)
 - Adjacent models like FLAVA -->

#### Multipurpose Models
Since the early years of machine learning, multitask and multimodal learning paradigms have been around. Multitask learning [@Crawshaw2020]
is the paradigm of training a model on different tasks with the intention that the model transfers the learned knowledge to new tasks,
such that fewer resources are required to learn new tasks. Akin to humans, it is intended that the model benefits from previously learned tasks.
Humans do not learn every task from scratch. However, machine learning models do. It is assumed that related tasks let the model further
generalize. Although there exists field-specific issues like catastrophic forgetting and negative transfer, this approach is also promising
for future implementations.
Multimodal learning [@Baltrusaitis2019] is a paradigm in which a machine learning model is supplied with multiple modalities like images, text,
tables, etc. As in multitask learning, this approach is also inspired by human intelligence since humans perceive the world through multiple
senses. It is thus assumed that multimodal models achieve better performance due to the higher input quality of the provided data. However,
this field also has some specific problems, mainly focusing on how the different representations can be aligned when fused.
We want to marry the two paradigms to form a so-called multi-purpose model for this work. These kinds of models are both multimodal and multitask.
We assume that this merge even further improves the quality of the predictions. Due to the novelty of this fusion (only a few models have been
proposed, as we will see soon), there is no knowledge of any specific problems in this setup. Directions and possible implications will be discussed
at the end of the chapter.

### Previous Work

#### MultiModel
The first prominent multi-purpose model is the so-called MultiModel [@Kaiser2017]. This model, from the pre-transformer time, combines multiple
architectural approaches from different fields to tackle both multimodality and multiple tasks. The model itself is itself inspired by the
encoder-decoder architecture, popular in NLP at that time, making it an autoregressive model.
The model consists of four important modules, which are the so-called modality nets, the encoder, the  I/O Mixer, and the decoder.
The modality nets are used to form a representation on which the other modules can work such that it can be fed into the encoder or the I/O encoder
(which is necessary because of the autoregressive structure) but also construct the output since they are also used to decode from the internal
representation to the specific modality. For the language task, the modality net tokenizes the input sequences and is then transformed into the
internal representation using learned embeddings?. For the output, the representation is fed into a simple feed-forward network, which is then
fed into a softmax function. The modality net for images is multiple stacked convolution operations as done in Xception (TODO). Furthermore, there
are also nets for audio and categorical modalities.
Inside the model, where the unified representations are used, there is the encoder, which consists of multiple convolution operations and a mixture-of-expert
layer block. The output of the encoder is further passed on to the I/O mixer and the decoder which are now used to produce the output in an autoregressive way.
The decoder produces the output, and the I/O mixer reads the previous output and combines it with the output of the encoder using attention and convolutional
operations. Since this architecture is from the pre-transformer era, the attention mechanism used here is cross-attention.
The decoder eventually processes the output of the encoder and the I/O mixer, thus the input sequence and also the generated sequence, to produce proper output,
which is done using attention and convolutional operations.

#### Unified Transformer (UniT)
The Unified Transformer (UniT) [@Hu2021] is a thoroughly used transformer network with multiple encoders for each modality. Only a visual and a text encoder
have been used in the initial setting. However, the authors state that an arbitrary amount of encoders can be used. For the textual input, a BERT model [@Devlin2018]
has been used, while for the visual encoder, a DETR [@Carion2020] has been used. In this approach, the images are first pre-encoded using a ResNet [@ResNet].
After the input image is encoded and linearly projected to the hidden dimension of the transformer, another task-specific vector is added to the data and fed
into the here used visual transformer, which follows DETR. The authors chose BERT [@Devlin2018] as an encoder for the textual representation. To encode the
text, words of sentences are tokenized, and BERT-specific tokens, like the [CLS] token, are added. As in the visual encoder, a task-specific vector is added
to the input, which is later removed from the output sequence. After the data from all modalities is encoded, it is concatenated and passed to the decoder, a
vanilla transformer decoder according to [@vaswani2017attention] and the one used in DETR. To the embedded sequence, a task-specific query representation sequence is
also passed. Initially, the authors used both task-specific and task-agnostic decoders for their experiments.

After some developments in the field of multimodal transformers took place (VilBERT, ViT, etc.), the Unified Transformer (UniT) [@Hu2021] was introduced.
Compared to previous approaches in the multi-purpose models, UniT aims to simplify the architecture. Achieving the capability of multitasking resulted in
many hyperparameters and specific submodels to be set by hand for each task and or modality. UniT tried to achieve independence of this caveat, despite also
using some task-specific submodules. The approach is as follows; transformers encode the input sequence on each domain, and the input encodings are concatenated
and then passed on to a transformer decoder which is connected to task-specific output heads. Even though these heads were to be set manually, the model proved
the fit of transformers for multi-purpose models. On top of the decoder, task-specific heads are used to transform the obtained sequence into a solution to the
given tasks. These heads are trainable networks, which are to be switched if specific tasks are used.
The model often showed better results than a single-task specifically learned model. The model outperformed the single-task trained model for visual question answering
(vqa), COCO [@mccoco], and visual genome detection [@Krishna2017]. On further tasks, when the model was trained for up to 8 tasks, it showed still comparable performance,
however most of the time lower, than domain-specific models like BERT, VisualBERT [@Li2019], and DETR.

#### OFA
Another transformer-based model is OFA [@Wang2022]. The multi-purpose approach is implemented such that every input is tokenized into a unified vocabulary, which becomes
possible since also images can be turned into tokens. Also, output sequences can be turned into their original or intended modality again, such that something like image
generation from the text also becomes possible.

#### Gato
To combine different modalities and create a model which is also capable of solving different tasks, the generalist agent “gato” was introduced [@Reed2022]. To additionally improve
the model, reinforcement learning was also included to allow the model to become sequential and interact with it's environment. The model is not just able of solving text
and visual tasks, but also to solve classic reinforcement learning tasks like playing ATARI games and proprioception. The results of the model are state-of-the-art.
Reinforcement Learning is another approach in machine learning, where there is supervised learning, unsupervised learning and reinforcement learning. This technique is
used to model sequential decision propblems namely, modeling the decision of a model depending on a specific state. The classic RL setup consists of an environment and an
agent. The agent is led by a policy function and is informed about it's decision using reward from the environment. The policy is then updated to optimize for the as best
as possible policy by maximizing the reward of the agent. RL proved to be beneficial in many different setups and led in 2016 with DeepMind's AlphaGo to a breakthrough by
beating the grandmaster of go, which is considered a very complex game.
The internally used policy is a transformer, which is capable of dealing with discrete input (tokens) and also continuous properties of it's environment. Text, visual
properties, buttons and movements are tokenized such that these entities can be embedded as it is commonly used in natural language processing. The model itself is a 
ecoder of a transformer and is trained autoregressively, such that based on the previous sequence, the next token is predicted. This approach is akin to the GPT family.
To predict multiple modalities based on previous inputs, the model needs to work with embeddings which itself are based on tokens. Although the model is is multi-purpose,
different modalities are first processed using specific models at their entry points. Natural language data is encoded using SentencePiece, images are tokenized using the
same procedure as in ViT and subsequently encoded using a ResNet v2 [@He2016b], real world entities like buttons for games are also encoded to tokens allowing a transformer
to become a multimodal model. Different techniques are applied to further represent these embeddings as sequences since transformer models are specifically designed for
translation tasks and thus for sequence modeling. Text tokens remain in their intended order, while image tokens are represented as a raster to represent specific entities
in a correct spatial representation. Tensors are also represented in a way such that rows are an important feature for sequencing. Nested structures are also ordered using
keys to represent nesting. Specific observations for reinforcement learning are also sequenced such that actions and observations are also tokenized and sequenced.

<!-- ### TODO
 - Results on tasks
 - Describe architecture in detail -->

### Pathway Proposal

Multipurpose models can also be thought of without the currently well-performing transformer architecture. A proposal for a new approach to multipurpose models was introduced as Google's
Pathways [@Dean2021] aiming to address current pitfalls in deep learning.
Current deep learning consists mainly of large dense neural networks trained for a specific task. This paradigm is limiting so that no transfer between tasks is possible, and every
time a model is trained, it needs to learn from scratch (or needs to be pre-trained from scratch). Human intelligence can also transfer knowledge between tasks, so it is not necessary
to learn everything from scratch. However, transfer in deep learning models is not sufficiently possible, which is a severe constraint. The problem of catastrophic forgetting and negative transfer may arise.

#### Idea

Pathways follow a different idea to overcome the problems mentioned above. The model consists of a large graph, through which input can be mapped to an output. The nodes of the network
are neural networks themselves. A pass through this network does not include calling all nodes and thus all neural networks, but only a few. The pass follows a specific path from the input to the output.
The underlying idea behind this is similar to the mixture-of-expert models described above. Only the specific networks dedicated to solving this particular problem are to be called.
However, pathways consist solely of layers of expert networks.
At this point, it is necessary to recall that multi-task learning aims to generalize better on new tasks since the knowledge about previously learned tasks can be applied.
This idea is the foundation of pathways too, where specialist networks (nodes) are combined in a larger network. It is assumed that the model's generalization capabilities
increase severely by finding an appropriate path for a task to the appropriate specialist nodes. In this setup, the particular problem-solving capabilities are combined.
Furthermore, multimodality is also considered a potential extension. Adding more modalities might not be a difficult problem considering the architecture of the previously
introduced transformer-based models.
Overall the approach of a sparse model combining multiple experts offers many opportunities to combine modalities and reuse task-specific capabilities. The sparsity of
the model offers increased inference time since only some parts of the networks are activated during inference.

Another part of the pathways project includes the improvement of current hardware limitations. It is already observable that Moore's Law (each x years, the compute capacity doubles)
slows severely down while deep learning research has grown exponentially in the late 2010 years. Thus, also hardware needs to be adapted to the
growing demand in deep learning. In the context of the pathway proposal, a novel framework for Google datacenters has been introduced, aiming to reduce overhead during computation
and access specific parts of the model to utilize the technical advantages of sparse networks.
As in dense models, it is not necessary to use the whole network but only chunks of it, which is in contrast to previous dense models, which require the whole model to be accessed.
So far, two large pre-trained models have been introduced based on the new training framework. One is the Pathways Language Model (PaLM) [Chowdhery2022], which is by now the largest
language model using 540 billion parameters, Minerva [@Lewkowycz2022], which is based on PaLM, and Parti [@parti], which is a text-to-image generator.


<!-- ### TODO
 - More details on how hardware plays in the grand Pw proposal -->
 <!-- Downsides
  -->
#### Related Work to Pathways
In this section, we will look at similar architecture to the proposed pathway model, that will provide a further overview on how capabilities in this field have developed.

#### PathNet
An architecture for neural networks [@Fernando2017]. Also achieved multitask solving by introducing a novel training algorithm. In contrast to single-task neural networks,
PathNet does not train the network solely on one task but on the whole network, but partially on one task and on a fraction of the network, randomly chosen. With this approach,
knowledge sharing becomes possible.
The pathway consists of a graph of networks where the networks are organized in columns of hidden layers in the network. Each node in this graph is a neural network itself.
This design intends to only train a path throughout the network on a specific task and subsequently train the network on an alternative path on another task. Thereby allowing
the network to transfer already trained capability to the new task.
The model achieves this transfer by selecting the best path using an evolutional approach. At first, an initial population of paths is initialized and evaluated against each
other in a binary way after trained T/some epochs. The better-performing algorithm will then be modified to further compete against other pathways. After the winner is found,
the winner's path is frozen, meaning that all weights are not updated anymore to keep the performance on the trained task and avoid catastrophic forgetting. After fixing the
winning path, all other parameters are newly initialized.
Now, the same procedure is applied again to another task that is intended to benefit from the knowledge of the previous task. During this procedure, the tournament of different
paths starts again, where the paths are trained and evaluated without the nodes from the previous winner path and eventually fixed again.

#### LIMoE
In the context of Pathways, the work LIMoE (Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts) [@Mustafa2022] is also necessary to be mentioned.
LIMoE combines text and vision input using a transformer encoder, which includes an MoE layer that can be used sparsely during inference time. Besides the context of the pathways
proposal, LIMoE also aims to provide a new approach to contrastive learning. While previous methods used two models (two-tower) to encode the modalities, LIMoE is solely based
on one model, where the modalities are processed in a single transformer-like model (one-tower).
Thus, the model described in LIMoE is based on a modified transformer. The text data is encoded using one-hot-SentencePiece encoding, while images are tokenized in the same way as
done in ViT [@dosovitskiy2020image] to provide the input appropriately. The main difference to the standard transformer is an MoE layer at the spot where the feed-forward network
usually lies. In this layer, $E$ experts are used, which are themselves feed-forward-networks. For each token, $K$ appropriate experts will map the tokens further downstream.
The routing is computed by a gating net network, which uses top-k routing. Another feature here is a fixed-length buffer for each expert in the MoE layer. This buffer is used to
store tokens before an expert network processes them, assuming that the allocations of tokens for each expert are balanced. If it is impossible to buffer tokens for the experts,
the tokens will be dropped. To furthermore decide which tokens are to be dropped, Batch Priority Routing [@Riquelme2021] is used to provide a ranking mechanism on how which token
is to be dropped. The output of the transformer encoder is then average pooled and subsequently multiplied with a modality-specific weight matrix, which produces the eventual output
for the token of both modalities $z_{image}$ and $z_{text}$.
The model is trained using a contrastive objective. The contrastive loss, in this case, aims to maximize the paired visual and textual input while minimizing all combinations of
unpaired embeddings. This objective can be achieved by using the dot-product as a similarity measure between the two embeddings of both modalities, which provide a differentiable
operation through which the overall loss can be minimized.
Additionally, to contrastive loss, the pitfalls of a multimodal MoE are also considered. One challenge in MoE is the correct balancing of routing to experts, which is even more
challenging when using unbalanced multimodal data. To address this issue, two new losses based on entropy are introduced. Entropy can be used as an appropriate term since it provides
a valuable number for the uniformity of the distribution, which is necessary to balance the expert assignments. The losses are aimed at controlling the allocation of experts to tokens,
which is also necessary to fulfill the assumptions for the implemented buffer.
One loss considers the token level (local) routing, and the other considers the overall expert routing distribution (global). The local loss aims to achieve no uniform behavior in expert
allocation such that each token is indeed assigned to specific experts. In contrast, the overall global loss aims to achieve uniformity over all tokens to avoid a collapse in which tokens
are solely assigned to a few experts which do not have the capacity to deal with all tokens. These losses are computed for each modality. Furthermore, already available losses for training
MoE models were also added to avoid known downsides of MoE models.
LIMoE was compared against similar models, which show the pitfalls of being solely dense and specialized on a task, which is also the case for two-tower setups like CLIP [@radford2021learning].
The test dataset was ImageNet [@deng2009imagenet] and MS-COCO [@mccoco]. Overall, LIMoE-H/14 (largest model, 12 MoE-layers, 32 experts per layer) achieved strong performance considering that only
one model was used for two modalities against specialist models in two-tower setups. It was also possible to outperform CLIP and ALIGN by a significant margin while using not too many
additional parameters. Models that achieved similar results to LIMoE used at least twice the number of parameters for a forward pass. 
So far, LIMoE is a new approach to contrastive learning. It showed that the sparse approach could significantly lead to strong performance while using fewer parameters for inference.
Furthermore, it also includes essential aspects of the pathways proposal using the MoE layer where sparsity comes into play. The authors stated that the critical aspect is how the model
routes to the specific expert, which might also be helpful in an additional multitask setup, which is an important step towards a pathway model. However, the authors also stated that merging
modalities remains a field of research, as well as adding further modalities, since Vision and text have already proven to be non-trivial to combine, and more problems might arise with further modalities.

#### muNet
While LIMoE offered a new appraoch to multimodality, another model, named muNet [@Gesmundo2022], aims at solving multi-tasking. Asit was already described in the initial pathways proposal, many aspects of current
machine learning lack efficiency. This is especially a problem when it comes to training models from scratch everytime and not reusing the already acquired knowledge. Even transfer learning as used in BERT [@devlin]
only trains on a specific new set again, which is based on a copy of the original model, thus modifiying the original model and furthermore not reusing the acquired knowledge in fine-tuning, so cthat the iniital 
eneralizability is lost in the process. muNet aims to alleviate this problem by introducing an evolutionary approach to build an appropriate model, which is extensible, keeps prior knowledge while being unaffected
of problems in multi-task learning, and furthermore alleviate the necessity of hyperparametertuning.
This approach is based on an evolutionary algorithm, which has been described earlier (X).
- architecture, algorithm
muNet is based on an evolutionary algorithm. The evolution here is about how the onverall configuration of the network is changed. Starting from an initial model, varieties of different configurations are mutated
based on the initial model and then evaluated regarding their performance. While doing so, the parameters of the parent generation is fixed such that the acquired knowledge about previous tasks is not overwritten.
This technique allows to avoid catastrophic forgetting, which is a common issue in multit-task learning.
- Mutations
Evolutionary algorithms require methods to mutate the initial parent population such that new modificiations (children) are obtained, from which a winning candidate is then chosen for the next generation.

- evolutianary algo

- Aspect about multitask learning. 

- Results
- Conclusion


#### Conclusion Pathways
- muNet and LIMoE show sparsity works (less param, good performance)
- Evolutionary with large models


### Discussion

Considering the pace of the field and that results, as seen in Gato, flamingo and parti would not have been anticipated a few years ago, it is highly likely to see further
breakthroughs in the field of multi-purpose models, which is especially the case since hardware-related issues are now being addressed and further developed. Based on these
developments, more general models will be published in the near future. Models with even higher capabilities will also let new questions and problems arise. At this point,
there is already the first issue arising, which is the trend of proprietary models. When GPT-3 was published, OpenAI closed access to the model leaving only a few with API keys
to access the model.
In contrast to the introduction of BERT, which was open source, there did not follow a trend of GPTology as it was done with BERTology, leaving the model underresearched compared
to its open source and significantly smaller peers. On the other side, there also exists a trend of open sourcing some models, as seen with GPT-J and GPT-Neo or recently with the
introduction of OPT by meta. However, even though this trend exists, there is the issue of the increasing size of the models, which makes it almost impossible to train or even
fine-tune these models without using massive amounts of computational resources.
Another issue that comes with the higher capability of these models is the societal impact of these models. Pop culture has severely impacted the perception of artificial
intelligence, which might also become a topic in the future. It has already been a public issue that a google employee claimed that LamDA is sentient. With further high-quality
models, there might be more discussions on how to deal with AI in the future. Since this already showed that the Turing test might not be an appropriate metric anymore, it might
also be helpful to research new metrics for AI and possibly AGI.
In his TED-talk, Dean saw Pathway as a promising path toward AGI, which is a bold statement. However, considering the quality of already existing models, it might be a significant
step in increasing the quality of generative models for solving multiple general tasks. Nevertheless, new problems will arise which are likely not solved with this model like
continuous learning. The examined models and pathways are only trained once and then remain frozen in their knowledge. This issue is already visible in older language models like
BERT and GPT, where Donald Trump is still president and COVID-19 does not exist.

<!-- ### TODO
 - Proposal for unifying standardizing evaluation, common benchmarks etc.
 - Outlook -->

