## Multi-purpose Models

*Author: Philipp Koch*

*Supervisor: Rasmus Hvingelby*

### Intro

After we describe further modalities in the previous sections, we will look at truly multipurpose models.
Multitask multimodal models have already been proposed, like UniT, which extends the transformer architecture
to deal with different modalities and tasks. However, previous multitask multimodal models remain limited in
different aspects, which we will describe and discuss further. To become genuinely multipurpose, however, a model
must be able to solve different tasks without fine-tuning and must be capable of dealing with different modalities.
Thus, it must be able to transfer knowledge in-between tasks but must also be able to allocate capabilities for
different modalities.
The recently introduced deep learning architecture Pathways is designed to be multipurpose. Pathways builds on
newly designed hardware and software dedicated to addressing the challenges of contemporary deep learning models,
which are ever-growing, where GPT-3 might be the most prominent example. We will discuss previous drawbacks and
describe how Pathways aims to solve these issues. Besides the hardware aspect, Pathways provides a large neural
network constructed as a directed acyclic graph (DAG). The input is passed through the network on different paths.
Each node of the network is itself a neural network aimed at solving a specific aspect of a task. Using these
different neural networks inside the model allows the model to be multitask and transfer knowledge in-between tasks.
Another important aspect of this architecture is the obtained sparsity. When computed, just necessary nodes are computed,
resulting in higher overall performance.
Furthermore, the model is intended to absorb different modalities as input, where no implementation has been found.
Multimodality is further used hypothetically in the initial blog post. However, the similar model PathNet also achieves
multimodality. The only model based on Pathways is the language model (PaLM), which is multilingual and capable of
understanding code and solving mathematical tasks. However, the multimodality here remains questionable. Future
Pathways-based models might provide more insight if the claim to step further toward artificial general intelligence (AGI)
of the authors of Pathways and PathNet is true or not. Eventually, we will discuss the impact of the new Pathways
multipurpose model since it might have a large impact on deep learning in the upcoming future. Broader applicable
models will become feasible yet also centralize the usage, thus reducing accessibility and subsequently research on these models.


### Introduction
In recent years and months, more and more focus has shifted to transformers being used in a multimodal setting e.g. [@Lu2020].
However, with the introduction of ViT [@dosovitskiy2020image], it became clear that these models are not just appropriate for NLP.
Recent developments have proven transformers to be general models as long as input can be tokenized and presented to them. Although
transformers have been successful in a multimodal and multitask learning setting, other models were also around too, and transformers
might be further enhanced by using so-called Mixture-of-Expert layers as done in [@Fedus2021] and recently [@Mustafa2022].
In this chapter, multi-purpose models will be surveyed. At first, the term multi-purpose will be clarified since there is, to our best
knowledge, no standard definition for this term. Different approaches from recent work will be presented, and eventually, the chapter
will diverge to future developments, as outlined in [@Dean21]. Jeff Dean proposed a promising architecture for future multi-purpose models,
which will be further examined on how this proposal has already been implemented. The chapter will conclude with an outlook and a discussion
on how the field will likely evolve in the following years.

<!-- ### TODO
 - Broader survey on previous work (also VisualBERT, VilBERT etc.)
 - Adjacent models like FLAVA -->

#### Multipurpose Models
Since the early years of machine learning, multitask and multimodal learning paradigms have been around. Multitask learning [@Crawshaw2020]
is the paradigm of training a model on different tasks with the intention that the model transfers the learned knowledge to new tasks,
such that fewer resources are required to learn new tasks. Akin to humans, it is intended that the model benefits from previously learned tasks.
Humans do not learn every task from scratch. However, machine learning models do. It is assumed that related tasks let the model further
generalize, similat to human intelligence. Although there exists field-specific issues like catastrophic forgetting and negative transfer, this approach is also promising
for future implementations.
Multimodal learning [@Baltrusaitis2019] is a paradigm in which a machine learning model is supplied with multiple modalities like images, text,
tables, etc. As in multitask learning, this approach is also inspired by natural intelligence since intelligent beings perceive the world through multiple
senses. It is thus assumed that multimodal models achieve better performance due to the higher input quality of the provided data. However,
this field also has some specific problems, mainly focusing on how the different representations can be aligned when fused.
We want to marry the two paradigms to form a so-called multi-purpose model for this work. These kinds of models are both multimodal and multitask.
We assume that this merge even further improves the quality of the predictions. Due to the novelty of this fusion (only a few models have been
proposed, as we will see soon), there is no knowledge of any specific problems in this setup. Directions and possible implications will be discussed
at the end of the chapter.

### Previous Work

#### MultiModel
A first prominent multi-purpose model is the so-called MultiModel [@Kaiser2017]. This model, from the pre-transformer time, combines multiple
architectural approaches from different fields to tackle both multimodality and multiple tasks.
The model consists of four essential modules: the so-called modality nets, the encoder, the  I/O Mixer, and the decoder.
The modality nets function as translators between the data and a suitable representation for the inner modules. They also follow the purpose of
back translating to create output. For language tasks, the modality net is a tokenizer that 
utputs the appropriate embeddings, while for vision tasks, convolution operations transform the images into the proper representation. Furthermore,
there are also nets for audio and categorical modalities.
For the language task, the modality net tokenizes the input sequences and is then transformed into the internal representation using learned embeddings?
For the output, the representation is fed into a simple feed-forward network, which is then fed into a softmax function. The modality net for images is
multiple stacked convolution operations as done in Xception (TODO).
For the architecture and the chapter generally, it is necessary to recall the concept of Mixture of Experts (MoE) (Shaazer, jacobs  Jacobs), which is an
ensemble of different neural networks inside the layer. The neural networks are not used for every forward pass but only if the data is well suited to
be dealt with by a specific expert. MoEs allow being more computationally efficient while still keeping or even improving performance. Training MoEs
usually requires balancing the experts so that the routing does not collapse into one or a few experts. An additional gating network decides which of
the experts is called. Gating can be implemented so that only $k$ experts are used, which reduces the computational costs for inference. 
Inside the model, where the unified representations are used, there is the encoder, which consists of multiple convolution operations and a mixture-of-expert
layer block. The output of the encoder is further passed on to the I/O mixer and the decoder. The decoder produces the next token, while the I/O mixer reads the
previous tokens and provides further context to the decoder. Both modules produce the output in an autoregressive manner. The I/O mixer combines the produced
output with the input from the encoder using cross attention.
The decoder produces the output, and the I/O mixer reads the previous output and combines it with the output of the encoder using attention and convolutional
operations. Since this architecture is from the pre-transformer era, the attention mechanism used here is cross-attention.
The decoder eventually processes the output of the encoder and the I/O mixer, thus the input sequence and the generated sequence, to produce proper output,
which is done using attention and convolutional operations. A positional encoding conceptually similar to the one in transformers was also used for the attention mechanism.
Multimodel was trained on eight datasets, from which six were from the language domain and COCO (X) as well as ImageNet (X) additionally. For training, four
experts in the MoE layers were used. Compared to training Multimodel for each task specifically, the combined training showed comparable or better performance. 
he combined trained multimodel on ImageNet (X) and machine translation were below sota models.
<!-- No output for IMages  -->

#### Unified Transformer (UniT)
A more recent multi-purpose model is UniT (Unified Transformer) [@Hu2021]. UniT is built upon the transformer architecture, in which both encoder and decoder are used.
To account for multimodality and multitasking, the transformer is enhanced.
The encoder part of UniT consists of two encoders since the initial setup is aimed at the modalities of text and vision. However, more modality-specific encoders may
be added. For the case of language, a BERT model [@Devlin2018] is used, while a DETR model [@Carion2020] is used for vision. The [CLS] token is also used in the BERT
encoder, which is also included in the output sequence. A task-specific token is additionally added to the input of the encoders.
The output of the encoders is then concatenated in case all modalities are used. Otherwise, only the modality-specific encodings are passed on.
The decoder is modality agnostic and thus stays the same;its input is the concatenated sequence of the encodings and an additional task-specific sequence. Since the
decoder architecture sticks to the DETR model, the obtained sequence of the decoder is then passed to task-specific output heads, which need to be changed for each
task and are also learned. A task-specific loss was used for object detection, while the cross-entropy loss was used for the other tasks.
In experiments, UniT was evaluated against a single-domain version of itself. The general model outperformed the specialist one on multimodal tasks but was outperformed
on unimodal tasks by the specialist UniT. UniT was furthermore also outperformed by sota models, although the numbers remained close.
Even though UniT does not achieve sota or consistently outperforms its specialist version, it is a powerful method to achieve a simple multi-purpose model. Using available
encoder models, it is easily extendable and does not require a novel embedding scheme.

#### OFA
Another multi-purpose transformer is OFA (Once For All) [@Wang2022]. To utilize the seq2seq architecture of the transformer, all tasks and the respective modality
presented are transformed into seq2seq problems.
While other models fuse the modalities in the model, such that the modalities are presented as is to the model, OFA tries a different approach, tokenizing every
input instead of using a shared vocabulary for all available modalities and tasks done differently for input and output. Since tokenizing an image using a vocabulary
is not feasible, a similar approach to ViT is used (where input is flattened to 16x 16) to obtain a flattened sequence of $P$ representations. These representations
are later used along the embeddings of tokens from text input, which is tokenized using Byte-Pair-Encoding (Sennrich). After feeding the embeddings to the encoder,
the decoder produces the output as a sequence again. In this case, however, images are represented as a sequence of tokens, similar to DALL-E (RAMESH X). Furthermore,
a unique sequence for the bounding box is also used for object detection and recognition. To generate the task-specific eventual solution, it is thus required that
another model is used to generate the images based on the tokens and to visualize the bounding boxes based on the yielded coordinates.
Since OFA is an autoregressive model, the objective is based on cross-entropy loss. The probability for the next token is predicted based on the previously produced
tokens and the input provided.
OFA was trained on different crossmodal: visual grounding, grounded captioning, image-text matching, image captioning, and visual question answering. And unimodal tasks:
image infilling, object detection, and text reconstruction as in BART.
OFA outperformed sota models on cross-modal tasks like image captioning, visual question answering, visual entailment, and visual grounding. On uni-modal tasks,
OFA performed well, although it does not outperform sota models here.

#### Gato
Another model that uses the seq2seq architecture of the transformer is GATO (A Generalist Agent). The model can be used as a language model, an agent to play games,
and an agent to control robotics.
As in OFA, the problems are transformed into a seq2seq problem, on which a transformer (decoder only) is applied. Although the model was trained in a supervised fashion,
it is intended to view GATO as a reinforcement learning agent.
Every data input is embedded, as in OFA, where the critical difference here is that actions are encoded too.
Visual input is encoded using a flattened sequence of 16x16 patches and embedded using a ResNet (X), while text input is tokenized using SentencePiece (X). Furthermore,
discrete values like buttons in games and continuous data like movements from robotics are tokenized in a 1024 vocabulary each. To represent all modalities sequentially,
the different tokens are concatenated. A separator token is added to distinguish the observations from the following action. Using this approach, the transformer can predict
the next token, the action, based on the conditioning values:
<!-- $$\[x_{Text}, x_{Images}, x_{Discrete and Continuous Values} | y_{Action}\]$$ -->
Since it is only necessary to predict the action based on the conditioning values, a mask function is added to the cross-entropy loss function, which masks the conditioning
values so that only the next action is predicted. The masking function is always one for text since every previous text token is necessary to predict the next one.
GATO was evaluated in RL-based tasks against specialist RL agents, where GATO was lower than the specialist agents or at a human level. On unseen tasks, GATO required fine-tuning
since few-shot learning is not feasible due to the input length restrictions in transformers. However, the results were mixed, some improvements were possible, the expert was
outperformed, but also massive fine-tuning efforts only led to small gains. However it was found that the generalist agent outperformed the specialist agent (in this case, a GATO
model only trained on this task from scratch), only trained for this task, most of the times. Only one task, where also fine-tuning did not increase the performance, the specialist 
odel achieved better results.
In robotics, GATO showed similar behavior to the baseline sota model. Additionally, GATO also showed 
capabilities in image captioning and dialogue modeling. However, these aspects were not elaborated further.

#### Comparison
We revisited four multi-purpose models (Multimodel, UniT, OFA, and GATO), of which three were based on the transformer architecture. The last two models transformed the tasks into
seq2seq problems for feeding the transformer. UniT, using the entire transformer architecture, also eventually produces a sequence, but the focus here is on task-specific heads. At
this point, a similarity to OFA can be found since OFA also needs to use additional task models to produce the eventual output, like the visualizer and the image generator. Indirectly
or directly, all presented models produce a sequence as output, which might be an intermediate result of the nature of the transformer, which is a naturally autoregressive model. However,
while UniT used multiple encoders for different modalities, it might be interesting to see potentially different decoders, especially with the rise of diffusion, where the sequential output of
the transformer decoder might only be a hindrance.
Although these models did not achieve sota most of the time, sometimes they showed a better performance than their specialist version (GATO, Multimodel)

### Pathway Proposal

Multipurpose models can also be thought of without the currently well-performing transformer architecture. A proposal for a new approach to multipurpose models was introduced as Google's
Pathways [@Dean21] aiming to address current pitfalls in deep learning.
Current deep learning consists mainly of large dense neural networks trained for a specific task. This paradigm is limiting so that no transfer between tasks is possible, and every
time a model is trained, it needs to learn from scratch (or needs to be pre-trained from scratch). Human intelligence can also transfer knowledge between tasks, so it is not necessary
to learn everything from scratch. However, transfer in deep learning models is not sufficiently possible, which is a severe constraint. The problem of catastrophic forgetting and negative transfer may arise.

#### Idea

Pathways follow a different idea to overcome the problems mentioned above. The model consists of a large graph, through which input can be mapped to an output. The nodes of the network
are neural networks themselves. A pass through this network does not include calling all nodes and thus all neural networks, but only a few. The pass follows a specific path from the input to the output.
The underlying idea behind this is similar to the mixture-of-expert models described above. Only the specific networks dedicated to solving this particular problem are to be called.
However, pathways consist solely of layers of expert networks.
At this point, it is necessary to recall that multi-task learning aims to generalize better on new tasks since the knowledge about previously learned tasks can be applied.
This idea is the foundation of pathways too, where specialist networks (nodes) are combined in a larger network. It is assumed that the model's generalization capabilities
increase severely by finding an appropriate path for a task to the appropriate specialist nodes. In this setup, the particular problem-solving capabilities are combined.
Furthermore, multimodality is also considered a potential extension. Adding more modalities might not be a difficult problem considering the architecture of the previously
introduced transformer-based models.
Overall the approach of a sparse model combining multiple experts offers many opportunities to combine modalities and reuse task-specific capabilities. The sparsity of
the model offers increased inference time since only some parts of the networks are activated during inference.
Another part of the pathways project includes the improvement of current hardware limitations. It is already observable that Moore's Law (each x years, the compute capacity doubles)
slows severely down while deep learning research has grown exponentially in the late 2010 years. Thus, also hardware needs to be adapted to the
growing demand in deep learning. In the context of the pathway proposal, a novel framework for Google datacenters has been introduced, aiming to reduce overhead during computation
and access specific parts of the model to utilize the technical advantages of sparse networks.
As in dense models, it is not necessary to use the whole network but only chunks of it, which is in contrast to previous dense models, which require the whole model to be accessed.
So far, two large pre-trained models have been introduced based on the new training framework. One is the Pathways Language Model (PaLM) [Chowdhery2022], which is by now the largest
language model using 540 billion parameters, Minerva [@Lewkowycz2022], which is based on PaLM, and Parti [@parti], which is a text-to-image generator.

### Related Work to Pathways
In this section, similar architecture will be revisited to provide an overview how close current research has come to achieve a model like the pathway proposal.

#### PathNet
An earlier approach for a sparse multi-task network, which looks deceptively similar, is Pathnet [@Fernando2017]. Pathnet is a training algorithm to reuse knowledge on a previously
learned task without the risk of catastrophic forgetting, thus using the positive aspects of multi-task learning.
The objective of the pathnet is built upon an evolutionary algorithm (EA). An evolutionary algorithm is used to optimize a discrete problem where derivative-based algorithms cannot be
applied. The algorithm is based on a population and a fitness function. Parts of the population are chosen to create offspring either by mutation or recombination. The resulting population
is then evaluated with respect to their fitness function, and only the best-suited individuals are kept. The same procedure is repeated based on the resulting population until a specific
criterion is met (e.g., convergence). While evolving the population, it is necessary to balance exploration and exploitation to find the desired outcome. Since EAs are research topics
themselves and may vary heavily, we refer to [@Baeck1993] and, more recently, to [@Doerr2021] for further insights.
Neural networks are often depicted as a graph in which the input is directed to all nodes in hidden layers. The output of hidden layers is again directed to each node in the next
hidden layer or nodes in an output layer.
In this case, each node is itself a neural network; thus, all data is passed to each node.
The training algorithm finds the best paths for a specific task through the network. At first random paths through the network are initialized, then the paths are trained for $T$
epochs. After training, the paths are evaluated against each other, where the winning path overwrites the losing path. To achieve exploration, however, the overwritten path is mutated.
The path is intended to deviate a little by also visiting neighboring nodes to achieve exploration. Until a specific criterion (e.g., number of epochs) to stop is reached, the current
paths are frozen so that no more modifications to the parameters of the networks on this path are possible. All other parameters are newly initialized again. Also, a different, task-specific
head is initialized. The same procedure is now done again for another task. The main difference is that the previously obtained path, including the trained networks, is frozen during training
so that the model can transfer knowledge from the previous task to the new task. The model finds appropriate paths throughout the network until the stopping criterion is met.
Pathnet was evaluated on supervised learning tasks and in reinforcement learning scenarios. As a baseline learning from scratch and fine-tuning were chosen. Fine-tuning is done on top of the
first model, which Pathnet trains for the first task.
Overall, pathnet improved training in the second task for time and quality in prediction, compared to standard fine-tuning and learning from scratch.
Although pathnet only looks like pathways on the surface, it has shown a positive transfer from previously trained models in a network like this. Different tasks can reuse the knowledge
from training on previous tasks.

#### LIMoE
In the context of Pathways, the work LIMoE (Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts) [@Mustafa2022] is also necessary to be mentioned.
LIMoE combines text and vision input using a transformer encoder, which includes an MoE layer that can be used sparsely during inference time. Besides the context of the pathways
proposal, LIMoE also aims to provide a new approach to contrastive learning. While previous methods used two models (two-tower) to encode the modalities, LIMoE is solely based
on one model, where the modalities are processed in a single transformer-like model (one-tower).
Thus, the model described in LIMoE is based on a modified transformer. The text data is encoded using one-hot-SentencePiece encoding, while images are tokenized in the same way as
done in ViT [@dosovitskiy2020image] to provide the input appropriately. The main difference to the standard transformer is an MoE layer at the spot where the feed-forward network
usually lies. In this layer, $E$ experts are used, which are themselves feed-forward-networks. For each token, $K$ appropriate experts will map the tokens further downstream.
The routing is computed by a gating net network, which uses top-k routing. Another feature here is a fixed-length buffer for each expert in the MoE layer. This buffer is used to
store tokens before an expert network processes them, assuming that the allocations of tokens for each expert are balanced. If it is impossible to buffer tokens for the experts,
the tokens will be dropped. To furthermore decide which tokens are to be dropped, Batch Priority Routing [@Riquelme2021] is used to provide a ranking mechanism on how which token
is to be dropped. The output of the transformer encoder is then average pooled and subsequently multiplied with a modality-specific weight matrix, which produces the eventual output
for the token of both modalities $z_{image}$ and $z_{text}$.
The model is trained using a contrastive objective. The contrastive loss, in this case, aims to maximize the paired visual and textual input while minimizing all combinations of
unpaired embeddings. This objective can be achieved by using the dot-product as a similarity measure between the two embeddings of both modalities, which provide a differentiable
operation through which the overall loss can be minimized.
Additionally, to contrastive loss, the pitfalls of a multimodal MoE are also considered. One challenge in MoE is the correct balancing of routing to experts, which is even more
challenging when using unbalanced multimodal data. To address this issue, two new losses based on entropy are introduced. Entropy can be used as an appropriate term since it provides
a valuable number for the uniformity of the distribution, which is necessary to balance the expert assignments. The losses are aimed at controlling the allocation of experts to tokens,
which is also necessary to fulfill the assumptions for the implemented buffer.
One loss considers the token level (local) routing, and the other considers the overall expert routing distribution (global). The local loss aims to achieve no uniform behavior in expert
allocation such that each token is indeed assigned to specific experts. In contrast, the overall global loss aims to achieve uniformity over all tokens to avoid a collapse in which tokens
are solely assigned to a few experts which do not have the capacity to deal with all tokens. These losses are computed for each modality. Furthermore, already available losses for training
MoE models were also added to avoid known downsides of MoE models.
LIMoE was compared against similar models, which show the pitfalls of being solely dense and specialized on a task, which is also the case for two-tower setups like CLIP [@radford2021learning].
The test dataset was ImageNet [@deng2009imagenet] and MS-COCO [@mccoco]. Overall, LIMoE-H/14 (largest model, 12 MoE-layers, 32 experts per layer) achieved strong performance considering that only
one model was used for two modalities against specialist models in two-tower setups. It was also possible to outperform CLIP and ALIGN by a significant margin while using not too many
additional parameters. Models that achieved similar results to LIMoE used at least twice the number of parameters for a forward pass. 
So far, LIMoE is a new approach to contrastive learning. It showed that the sparse approach could significantly lead to strong performance while using fewer parameters for inference.
Furthermore, it also includes essential aspects of the pathways proposal using the MoE layer where sparsity comes into play. The authors stated that the critical aspect is how the model
routes to the specific expert, which might also be helpful in an additional multitask setup, which is an important step towards a pathway model. However, the authors also stated that merging
modalities remains a field of research, as well as adding further modalities, since Vision and text have already proven to be non-trivial to combine, and more problems might arise with further modalities.

#### muNet
Another model which is worth considering in the context of the pathways proposal is muNet [@Gesmundo2022a]. The initial criticism of current inefficiencies in deep learning is addressed by providing a framework to avoid discarding knowledge. muNet is an approach to improving multi-task learning with a focus on fine-tuning. In current practice for fine-tuning, a pre-trained model is copied and then explicitly trained on a task by overwriting previous knowledge. In other cases, models are only trained from scratch to solve one specific task.
muNet evolves a pre-trained model on different tasks, using an EA to reuse already trained knowledge. An initial model is modified so that it becomes more and more optimized to adapt to specific tasks while keeping all previously learned knowledge. These modifications are so-called mutations applied to the model to score better. Eventually, a set of models is obtained, which includes new neural networks based majorly on the parameters of the initial model. The new modules can be seen as paths to layers from the initial network and newly added layers as well. Although the network is not sparse, the concept of reusing many parts of a network while accounting for multiple tasks is to be mentioned in the context of the pathway proposal.
The EA of muNet starts with an initially proposed model that is mutated further on. All further mutations are stored so that after a set of candidates is available, the set can be split into models trained for this task (active population) and models for other tasks (inactive population). These two sets become the sets of candidates for the following task-specific iterations.
Training a specific task follows three steps: sampling models, mutating, training, and evaluation. The best scoring model is added to the active population for further mutation.
A sample algorithm accounts for exploration and exploitation to get a candidate model for subsequent mutation. The active population is ordered in a descending list based on the model's score. Each list entry is then revisited, starting from the highest scoring model onward, so that the better performing models are considered first (exploitation). The draw probability is computed as: $\mathbb P(m|t) = 0.5 ^{ \#timesSelected(m, t)}$, where $\#timesSelected(m, t)$ (where $\#timesSelected(m, t)$ is the amount of previous mutations based on model $m$ for task $t$). The more unsuccessful mutations the model has had before, the smaller the draw probability becomes. Thus, exploration is emphasized by considering previous attempts and allowing other models to be preferred. However, if this method does not yield a candidate, a model is drawn from the union of the inactive and active population.
Applying mutations is the next step in the algorithm. A random number of mutations are drawn from the set of possible mutations, which include:

- Layer cloning, a layer is cloned for training. The layer's parameters are copied from the parent model so that training can continue using the same knowledge. The other layers are still used but are not updated. Additionally, the task-specific head layer is cloned to account for the underlying changes. In the case of training on a new task, the head is also newly initialized.
- Layer insertion. Two layers are added to the model as residual adapters ([@Rebuffi2017], [Houlsby2019@]). The second layer is zero-initialized to keep an identity function so that training can continue from the state before mutation.
- Layer removal is used to skip layers while still using all other of the parent model in a frozen state.
- Hyperparameter change. Sample hyperparameters that are close to the ones of the parent model. A list of neighboring values is constructed from which a parameter is drawn.

Subsequently, the models are trained on the task and scored. If the mutated model is better than the parent model, it is also added to this task's set of active models. This routine is done for all tasks and can be repeated several times. Ultimately, only the best scoring models are kept for each task, yielding a list of models consisting of paths to layers of the old model and newly trained ones. 
muNet was evaluated for fine-tuning against a ViT instance, which is aimed at being the most generalizable [@Steiner2021]. The evaluation benchmarks consisted of multiple classification problems (multi-task). ViT was fine-tuned on all of these tasks as a baseline. In contrast, another ViT was evolved using muNet, on which the baseline model was evaluated. The approach using muNet outperformed the fine-tuned ViT while using significantly fewer parameters.
Although muNet is neither a multipurpose model nor a sparse model, the concept of building task-specific paths by enhancing pre-trained models fits in the broader context of the pathway proposal. Research in the direction of combining automated routing for tasks and adding multi-modality might lead toward the vision of pathways and is thus highly recommended.

<!-- 
  - not peer-reviewed
  - no multimodality
  - tested for fine-tuning
  - task token?
  - Include earlier knowledge (hyperparams, layers etc.) -->


### Conclusion Pathways
We presented models that might lead to a pathway model or a similar model. However, it must be stated that no model has been implemented so far up to this day (late august 2022). Although the
path regarding appropriate hardware has already been paved, it is not sure how exactly the model will look. Many open questions remain. Will it have a layer-like structure as pathnet and as shown
in the proposal video [@Dean21]? Will the path have a hierarchical structure, intermediate nodes, or loops? What will the sparsity look like in the model itself? How exactly will routing be achieved?
However, the previously mentioned successes in multi-task learning, hardware, and the combination of MoE and multi-modal, as seen in LIMoE, already hint that this direction might be further researched
in the upcoming month.
 
<!-- - muNet and LIMoE show sparsity works (less param, good performance)
- Evolutionary with large models
- routing munet
In conclusion  -->

### Discussion

Considering the pace of the field and that results, as seen in Gato, flamingo and parti would not have been anticipated a few years ago, it is highly likely to see further
breakthroughs in the field of multi-purpose models, which is especially the case since hardware-related issues are now being addressed and further developed. Based on these
developments, more general models will be published in the near future. Models with even higher capabilities will also let new questions and problems arise. At this point,
there is already the first issue arising, which is the trend of proprietary models. When GPT-3 was published, OpenAI closed access to the model leaving only a few with API keys
to access the model.
In contrast to the introduction of BERT, which was open source, there did not follow a trend of GPTology as it was done with BERTology, leaving the model underresearched compared
to its open source and significantly smaller peers. On the other side, there also exists a trend of open sourcing some models, as seen with GPT-J and GPT-Neo or recently with the
introduction of OPT by meta. However, even though this trend exists, there is the issue of the increasing size of the models, which makes it almost impossible to train or even
fine-tune these models without using massive amounts of computational resources.
Another issue that comes with the higher capability of these models is the societal impact of these models. Pop culture has severely impacted the perception of artificial
intelligence, which might also become a topic in the future. It has already been a public issue that a google employee claimed that LamDA is sentient. With further high-quality
models, there might be more discussions on how to deal with AI in the future. Since this already showed that the Turing test might not be an appropriate metric anymore, it might
also be helpful to research new metrics for AI and possibly AGI.
In his TED-talk, Dean saw Pathway as a promising path toward AGI, which is a bold statement. However, considering the quality of already existing models, it might be a significant
step in increasing the quality of generative models for solving multiple general tasks. Nevertheless, new problems will arise which are likely not solved with this model like
continuous learning. The examined models and pathways are only trained once and then remain frozen in their knowledge. This issue is already visible in older language models like
BERT and GPT, where Donald Trump is still president and COVID-19 does not exist.

<!-- ### TODO
 - Proposal for unifying standardizing evaluation, common benchmarks etc.
 - Outlook -->

