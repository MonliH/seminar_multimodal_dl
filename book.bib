@ARTICLE{Lu2020,
  AUTHOR={Yu, Jun and Li, Jing and Yu, Zhou and Huang, Qingming},
  JOURNAL={IEEE Transactions on Circuits and Systems for Video Technology}, 
  TITLE={Multimodal Transformer With Multi-View Visual Representation for Image Captioning}, 
  YEAR={2020},
  VOLUME={30},
  NUMBER={12},
  PAGES={4467-4480},
  DOI={10.1109/TCSVT.2019.2947482}}


@misc{Fedus2021,
  DOI = {10.48550/ARXIV.2101.03961},
  URL = {https://arxiv.org/abs/2101.03961},
  AUTHOR = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  KEYWORDS = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  TITLE = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  PUBLISHER = {arXiv},
  YEAR = {2021},
  COPYRIGHT = {arXiv.org perpetual, non-exclusive license}
}

@misc{Mustafa2022,
  DOI = {10.48550/ARXIV.2206.02770},
  URL = {https://arxiv.org/abs/2206.02770},
  AUTHOR = {Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  TITLE = {Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts},
  PUBLISHER = {arXiv},
  YEAR = {2022},
  COPYRIGHT = {Creative Commons Attribution 4.0 International}
}

@article{Carion2020,
  AUTHOR    = {Nicolas Carion and
               Francisco Massa and
               Gabriel Synnaeve and
               Nicolas Usunier and
               Alexander Kirillov and
               Sergey Zagoruyko},
  TITLE     = {End-to-End Object Detection with Transformers},
  JOURNAL   = {CoRR},
  VOLUME    = {abs/2005.12872},
  YEAR      = {2020},
  URL       = {https://arxiv.org/abs/2005.12872},
  EPRINTTYPE= {arXiv},
  EPRINT    = {2005.12872},
  TIMESTAMP = {Thu, 28 May 2020 17:38:09 +0200},
  BIBURL    = {https://dblp.org/rec/JOURNALs/corr/abs-2005-12872.bib},
  BIBSOURCE = {dblp computer science bibliography, https://dblp.org}
}

@misc{Crawshaw2020,
  DOI = {10.48550/ARXIV.2009.09796},
  URL = {https://arxiv.org/abs/2009.09796},
  AUTHOR = {Crawshaw, Michael},
  KEYWORDS = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  TITLE = {Multi-Task Learning with Deep Neural Networks: A Survey},
  PUBLISHER = {arXiv},
  YEAR = {2020},
  COPYRIGHT = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{Baltrusaitis2019,
  AUTHOR={Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  JOURNAL={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  TITLE={Multimodal Machine Learning: A Survey and Taxonomy}, 
  YEAR={2019},
  VOLUME={41},
  NUMBER={2},
  PAGES={423-443},
  DOI={10.1109/TPAMI.2018.2798607}}

@article{Kaiser2017,
  TITLE	= {One Model To Learn Them All},
  AUTHOR	= {Lukasz Kaiser and Aidan N. Gomez and Noam Shazeer and Ashish Vaswani and Niki Parmar and Llion Jones and Jakob Uszkoreit},
  YEAR	= {2017},
  URL	= {https://arxiv.org/pdf/1706.05137.pdf},
  JOURNAL	= {arXiv}
}

@INPROCEEDINGS{Hu2021,
  AUTHOR={Hu, Ronghang and Singh, Amanpreet},
  BOOKTITLE={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  TITLE={UniT: Multimodal Multitask Learning with a Unified Transformer},
  YEAR={2021},
  VOLUME={},
  NUMBER={},
  PAGES={1419-1429},
  DOI={10.1109/ICCV48922.2021.00147}}

@misc{Li2019,
  DOI = {10.48550/ARXIV.1908.03557},
  URL = {https://arxiv.org/abs/1908.03557},
  AUTHOR = {Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  TITLE = {VisualBERT: A Simple and Performant Baseline for Vision and Language},
  PUBLISHER = {arXiv},
  YEAR = {2019},
  COPYRIGHT = {arXiv.org perpetual, non-exclusive license}
}

@online{Dean21,
  AUTHOR = {Jeff Dean},
  TITLE = {Introducing Pathways: A next-generation AI architecture},
  YEAR = 2021,
  URL = {https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/},
  URLdate = {2022-08-23}
}

@article{Krishna2017,
  AUTHOR = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
  TITLE = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
  YEAR = {2017},
  ISSUE_DATE= {May       2017},
  PUBLISHER = {Kluwer Academic Publishers},
  ADDRESS = {USA},
  VOLUME = {123},
  NUMBER = {1},
  ISSN = {0920-5691},
  URL = {https://DOI.org/10.1007/s11263-016-0981-7},
  DOI = {10.1007/s11263-016-0981-7},
  JOURNAL = {Int. J. Comput. Vision},
  MONTH = {may},
  PAGES = {32–73},
  NUMPAGES = {42},
  KEYWORDS = {Language, Relationships, Attributes, Question answering, Scene graph, Crowdsourcing, Computer vision, Knowledge, Image, Objects, Dataset}
}


@InProceedings{Wang2022,
  TITLE = 	 {{OFA}: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  AUTHOR =       {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  bookTITLE = 	 {Proceedings of the 39th International Conference on Machine Learning},
  PAGES = 	 {23318--23340},
  YEAR = 	 {2022},
  EDITOR = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  VOLUME = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  MONTH = 	 {17--23 Jul},
  PUBLISHER =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wang22al/wang22al.pdf},
  URL = 	 {https://proceedings.mlr.press/v162/wang22al.html},
  abstract = 	 {In this work, we pursue a unified paradigm for multimodal pretraining to break the shackles of complex task/modality-specific customization. We propose OFA, a Task-Agnostic and Modality-Agnostic framework that supports Task Comprehensiveness. OFA unifies a diverse set of cross-modal and unimodal tasks, including image generation, visual grounding, image captioning, image classification, language modeling, etc., in a simple sequence-to-sequence learning framework. OFA follows the instruction-based learning in both pretraining and finetuning stages, requiring no extra task-specific layers for downstream tasks. In comparison with the recent state-of-the-art vision &amp; language models that rely on extremely large cross-modal datasets, OFA is pretrained on only 20M publicly available image-text pairs. Despite its simplicity and relatively small-scale training data, OFA achieves new SOTAs in a series of cross-modal tasks while attaining highly competitive performances on uni-modal tasks. Our further analysis indicates that OFA can also effectively transfer to unseen tasks and unseen domains. Our code and models are publicly available at https://github.com/OFA-Sys/OFA.}
}

@misc{Reed2022,
  DOI = {10.48550/ARXIV.2205.06175},
  URL = {https://arxiv.org/abs/2205.06175},
  AUTHOR = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  KEYWORDS = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Machine Learning (cs.LG), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  TITLE = {A Generalist Agent},
  PUBLISHER = {arXiv},
  YEAR = {2022},
  COPYRIGHT = {Creative Commons Attribution 4.0 International}
}

@article{Chowdhery2022,
  TITLE	= {PaLM: Scaling Language Modeling with Pathways},
  AUTHOR	= {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  YEAR	= {2022},
  URL	= {https://arxiv.org/abs/2204.02311},
  JOURNAL	= {arxiv:2204.02311}
}

@misc{Yu2022,
  DOI = {10.48550/ARXIV.2206.10789},
  URL = {https://arxiv.org/abs/2206.10789},
  AUTHOR = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  TITLE = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
  PUBLISHER = {arXiv},
  YEAR = {2022},
  COPYRIGHT = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Fernando2017,
  TITLE	= {PathNet: Evolution Channels Gradient Descent in Super Neural Networks},
  AUTHOR	= {Chrisantha Fernando and Dylan Banarse and Charles Blundell and Yori Zwols and David Ha and Andrei A. Rusu and Alexander Pritzel and Daan Wierstra},
  YEAR	= {2017},
  URL	= {https://arxiv.org/abs/1701.08734}
}

@InProceedings{He2016b,
  AUTHOR={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  EDITOR={Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  TITLE={Identity Mappings in Deep Residual Networks},
  BOOKTITLE={Computer Vision -- ECCV 2016},
  YEAR={2016},
  PUBLISHER={Springer International Publishing},
  ADDRESS={Cham},
  PAGES={630--645},
  ISBN={978-3-319-46493-0}
}

@INPROCEEDINGS{Dean20,
  AUTHOR={Dean, Jeffrey},
  BOOKTITLE={2020 IEEE International Solid- State Circuits Conference - (ISSCC)},
  TITLE={1.1 The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design},
  YEAR={2020},
  VOLUME={},
  NUMBER={},
  PAGES={8-14},
  DOI={10.1109/ISSCC19947.2020.9063049}}

@techreport{Lewkowycz2022,
  TITLE	= {Solving Quantitative Reasoning Problems with Language Models},
  AUTHOR	= {Aitor Lewkowycz and Anders Andreassen and David Martin Dohan and Ethan S Dyer and Henryk Michalewski and Vinay Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
  YEAR	= {2022},
  URL	= {https://arxiv.org/abs/2206.14858}
}

@inproceedings{Riquelme2021,
 AUTHOR = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr\'{e} and Keysers, Daniel and Houlsby, Neil},
 BOOKTITLE = {Advances in Neural Information Processing Systems},
 EDITOR = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 PAGES = {8583--8595},
 PUBLISHER = {Curran Associates, Inc.},
 TITLE = {Scaling Vision with Sparse Mixture of Experts},
 URL = {https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf},
 VOLUME = {34},
 YEAR = {2021}
}

@misc{Gesmundo2022a,
  DOI = {10.48550/ARXIV.2205.10937},
  URL = {https://arxiv.org/abs/2205.10937},
  AUTHOR = {Gesmundo, Andrea and Dean, Jeff},
  KEYWORDS = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  TITLE = {muNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-tuning Multitask Systems},
  PUBLISHER = {arXiv},
  YEAR = {2022},
  COPYRIGHT = {Creative Commons Attribution 4.0 International}
}


@article{Steiner2021,
  DOI = {10.48550/ARXIV.2106.10270},
  URL = {https://arxiv.org/abs/2106.10270},
  AUTHOR = {Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  KEYWORDS = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  TITLE = {How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  PUBLISHER= {arXiv},
  YEAR = {2021},
  COPYRIGHT = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{Houlsby2019,
  TITLE = 	 {Parameter-Efficient Transfer Learning for {NLP}},
  AUTHOR =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  bookTITLE = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2790--2799},
  YEAR = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/houlsby19a.html},
  abstract = 	 {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8%$ of the performance of full fine-tuning, adding only $3.6%$ parameters per task. By contrast, fine-tuning trains $100%$ of the parameters per task.}
}

@inproceedings{Rebuffi2017,
 AUTHOR = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 bookTITLE = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 TITLE = {Learning multiple visual domains with residual adapters},
 url = {https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf},
 volume = {30},
 YEAR = {2017}
}


@misc{Bilen2017,
  AUTHOR = {Bilen, Hakan and Rebuffi, SSylvestre and Jakab, Tomas},
  TITLE = {Visual domain decathlon},
  YEAR = {2017}
}

@article{Doerr2021,
  AUTHOR = {Doerr, Benjamin and Neumann, Frank},
  TITLE = {A Survey on Recent Progress in the Theory of Evolutionary Algorithms for Discrete Optimization},
  YEAR = {2021},
  issue_date = {December 2021},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {1},
  number = {4},
  issn = {2688-299X},
  url = {https://doi.org/10.1145/3472304},
  doi = {10.1145/3472304},
  journal = {ACM Trans. Evol. Learn. Optim.},
  month = {oct},
  articleno = {16},
  numpages = {43},
  keywords = {parameterized complexity, discrete optimization, evolutionary algorithms, estimation of distribution algorithms, Theory}
}

@ARTICLE{Baeck1993,
  AUTHOR={Bäck, Thomas and Schwefel, Hans-Paul},
  JOURNAL={Evolutionary Computation},
  TITLE={An Overview of Evolutionary Algorithms for Parameter Optimization},
  YEAR={1993},
  VOLUME={1},
  NUMBER={1},
  PAGES={1-23},
  DOI={10.1162/evco.1993.1.1.1}}

@misc{Hinton2015,
  doi = {10.48550/ARXIV.1503.02531},
  url = {https://arxiv.org/abs/1503.02531},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Distilling the Knowledge in a Neural Network},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Shaazer2017,
  title	= {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author	= {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  year	= {2017},
  URL	= {https://openreview.net/pdf?id=B1ckMDqlg}
}

@ARTICLE{Jordan1994,
  author={Jordan, Michael I. and Jacobs, Robert A.},
  journal={Neural Computation}, 
  title={Hierarchical Mixtures of Experts and the EM Algorithm}, 
  year={1994},
  volume={6},
  number={2},
  pages={181-214},
  doi={10.1162/neco.1994.6.2.181}}

@ARTICLE{Jacobs1991,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  doi={10.1162/neco.1991.3.1.79}}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}


@InProceedings{pmlr-v139-ramesh21a,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@ARTICLE{ResNet,
  AUTHOR = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  URL = {http://arxiv.org/abs/1512.03385},
  DATE = {2015},
  EPRINT = {1512.03385},
  EPRINTTYPE = {arXiv},
  JOURNALTITLE = {CoRR},
  TITLE = {Deep Residual Learning for Image Recognition},
}

@INPROCEEDINGS{mccoco,
  AUTHOR = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {Computer Vision -- ECCV 2014},
  DATE = {2014},
  ISBN = {978-3-319-10602-1},
  PAGES = {740--755},
  TITLE = {Microsoft COCO: Common Objects in Context},
}

@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}

@ARTICLE{Devlin2018,
  AUTHOR = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  DATE = {2018-10-11},
  EPRINT = {1810.04805},
  EPRINTCLASS = {cs.CL},
  EPRINTTYPE = {arXiv},
  FILE = {:http\://arxiv.org/pdf/1810.04805v2:PDF},
  KEYWORDS = {cs.CL},
  TITLE = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
}

@ARTICLE{brown2020language,
  AUTHOR = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  DATE = {2020},
  JOURNALTITLE = {Advances in neural information processing systems},
  PAGES = {1877--1901},
  TITLE = {Language models are few-shot learners},
  VOLUME = {33},
}

@article{ImageNet,
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  title = {ImageNet Large Scale Visual Recognition Challenge},
  year = {2015},
  issue_date = {December  2015},
  publisher = {Kluwer Academic Publishers},
  address = {USA},
  volume = {115},
  number = {3},
  issn = {0920-5691},
  url = {https://doi.org/10.1007/s11263-015-0816-y},
  doi = {10.1007/s11263-015-0816-y},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  journal = {Int. J. Comput. Vision},
  month = {dec},
  pages = {211–252},
  numpages = {42},
  keywords = {Benchmark, Object detection, Large-scale, Object recognition, Dataset}
}

@ARTICLE{dosovitskiy2020image,
  AUTHOR = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  DATE = {2020},
  JOURNALTITLE = {arXiv preprint arXiv:2010.11929},
  TITLE = {An image is worth 16x16 words: Transformers for image recognition at scale},
}

@ARTICLE{vaswani2017attention,
  AUTHOR = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {Ł}ukasz and Polosukhin, Illia},
  DATE = {2017},
  JOURNALTITLE = {Advances in neural information processing systems},
  TITLE = {Attention is all you need},
  VOLUME = {30},
}

@INPROCEEDINGS{radford2021learning,
  AUTHOR = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  ORGANIZATION = {PMLR},
  BOOKTITLE = {International Conference on Machine Learning},
  DATE = {2021},
  PAGES = {8748--8763},
  TITLE = {Learning transferable visual models from natural language supervision},
}

@INPROCEEDINGS{deng2009imagenet,
  AUTHOR = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  ORGANIZATION = {Ieee},
  BOOKTITLE = {2009 IEEE conference on computer vision and pattern recognition},
  DATE = {2009},
  PAGES = {248--255},
  TITLE = {Imagenet: A large-scale hierarchical image database},
}

@ARTICLE{parti,
  AUTHOR = {Yu, Jiahui and Xu, Yuanzhong and Koh, Jing and Luong, Thang and Baid, Gunjan and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu and Hutchinson, Ben and Han, Wei and Parekh, Zarana and Li, Xin and Zhang, Han and Baldridge, Jason and Wu, Yonghui},
  DATE = {2022-06},
  DOI = {10.48550/arXiv.2206.10789},
  TITLE = {Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
}